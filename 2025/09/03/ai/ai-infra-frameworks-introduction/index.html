<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="keywords" content="Hexo Theme Redefine"><meta name="author" content="Fre5h1nd"><script>!function(){const e="dark";function t(t){const o=t===e,c=document.documentElement;c.setAttribute("data-theme",t),c.classList.add(t),c.classList.remove(o?"light":e),c.style.colorScheme=t}const o=function(){try{const t=localStorage.getItem("REDEFINE-THEME-STATUS");if(t){const{isDark:o}=JSON.parse(t);return o?e:"light"}}catch(e){}return matchMedia("(prefers-color-scheme: dark)").matches?e:"light"}();t(o),matchMedia("(prefers-color-scheme: dark)").addEventListener("change",({matches:o})=>{localStorage.getItem("REDEFINE-THEME-STATUS")||t(o?e:"light")}),"loading"!==document.readyState?document.body.classList.add(o+"-mode"):document.addEventListener("DOMContentLoaded",()=>{document.body.classList.add(o+"-mode"),document.body.classList.remove((o===e?"light":e)+"-mode")})}()</script><style>:root[data-theme=dark]{--background-color:#202124;--background-color-transparent:rgba(32, 33, 36, 0.6);--second-background-color:#2d2e32;--third-background-color:#34353a;--third-background-color-transparent:rgba(32, 33, 36, 0.6);--primary-color:#0066CC;--first-text-color:#ffffff;--second-text-color:#eeeeee;--third-text-color:#bebec6;--fourth-text-color:#999999;--default-text-color:#bebec6;--invert-text-color:#373D3F;--border-color:rgba(255, 255, 255, 0.08);--selection-color:#0066CC;--shadow-color-1:rgba(255, 255, 255, 0.08);--shadow-color-2:rgba(255, 255, 255, 0.05)}:root[data-theme=light]{--background-color:#fff;--background-color-transparent:rgba(255, 255, 255, 0.6);--second-background-color:#f8f8f8;--third-background-color:#f2f2f2;--third-background-color-transparent:rgba(241, 241, 241, 0.6);--primary-color:#0066CC;--first-text-color:#16171a;--second-text-color:#2f3037;--third-text-color:#5e5e5e;--fourth-text-color:#eeeeee;--default-text-color:#373D3F;--invert-text-color:#bebec6;--border-color:rgba(0, 0, 0, 0.08);--selection-color:#0066CC;--shadow-color-1:rgba(0, 0, 0, 0.08);--shadow-color-2:rgba(0, 0, 0, 0.05)}body{background-color:var(--background-color);color:var(--default-text-color)}:root[data-theme=dark] body{background-color:var(--background-color);color:var(--default-text-color)}</style><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://registry.npmmirror.com" crossorigin><link rel="canonical" href="https://freshwlnd.github.io/2025/09/03/ai/ai-infra-frameworks-introduction/"><meta name="robots" content="index,follow"><meta name="googlebot" content="index,follow"><meta name="revisit-after" content="1 days"><meta name="description" content="介绍AI推理基础设施框架的发展历程"><meta property="og:type" content="article"><meta property="og:title" content="【AI】AI-Infra框架初识：从vLLM到SGLang、Aibrix与Mooncake的性能革命"><meta property="og:url" content="https://freshwlnd.github.io/2025/09/03/ai/ai-infra-frameworks-introduction/index.html"><meta property="og:site_name" content="Fre5h1nd&#39;s Blog"><meta property="og:description" content="介绍AI推理基础设施框架的发展历程"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic1.zhimg.com/v2-9c4fc47e5c35538026efc1d247d5ea4c_1440w.jpg"><meta property="og:image" content="https://pic2.zhimg.com/v2-6b57ab25c4f74cabe76ce186d7f630a9_1440w.jpg"><meta property="article:published_time" content="2025-09-03T06:44:55.000Z"><meta property="article:modified_time" content="2025-09-05T02:58:47.467Z"><meta property="article:author" content="Fre5h1nd"><meta property="article:tag" content="AI基础设施"><meta property="article:tag" content="推理优化"><meta property="article:tag" content="vLLM"><meta property="article:tag" content="SGLang"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://pic1.zhimg.com/v2-9c4fc47e5c35538026efc1d247d5ea4c_1440w.jpg"><script src="https://www.googletagmanager.com/gtag/js?id=G-GGER3VDLBF"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-GGER3VDLBF")</script><link rel="icon" type="image/png" href="/images/favicon.png" sizes="192x192"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.png"><meta name="theme-color" content="#A31F34"><link rel="shortcut icon" href="/images/favicon.png"><title>【AI】AI-Infra框架初识：从vLLM到SGLang、Aibrix与Mooncake的性能革命 | Fre5h1nd&#39;s Blog</title><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fonts/Chillax/chillax.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/css/build/tailwind.css"><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fonts/GeistMono/geist-mono.css"><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fonts/Geist/geist.css"><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/anime.min.js"></script><script id="hexo-configurations">window.config={hostname:"freshwlnd.github.io",root:"/",language:"zh-CN",path:"search.xml"},window.theme={articles:{style:{font_size:"16px",line_height:1.5,image_border_radius:"14px",image_alignment:"center",image_caption:!0,link_icon:!0,delete_mask:!1,title_alignment:"left",headings_top_spacing:{h1:"3.2rem",h2:"2.4rem",h3:"1.9rem",h4:"1.6rem",h5:"1.4rem",h6:"1.3rem"}},word_count:{enable:!0,count:!0,min2read:!0},author_label:{enable:!0,auto:!0,list:[]},code_block:{copy:!0,style:"mac",highlight_theme:{light:"github",dark:"vs2015"},font:{enable:!1,family:null,url:null}},toc:{enable:!0,max_depth:3,number:!0,expand:!0,init_open:!0},copyright:{enable:!0,default:"cc_by_nc_sa"},lazyload:!0,pangu_js:!1,recommendation:{enable:!0,title:"推荐阅读",limit:3,mobile_limit:2,placeholder:"/images/wallhaven-wqery6-light.webp",skip_dirs:[]}},colors:{primary:"#A31F34",secondary:null,default_mode:"light"},global:{fonts:{chinese:{enable:!1,family:null,url:null},english:{enable:!1,family:null,url:null},title:{enable:!1,family:null,url:null}},content_max_width:"1000px",sidebar_width:"210px",hover:{shadow:!0,scale:!0},scroll_progress:{bar:!0,percentage:!0},website_counter:{url:"https://cn.vercount.one/js",enable:!0,site_pv:!0,site_uv:!0,post_pv:!0},single_page:!0,preloader:!0,side_tools:{gear_rotation:!0,auto_expand:!1},open_graph:!0,google_analytics:{enable:!0,id:"G-GGER3VDLBF"},pjax:!0},home_banner:{enable:!0,style:"fixed",image:{light:"/images/wallhaven-wqery6-light.webp",dark:"/images/wallhaven-wqery6-dark.webp"},title:"Fre5h1nd's Blog",subtitle:{text:["Loading..."],hitokoto:{enable:!0,show_author:!1,api:"https://v1.hitokoto.cn?c=d&c=i&c=k"},typing_speed:100,backing_speed:80,starting_delay:500,backing_delay:1500,loop:!0,smart_backspace:!0},text_color:{light:"#fff",dark:"#d1d1b6"},text_style:{title_size:"2.8rem",subtitle_size:"1.5rem",line_height:1.2},custom_font:{enable:!1,family:null,url:null},social_links:{enable:!0,style:"default",links:{github:"https://github.com/Freshwlnd",instagram:null,zhihu:null,twitter:null,email:"xuyh@tongji.edu.cn"},qrs:{weixin:null}}},plugins:{feed:{enable:!0},aplayer:{enable:!0,type:"fixed",audios:[{name:"插叙人生（Montage）",artist:"Y.Z.H / 李晨曦Chrisulous",url:"http://music.163.com/song/media/outer/url?id=2071177415.mp3",cover:"https://p2.music.126.net/KrsOhbsXgsVtFi0C2w320g==/109951168823736894.jpg?imageView&thumbnail=360y360&quality=75&tostatic=0",lrc:null}]},mermaid:{enable:!0,version:"11.4.1"}},version:"2.8.5",navbar:{auto_hide:!1,color:{left:"#f78736",right:"#367df7",transparency:35},width:{home:"1200px",pages:"1000px"},links:{Home:{path:"/",icon:"fa-regular fa-house"},Archives:{path:"/archives",icon:"fa-regular fa-archive"},Tags:{path:"/tags",icon:"fa-regular fa-tags"},Categories:{path:"/categories",icon:"fa-regular fa-folder"},shuoshuo:{path:"/essays",icon:"fa-regular fa-seedling"},images:{path:"/masonry",icon:"fa-regular fa-image"},About:{icon:"fa-regular fa-user",submenus:{Me:"/about",Github:"https://github.com/Freshwlnd",Blog:"https://freshwlnd.github.io",Friends:"/links"}}},search:{enable:!0,preload:!0}},page_templates:{friends_column:2,tags_style:"blur"},home:{sidebar:{enable:!0,position:"left",first_item:"menu",announcement:"欢迎留下足迹",show_on_mobile:!0,links:{Archives:{path:"/archives",icon:"fa-regular fa-archive"},Tags:{path:"/tags",icon:"fa-regular fa-tags"},Categories:{path:"/categories",icon:"fa-regular fa-folder"}}},article_date_format:"auto",excerpt_length:200,categories:{enable:!0,limit:3},tags:{enable:!0,limit:3}},footerStart:"2018/07/04 21:00:00"},window.lang_ago={second:"%s 秒前",minute:"%s 分钟前",hour:"%s 小时前",day:"%s 天前",week:"%s 周前",month:"%s 个月前",year:"%s 年前"},window.data={masonry:!0}</script><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/fontawesome.min.css"><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/brands.min.css"><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/solid.min.css"><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/regular.min.css"><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Fre5h1nd's Blog" type="application/atom+xml">
</head><body><div class="progress-bar-container"><span class="scroll-progress-bar"></span> <span class="pjax-progress-bar"></span></div><style>:root{--preloader-background-color:#fff;--preloader-text-color:#000}@media (prefers-color-scheme:dark){:root{--preloader-background-color:#202124;--preloader-text-color:#fff}}@media (prefers-color-scheme:light){:root{--preloader-background-color:#fff;--preloader-text-color:#000}}@media (max-width:600px){.ml13{font-size:2.6rem!important}}.preloader{display:flex;flex-direction:column;gap:1rem;align-items:center;justify-content:center;position:fixed;padding:12px;top:0;right:0;bottom:0;left:0;width:100vw;height:100vh;background-color:var(--preloader-background-color);z-index:1100;transition:opacity .2s ease-in-out}.ml13{font-size:3.2rem;color:var(--preloader-text-color);letter-spacing:-1px;font-weight:500;font-family:Chillax-Variable,sans-serif;text-align:center}.ml13 .word{display:inline-flex;flex-wrap:wrap;white-space:nowrap}.ml13 .letter{display:inline-block;line-height:1em}</style><div class="preloader"><h2 class="ml13">Fre5h1nd&#39;s Blog</h2><script>var textWrapper = document.querySelector('.ml13');
        // Split text into words
        var words = textWrapper.textContent.trim().split(' ');

        // Clear the existing content
        textWrapper.innerHTML = '';

        // Wrap each word and its letters in spans
        words.forEach(function(word) {
            var wordSpan = document.createElement('span');
            wordSpan.classList.add('word');
            wordSpan.innerHTML = word.replace(/\S/g, "<span class='letter'>$&</span>");
            textWrapper.appendChild(wordSpan);
            textWrapper.appendChild(document.createTextNode(' ')); // Add space between words
        });

        var animation = anime.timeline({ loop: true })
            .add({
                targets: '.ml13 .letter',
                translateY: [20, 0],
                translateZ: 0,
                opacity: [0, 1],
                filter: ['blur(5px)', 'blur(0px)'],
                easing: "easeOutExpo",
                duration: 1200,
                delay: (el, i) => 300 + 20 * i,
            })
            .add({
                targets: '.ml13 .letter',
                translateY: [0, -20],
                opacity: [1, 0],
                filter: ['blur(0px)', 'blur(5px)'],
                easing: "easeInExpo",
                duration: 1000,
                delay: (el, i) => 15 * i,
                complete: function() {
                    hidePreloader();
                }
            }, '-=700');


        let themeStatus = JSON.parse(localStorage.getItem('REDEFINE-THEME-STATUS'))?.isDark;

        // If the theme status is not found in local storage, check the preferred color scheme
        if (themeStatus === undefined || themeStatus === null) {
            if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                themeStatus = 'dark';
            } else {
                themeStatus = 'light';
            }
        }

        // Now you can use the themeStatus variable in your code
        if (themeStatus) {
            document.documentElement.style.setProperty('--preloader-background-color', '#202124');
            document.documentElement.style.setProperty('--preloader-text-color', '#fff');
        } else {
            document.documentElement.style.setProperty('--preloader-background-color', '#fff');
            document.documentElement.style.setProperty('--preloader-text-color', '#000');
        }

        window.addEventListener('load', function () {
            setTimeout(hidePreloader, 5000); // Call hidePreloader after 5000 milliseconds if not already called by animation
        });

        function hidePreloader() {
            var preloader = document.querySelector('.preloader');
            preloader.style.opacity = '0';
            setTimeout(function () {
                preloader.style.display = 'none';
            }, 200);
        }</script></div><main class="page-container" id="swup"><div class="main-content-container flex flex-col justify-between min-h-dvh"><div class="main-content-header"><header class="navbar-container px-6 md:px-12"><div class="navbar-content transition-navbar"><div class="left"><a class="logo-title" href="/">Fre5h1nd&#39;s Blog</a></div><div class="right"><div class="desktop"><ul class="navbar-list"><li class="navbar-item"><a href="/"><i class="fa-regular fa-house fa-fw"></i> 首页</a></li><li class="navbar-item"><a href="/archives"><i class="fa-regular fa-archive fa-fw"></i> 归档</a></li><li class="navbar-item"><a href="/tags"><i class="fa-regular fa-tags fa-fw"></i> 标签</a></li><li class="navbar-item"><a href="/categories"><i class="fa-regular fa-folder fa-fw"></i> 分类</a></li><li class="navbar-item"><a href="/essays"><i class="fa-regular fa-seedling fa-fw"></i> 灵感</a></li><li class="navbar-item"><a href="/masonry"><i class="fa-regular fa-image fa-fw"></i> 图库</a></li><li class="navbar-item"><a class="has-dropdown" href="#" onclick="&#34;return" false;&#34;><i class="fa-regular fa-user fa-fw"></i> 关于 <i class="fa-solid fa-chevron-down fa-fw"></i></a><ul class="sub-menu"><li><a href="/about">ME</a></li><li><a target="_blank" rel="noopener" href="https://github.com/Freshwlnd">GITHUB</a></li><li><a href="https://freshwlnd.github.io">BLOG</a></li><li><a href="/links">友情链接</a></li></ul></li><li class="navbar-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></li></ul></div><div class="mobile"><div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div><div class="icon-item navbar-bar"><div class="navbar-bar-middle"></div></div></div></div></div><div class="navbar-drawer h-dvh w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between"><ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start"><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/"><span>首页 </span><i class="fa-regular fa-house fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/archives"><span>归档 </span><i class="fa-regular fa-archive fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/tags"><span>标签 </span><i class="fa-regular fa-tags fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/categories"><span>分类 </span><i class="fa-regular fa-folder fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/essays"><span>灵感 </span><i class="fa-regular fa-seedling fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/masonry"><span>图库 </span><i class="fa-regular fa-image fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item-sub text-base my-1.5 flex flex-col w-full"><div class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary cursor-pointer text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" navbar-data-toggle="submenu-About"><span>关于 </span><i class="fa-solid fa-chevron-right fa-sm fa-fw transition-all"></i></div><div class="flex-col items-start px-2 py-2 hidden" data-target="submenu-About"><div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl"><a class="text-third-text-color text-xl" href="/about">ME</a></div><div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl"><a class="text-third-text-color text-xl" target="_blank" rel="noopener" href="https://github.com/Freshwlnd">GITHUB</a></div><div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl"><a class="text-third-text-color text-xl" href="https://freshwlnd.github.io">BLOG</a></div><div class="drawer-navbar-item text-base flex flex-col justify-center items-start hover:underline active:underline hover:underline-offset-1 rounded-3xl"><a class="text-third-text-color text-xl" href="/links">友情链接</a></div></div></li></ul><div class="statistics flex justify-around my-2.5"><a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags"><div class="number text-2xl sm:text-xl text-second-text-color font-semibold">197</div><div class="label text-third-text-color text-sm">标签</div></a><a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories"><div class="number text-2xl sm:text-xl text-second-text-color font-semibold">35</div><div class="label text-third-text-color text-sm">分类</div></a><a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives"><div class="number text-2xl sm:text-xl text-second-text-color font-semibold">161</div><div class="label text-third-text-color text-sm">文章</div></a></div></div><div class="window-mask"></div></header></div><div class="main-content-body transition-fade-up"><div class="main-content"><div class="post-page-container flex relative justify-between box-border w-full h-full"><div class="article-content-container"><div class="article-title relative w-full"><img src="https://github.com/Freshwlnd/image/blob/blog/AI-Infra.png?raw=true" alt="【AI】AI-Infra框架初识：从vLLM到SGLang、Aibrix与Mooncake的性能革命" class="w-full h-60 sm:h-72 md:h-80 object-cover sm:rounded-t-large dark:brightness-75"><div class="w-full flex items-center absolute bottom-0 justify-start"><h1 class="article-title-cover text-center mx-6 my-6 text-second-text-color bg-background-color-transparent px-4 py-3 text-3xl sm:text-4xl md:text-5xl font-semibold backdrop-blur-lg rounded-xl border border-border-color">【AI】AI-Infra框架初识：从vLLM到SGLang、Aibrix与Mooncake的性能革命</h1></div></div><div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8"><div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]"><img src="/images/MessAround.jpeg"></div><div class="info flex flex-col justify-between"><div class="author flex items-center"><span class="name text-default-text-color text-lg font-semibold">Fre5h1nd</span> <span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv6</span></div><div class="meta-info"><div class="article-meta-info"><span class="article-date article-meta-item"><i class="fa-regular fa-pen-fancy"></i>&nbsp; <span class="desktop">2025-09-03 14:44:55</span> <span class="mobile">2025-09-03 14:44:55</span> <span class="hover-info">创建</span> </span><span class="article-date article-meta-item"><i class="fa-regular fa-wrench"></i>&nbsp; <span class="desktop">2025-09-05 10:58:47</span> <span class="mobile">2025-09-05 10:58:47</span> <span class="hover-info">更新</span> </span><span class="article-categories article-meta-item"><i class="fa-regular fa-folders"></i>&nbsp;<ul><li><a href="/categories/%E6%8A%80%E6%9C%AF/">技术</a>&nbsp;</li><li>></li><li><a href="/categories/%E6%8A%80%E6%9C%AF/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>&nbsp;</li></ul></span><span class="article-tags article-meta-item"><i class="fa-regular fa-tags"></i>&nbsp;<ul><li><a href="/tags/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/">AI基础设施</a>&nbsp;</li><li>| <a href="/tags/%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/">推理优化</a>&nbsp;</li><li>| <a href="/tags/vLLM/">vLLM</a>&nbsp;</li><li>| <a href="/tags/SGLang/">SGLang</a>&nbsp;</li></ul></span><span class="article-wordcount article-meta-item"><i class="fa-regular fa-typewriter"></i>&nbsp;<span>4k 字</span> </span><span class="article-min2read article-meta-item"><i class="fa-regular fa-clock"></i>&nbsp;<span>14 分钟</span> </span><span class="article-pv article-meta-item"><i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span></span></div></div></div></div><div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8"><h1 id="背景简介：为何需要AI-Infra框架？从大模型推理的痛点说起"><a href="#背景简介：为何需要AI-Infra框架？从大模型推理的痛点说起" class="headerlink" title="背景简介：为何需要AI-Infra框架？从大模型推理的痛点说起"></a>背景简介：<strong>为何需要AI-Infra框架？从大模型推理的痛点说起</strong></h1><p>大语言模型（LLM）的兴起使其从科研领域走向了实际应用。这些强大的模型在实际部署和应用中，需要专门的“AI基础设施框架”（AI-Infra）来保障其高效、稳定地运行。这些框架是LLM从研究走向工业级应用的关键。</p><blockquote><p>上个月sglang-v0.3.0和vllm-v0.6.0前后脚发布之后（注：该文章发布时间为2024.10），就一直想总结梳理一下现在主流的大模型推理引擎。因为我觉得这也算是一个有意义的节点吧，从此开源大模型推理引擎总算是由”非常粗糙，但是能用”的阶段迈入到了”好用，稍微有那么点粗糙”的阶段。</p><p>大模型的推理引擎实际也就是近一两年才开始飞速发展，从最开始的tgi和vllm并驾齐驱到如今sglang、lmdeply的异军突起，整个开源社区都是非常有活力的。</p><p>但是正如之前所说，从长远的一个视角看如今的开源引擎实际上都还是比较粗糙的，大家都是在摸索中前进。另一方面也是因为现在全世界的目光都聚焦在llm这里，新技术的更新换代太快了，做好一个大模型的推理引擎要做的事情实在是太太太太多了。除了要支持日新月异的<strong>新模型和新硬件</strong>，还要不断关心学术界最新的paper并且想方设法落地实现。而这些新的想法可能涉及到<strong>模型结构、计算策略、调度策略、存储策略、cuda内核、硬件加速</strong>等各个层级，这就需要开发者有非常广泛的知识范围和过硬的工程能力。</p><p>我一直认为大模型推理引擎最难的地方就在于：对模型和硬件的广泛支持以及如何将各种角度的不同优化方法兼容实现。因为写paper的人可以只关心他自己的idea，在transformer库的基础上写个简单demo就行，但是在推理引擎里落地的时候往往就会与其它模块有冲突，需要想办法去做各种兼容。退一步说，即使没有冲突的情况，你也需要对其他基础的优化比较熟悉，你才能在这些的基础上完成新功能的开发。</p><p>——开源大模型推理引擎现状及常见推理优化方法 - 齐夏的文章 - 知乎 <a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/755874470">https://zhuanlan.zhihu.com/p/755874470<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p></blockquote><p>在AI-Infra框架出现之前，大语言模型的实际部署和应用，尤其是推理环节，面临着一系列严峻的挑战。这些挑战不仅关乎性能，也关乎成本和效率，它们是催生AI-Infra框架的根本原因。</p><h2 id="1-1-GPU显存挑战：显存占用与内存碎片化"><a href="#1-1-GPU显存挑战：显存占用与内存碎片化" class="headerlink" title="1.1 GPU显存挑战：显存占用与内存碎片化"></a><strong>1.1 GPU显存挑战：显存占用与内存碎片化</strong></h2><p>大语言模型的核心是Transformer架构，其推理过程大致分为两个阶段：预填充（Prefill）和解码（Decode）。在解码阶段，模型需要逐个生成新的词元（token），而每一次生成，都需要访问前面所有词元的“键值缓存”（KV Cache），这部分数据占据了大量的GPU显存<a href="#refer-anchor-1"><sup>[1]</sup></a>。随着模型规模和用户请求数量的增加，GPU的显存常常爆满<a href="#refer-anchor-1"><sup>[1]</sup></a>。</p><p>更棘手的问题是内存碎片化。传统的推理方法在为每个用户会话分配KV Cache时，会预留一个连续的、固定的内存块。然而，用户请求的文本长度是动态的，当会话结束或内容比预分配的短时，就会产生大量无法被其他请求利用的零散显存碎片。这导致了显存资源的巨大浪费，严重影响了GPU的利用率。</p><h2 id="1-2-吞吐量挑战：处理大规模并发请求的挑战"><a href="#1-2-吞吐量挑战：处理大规模并发请求的挑战" class="headerlink" title="1.2 吞吐量挑战：处理大规模并发请求的挑战"></a><strong>1.2 吞吐量挑战：处理大规模并发请求的挑战</strong></h2><p>除了显存问题，另一个核心挑战是吞吐量瓶颈，即单位时间内能够处理的请求数量。传统的推理架构在处理并发请求时效率低下。例如，串行批处理（static batching）会等待一组请求全部完成输入后，再一起进行推理计算。如果批处理中的某个请求很长，其他所有请求都必须等待它完成，这导致了GPU计算资源在大部分时间内处于闲置状态，整体吞吐量无法有效提升<a href="#refer-anchor-1"><sup>[2]</sup></a>。</p><p>此外，传统的无状态推理架构在处理LLM应用时面临性能瓶颈：每次请求被随机路由到不同的计算实例，导致KV Cache无法有效复用、多轮对话上下文频繁重建、系统提示词重复处理，这严重影响了用户体验和系统效率<a href="#refer-anchor-1"><sup>[3]</sup></a>。这表明，通用的云计算架构在LLM这种特定工作负载面前，不再是最优解。</p><h2 id="1-3-复杂应用场景挑战：从简单对话到程序化调用"><a href="#1-3-复杂应用场景挑战：从简单对话到程序化调用" class="headerlink" title="1.3 复杂应用场景挑战：从简单对话到程序化调用"></a><strong>1.3 复杂应用场景挑战：从简单对话到程序化调用</strong></h2><p>早期的大模型应用以简单的单轮对话为主。但随着应用的发展，LLM的使用方式变得更加复杂，例如LLM参与多轮规划、推理以及与外部环境的交互等场景<a href="#refer-anchor-1"><sup>[4]</sup></a>。这些新的使用模式不再是简单的单轮对话形式，而是需要包含多个LLM调用，这些调用之间穿插着控制流，并且需要接收和产生结构化的输入和输出（比如JSON格式）<a href="#refer-anchor-1"><sup>[4]</sup></a>。</p><p>传统的推理引擎主要针对单次、无状态的推理进行优化，难以高效地处理这种复杂的“程序化调用”（LM Programs）范式。开发者必须在外部手动管理状态、编排调用顺序，这不仅繁琐，而且难以实现端到端的性能优化<a href="#refer-anchor-1"><sup>[4]</sup></a>。</p><h1 id="脉络梳理：AI-Infra框架的演进脉络与核心解法"><a href="#脉络梳理：AI-Infra框架的演进脉络与核心解法" class="headerlink" title="脉络梳理：AI-Infra框架的演进脉络与核心解法"></a>脉络梳理：<strong>AI-Infra框架的演进脉络与核心解法</strong></h1><p>针对前面提到的三大痛点，AI-Infra框架领域出现了一系列解决方案。</p><h2 id="2-1-vLLM：内存管理上的创新"><a href="#2-1-vLLM：内存管理上的创新" class="headerlink" title="2.1 vLLM：内存管理上的创新"></a><strong>2.1 vLLM：内存管理上的创新</strong></h2><blockquote><p>vllm原本只是作为PagedAttention的一个开源实现，但发展到今天已经成为llm推理引擎的标杆了。</p></blockquote><p>vLLM是AI-Infra框架中一个代表性的框架<a href="#refer-anchor-1"><sup>[5]</sup></a>，团队来自UC Berkeley。</p><ul><li><p><strong>技术：</strong>&#x5B83;率先对底层推理效率进行了优化。vLLM的核心贡献在于其独创的 <strong>PagedAttention</strong>技术，这一技术旨在高效管理注意力键和值的内存<a href="#refer-anchor-1"><sup>[6]</sup></a>。vLLM将KV Cache分割成多个离散的“块”（block），这些“块”可以根据需要动态地分配和管理。通过这种方式，vLLM解决了KV Cache显存碎片化的问题，显著提高了显存的利用率，使得GPU能够同时处理更多的并发请求，从而大幅提升了整体吞吐量<a href="#refer-anchor-1"><sup>[6]</sup></a>。</p></li><li><p><strong>优势：</strong>&#x76;LLM有着大量且稳定的开发者，Github上Contributors已经1500+人了，相比于SGLang的663人、Aibrix的75人、TensorRT的79人、Mooncake的84人，vLLM的开发人员投入是最高的。因此vLLM对模型的支持和对硬件的支持都是最完善的，以及各种功能也往往是最齐全的。<a href="#refer-anchor-1"><sup>[14]</sup></a></p></li></ul><p>vLLM的出现，让大模型的在线服务效率达到了一个全新的高度，也为后续的框架发展奠定了基础<a href="#refer-anchor-1"><sup>[7]</sup></a>。目前，vLLM的社区活跃度是最高的，github上issue和pr都很多，且大量paper都是以vLLM作为baseline来开发demo。</p><h2 id="2-2-SGLang：面向复杂应用场景的编程范式"><a href="#2-2-SGLang：面向复杂应用场景的编程范式" class="headerlink" title="2.2 SGLang：面向复杂应用场景的编程范式"></a><strong>2.2 SGLang：面向复杂应用场景的编程范式</strong></h2><p>SGLang也来自UC Berkeley，但是跟vLLM是不同的一拨人，核心团队基本都是交大的<a href="#refer-anchor-1"><sup>[14]</sup></a>（另有说法为：很多人都是vLLM的作者<a href="#refer-anchor-1"><sup>[4]</sup></a>）。</p><p>当vLLM解决了底层的显存和吞吐量问题后，SGLang将关注点提升到了更高层面的应用场景<a href="#refer-anchor-1"><sup>[4]</sup></a>。它专注于解决前文提到的“复杂应用场景：程序化调用”挑战，即如何让开发者能够像编写传统软件一样，编排复杂的LLM应用逻辑<a href="#refer-anchor-1"><sup>[4]</sup></a>。</p><ul><li><p><strong>技术：</strong>&#x53;GLang的核心思想是采用一种<strong>编译器设计</strong>的理念。它引入了一个“前端语言”和“后端运行时”协同设计的模式，允许开发者在框架内直接编写多步LLM调用和控制流，例如循环和条件判断<a href="#refer-anchor-1"><sup>[8]</sup></a>。这使得LLM能够更高效地处理工具使用、多轮推理和结构化生成等任务<a href="#refer-anchor-1"><sup>[8]</sup></a>。SGLang的这种设计，可以从根本上优化多对多的输入输出，并进行端到端的性能优化<a href="#refer-anchor-1"><sup>[4]</sup></a>。SGLang通过其RadixAttention技术，实现了对KV Cache的高效复用<a href="#refer-anchor-1"><sup>[8]</sup></a>。</p></li><li><p><strong>优势：</strong>&#x53;GLang的代码可拓展性很高，主流功能都有支持的情况下，代码比vLLM清晰简单很多，这对于二次开发来说是很重要的。社区活跃度虽然比不上vLLM，但是作者都很积极地回复issue。<a href="#refer-anchor-1"><sup>[14]</sup></a></p></li></ul><p>SGLang的出现，标志着AI-Infra框架开始从单纯的“性能优化”走向“应用范式创新”，它让LLM成为了一个可以被深度集成到复杂软件系统中的“计算单元”<a href="#refer-anchor-1"><sup>[4]</sup></a>。</p><h2 id="2-3-Aibrix与Mooncake：面向大规模部署的系统级创新"><a href="#2-3-Aibrix与Mooncake：面向大规模部署的系统级创新" class="headerlink" title="2.3 Aibrix与Mooncake：面向大规模部署的系统级创新"></a><strong>2.3 Aibrix与Mooncake：面向大规模部署的系统级创新</strong></h2><p>当LLM的应用从单机走向大规模集群部署时，新的挑战随之出现。vLLM和SGLang主要解决了单机或少量GPU环境下的效率问题，但面对大规模集群，就需要新的系统级解决方案。Aibrix和Mooncake的出现正是为了解决这一问题，它们将关注点从“引擎内部”转移到了“集群系统层面”<a href="#refer-anchor-1"><sup>[10]</sup></a>。</p><p><strong>Aibrix（来自字节跳动）</strong>&#x662F;一个云原生的开源框架，其核心使命是简化和优化大规模LLM在云环境中的部署<a href="#refer-anchor-1"><sup>[11]</sup></a>。它并非一个全新的推理引擎，而是一个协同vLLM等引擎运行的“控制平面”（Control Plane）<a href="#refer-anchor-1"><sup>[5]</sup></a>。Aibrix负责集群层面的资源调度、自适应扩缩容、负载均衡以及智能路由等任务<a href="#refer-anchor-1"><sup>[11]</sup></a>。根据一项实验数据，Aibrix的扩缩容响应时间可加速82%<a href="#refer-anchor-1"><sup>[5]</sup></a>。此外，Aibrix还引入了针对低秩适配（LoRA）模型的高密度管理，支持动态调度和加载LoRA适配器<a href="#refer-anchor-1"><sup>[11]</sup></a>。</p><p><strong>Mooncake（为Kimi服务的平台，由MoonshotAI提供，论文获FAST’25最佳论文奖）</strong>&#x5219;是一个专门为LLM推理场景设计的<strong>分布式KV Cache存储系统</strong><a href="#refer-anchor-1"><sup>[10]</sup></a>。它解决了在集群环境中，KV Cache无法在不同计算节点之间高效共享和复用的问题。Mooncake的核心是其“全局缓存+分离式推理架构”（KVCache-centric disaggregated architecture），它将预填充和解码的计算集群与KV Cache的存储集群分离<a href="#refer-anchor-1"><sup>[12]</sup></a>。通过聚合集群中未被充分利用的CPU、DRAM甚至SSD资源，Mooncake形成了一个统一的分布式内存池，供所有节点共享KV Cache<a href="#refer-anchor-1"><sup>[10]</sup></a>。这种设计使得计算资源可以根据负载动态增减，而KV Cache则可以在独立的存储池中持久化，并被所有节点复用，这对于长上下文、多轮对话场景尤其重要，能显著提升吞吐量和资源利用率<a href="#refer-anchor-1"><sup>[12]</sup></a>。</p><p>Aibrix和Mooncake的出现，反映了LLM应用已经进入大规模工业化生产阶段，关注点从单纯的性能，扩展到了成本、可扩展性和服务质量。</p><p></p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://pic1.zhimg.com/v2-9c4fc47e5c35538026efc1d247d5ea4c_1440w.jpg" alt="图1：Airbrix"><figcaption>图1：Airbrix</figcaption></figure><p></p><p></p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://pic2.zhimg.com/v2-6b57ab25c4f74cabe76ce186d7f630a9_1440w.jpg" alt="图2：Mooncake"><figcaption>图2：Mooncake</figcaption></figure><p></p><h1 id="框架横向对比：各自的定位与优劣"><a href="#框架横向对比：各自的定位与优劣" class="headerlink" title="框架横向对比：各自的定位与优劣"></a><strong>框架横向对比：各自的定位与优劣</strong></h1><p>通过以上分析，我们可以看到AI-Infra框架的主要生态系统。为了更清晰地理解它们的定位和特点，本节将通过表格形式对几个主要框架进行对比。同时，我们还引入一个来自硬件厂商的代表——NVIDIA的TensorRT-LLM，来展示不同的技术路径。</p><h2 id="主流AI-Infra框架能力对比"><a href="#主流AI-Infra框架能力对比" class="headerlink" title="主流AI-Infra框架能力对比"></a><strong>主流AI-Infra框架能力对比</strong></h2><table><thead><tr><th><strong>框架名称</strong></th><th><strong>核心解决问题</strong></th><th><strong>关键技术</strong></th><th><strong>典型应用场景</strong></th><th><strong>优点</strong></th><th><strong>局限性</strong></th></tr></thead><tbody><tr><td><strong>vLLM</strong></td><td>单机显存管理</td><td>PagedAttention，连续批处理</td><td>高性能API服务，单机部署</td><td>吞吐量高，易用性强，社区活跃<a href="#refer-anchor-1"><sup>[6]</sup></a></td><td>主要为单机引擎，集群扩展能力有限</td></tr><tr><td><strong>SGLang</strong></td><td>复杂应用编程与结构化生成</td><td>编译器设计，RadixAttention</td><td>复杂Agent，工具调用，多轮对话</td><td>支持复杂逻辑编排，编程范式友好<a href="#refer-anchor-1"><sup>[8]</sup></a></td><td>相对vLLM，底层性能优化空间可能略小</td></tr><tr><td><strong>Aibrix</strong></td><td>大规模集群资源管理与扩展</td><td>LLM专用自适应扩缩容，高效LoRA管理</td><td>大规模企业级生产环境部署</td><td>系统级优化，弹性高，降低成本<a href="#refer-anchor-1"><sup>[5]</sup></a></td><td>非核心推理引擎，需与vLLM等配合使用<a href="#refer-anchor-1"><sup>[5]</sup></a></td></tr><tr><td><strong>Mooncake</strong></td><td>分布式KV Cache与长上下文</td><td>KVCache分离式架构</td><td>长上下文场景，多机KV Cache共享</td><td>高效利用集群资源，支持超长上下文<a href="#refer-anchor-1"><sup>[12]</sup></a></td><td>纯缓存系统，需与引擎配合使用<a href="#refer-anchor-1"><sup>[10]</sup></a></td></tr><tr><td><strong>TensorRT-LLM</strong></td><td>极致单机性能与低延迟</td><td>量化，层/张量融合，CUDA内核优化</td><td>实时交互应用，边缘设备部署</td><td>性能高，延迟低，针对NVIDIA硬件深度优化<a href="#refer-anchor-1"><sup>[13]</sup></a></td><td>强硬件（NVIDIA）依赖性，通用性差<a href="#refer-anchor-1"><sup>[13]</sup></a></td></tr></tbody></table><p>从上表可以看出，这些框架并非相互替代，而是在不同层级上进行互补。</p><ul><li><p>vLLM和SGLang是“引擎层”的框架，专注于模型执行效率和应用逻辑；</p></li><li><p>而Aibrix和Mooncake则是“系统层”的框架，专注于集群管理和资源调度；</p></li><li><p>TensorRT-LLM则代表了一种由硬件厂商主导的、从底层进行优化的路径，它通过对NVIDIA硬件的深度适配，实现了超高的性能，但代价是牺牲了通用性和跨硬件的兼容性<a href="#refer-anchor-1"><sup>[13]</sup></a>。</p></li></ul><p>这种分层发展的趋势，反映了AI-Infra领域发展的成熟度。当底层引擎的性能问题得到解决后，开发者们会将目光投向更高层面的应用编程和大规模部署，而这些新挑战又催生了新一轮的框架创新。</p><hr><hr><ul><li>希望这篇博客对你了解AI-Infra框架有所帮助！如果你有任何问题或需要进一步的讨论，欢迎随时交流。</li><li>如果你喜欢这篇文章，欢迎<a class="link" target="_blank" rel="noopener" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="🗺️参考文献"><a href="#🗺️参考文献" class="headerlink" title="🗺️参考文献"></a>🗺️参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" target="_blank" rel="noopener" href="https://ppio.com/blogs/post/da-mo-xing-tui-li-cheng-ben-mei-nian-jiang-di-10bei-de-mi-mi-yi-wen-liao-jie-vllm-sglangdeng-zhu-liu-tui-li-yin-qing">[1] 大模型推理成本每年降低10倍的秘密：一文了解vLLM、SGLang等主流推理引擎 - PPIO<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/lqfarmer/article/details/140906949">[2] 从vLLM到大模型推理的最新进展_vllm复现-CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://aws.amazon.com/cn/blogs/china/accelerating-inference-on-llm-with-amazon-sagemaker-sticky-sessions/">[3] 利用Amazon SageMaker Sticky Session 实现大语言模型推理加速 | 亚马逊AWS官方博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://aijishu.com/a/1060000000476318">[4] SGLang：LLM推理引擎发展新方向- 极术社区- 连接开发者与智能计算 …<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://www.infoq.cn/article/ncbudc3vvp8kignttiof">[5] 字节跳动开源AIBrix：填补云原生大模型推理“系统层”空白 - InfoQ<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://vllm.hyper.ai/docs/">[6] 欢迎来到vLLM！ | vLLM 中文站<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://www.high-flyer.cn/blog/continuous-batching/">[7] Continuous Batching：一种提升LLM 部署吞吐量的利器 - 幻方<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/2401_85280106/article/details/147835433">[8] 学习笔记：主流大模型框架对比分析（Ollama、vLLM、SGlang …, <i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://qwen.readthedocs.io/zh-cn/latest/deployment/sglang.html">[9] SGLang - Qwen - Read the Docs<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://docs.lmcache.ai/kv_cache/mooncake.html">[10] Mooncake | LMCache<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/html/2504.03648v1">[11] AIBrix: Towards Scalable, Cost-Effective Large Language Model Inference Infrastructure<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://github.com/kvcache-ai/Mooncake">[12] Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI. - GitHub<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://developer.nvidia.cn/tensorrt">[13] NVIDIA TensorRT - NVIDIA 开发者<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/755874470">[14] 2024年-开源大模型推理引擎现状及常见推理优化方法 - 知乎<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://aibrix.readthedocs.io/latest/community/research.html">[15] Research Collaboration - AIBrix - Read the Docs<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://doi.org/10.1145/3600006.3613165">[16] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles (SOSP ‘23). Association for Computing Machinery, New York, NY, USA, 611–626.<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27872556474">[17] 【深度解读FAST’25最佳论文Mooncake】：存储为中心的大语言模型推理架构 - 知乎<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25874756271">[18] 字节跳动开源AIBrix：一个可扩展、经济高效的vLLM控制平面 - 知乎<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p></div><div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8"><div class="article-copyright-info-container"><ul><li><strong>标题:</strong> 【AI】AI-Infra框架初识：从vLLM到SGLang、Aibrix与Mooncake的性能革命</li><li><strong>作者:</strong> Fre5h1nd</li><li><strong>创建于 :</strong> 2025-09-03 14:44:55</li><li><strong>更新于 :</strong> 2025-09-05 10:58:47</li><li><strong>链接:</strong> https://freshwlnd.github.io/2025/09/03/ai/ai-infra-frameworks-introduction/</li><li><strong>版权声明: </strong>本文章采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a> 进行许可。</li></ul></div></div><ul class="post-tags-box text-lg mt-1.5 flex-wrap justify-center flex md:hidden"><li class="tag-item mx-0.5"><a href="/tags/AI%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/">#AI基础设施</a>&nbsp;</li><li class="tag-item mx-0.5"><a href="/tags/%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/">#推理优化</a>&nbsp;</li><li class="tag-item mx-0.5"><a href="/tags/vLLM/">#vLLM</a>&nbsp;</li><li class="tag-item mx-0.5"><a href="/tags/SGLang/">#SGLang</a>&nbsp;</li></ul><div class="recommended-article px-2 sm:px-6 md:px-8"><div class="recommended-desktop"><div class="recommended-article-header text-xl md:text-3xl font-bold mt-10"><i aria-hidden="true"></i><span>推荐阅读</span></div><div class="recommended-article-group"><a class="recommended-article-item" href="/2025/09/02/literature/literatureNotes87/" title="【论文】略读笔记87-经典-vLLM" rel="bookmark"><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ0GxilZUJp0mckpnfJNG719jWj1p6cMNkpZA&s" alt="【论文】略读笔记87-经典-vLLM" class="!max-w-none"> <span class="title">【论文】略读笔记87-经典-vLLM</span> </a><a class="recommended-article-item" href="/2024/09/10/literature/literatureNotes58/" title="【论文】略读笔记58-前沿-大模型抢占式调度" rel="bookmark"><img src="https://simg.baai.ac.cn/papers/converted_page_031ac5c2121fc92d973c8b5eca766e07-04.jpg" alt="【论文】略读笔记58-前沿-大模型抢占式调度" class="!max-w-none"> <span class="title">【论文】略读笔记58-前沿-大模型抢占式调度</span> </a><a class="recommended-article-item" href="/2023/12/08/ai-RL-introduction/" title="【AI】强化学习入门路径及优质资料" rel="bookmark"><img src="https://images.unsplash.com/photo-1553164878-dcde560aaabc?q=80&w=2670&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D" alt="【AI】强化学习入门路径及优质资料" class="!max-w-none"> <span class="title">【AI】强化学习入门路径及优质资料</span></a></div></div><div class="recommended-mobile"><div class="recommended-article-header text-xl md:text-3xl font-bold mt-10"><i aria-hidden="true"></i><span>推荐阅读</span></div><div class="recommended-article-group"><a class="recommended-article-item" href="/2025/09/02/literature/literatureNotes87/" title="【论文】略读笔记87-经典-vLLM" rel="bookmark"><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ0GxilZUJp0mckpnfJNG719jWj1p6cMNkpZA&s" alt="【论文】略读笔记87-经典-vLLM" class="!max-w-none"> <span class="title">【论文】略读笔记87-经典-vLLM</span> </a><a class="recommended-article-item" href="/2024/09/10/literature/literatureNotes58/" title="【论文】略读笔记58-前沿-大模型抢占式调度" rel="bookmark"><img src="https://simg.baai.ac.cn/papers/converted_page_031ac5c2121fc92d973c8b5eca766e07-04.jpg" alt="【论文】略读笔记58-前沿-大模型抢占式调度" class="!max-w-none"> <span class="title">【论文】略读笔记58-前沿-大模型抢占式调度</span></a></div></div></div><div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8"><div class="article-prev border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2"><a class="prev" rel="prev" href="/2025/09/04/k8s/k8s-volcano-create-schedule-contention-analysis/"><span class="left arrow-icon flex justify-center items-center"><i class="fa-solid fa-chevron-left"></i> </span><span class="title flex justify-center items-center"><span class="post-nav-title-item truncate max-w-48">【集群】云原生批调度实战：Volcano 深度解析（五）：CREATE/SCHEDULE 阶段“卡顿”现象解析与协程数优化实验</span> <span class="post-nav-item">上一篇</span></span></a></div><div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2"><a class="next" rel="next" href="/2025/09/02/literature/literatureNotes87/"><span class="title flex justify-center items-center"><span class="post-nav-title-item truncate max-w-48">【论文】略读笔记87-经典-vLLM</span> <span class="post-nav-item">下一篇</span> </span><span class="right arrow-icon flex justify-center items-center"><i class="fa-solid fa-chevron-right"></i></span></a></div></div><div class="comment-container px-2 sm:px-6 md:px-8 pb-8"><div class="comments-container mt-10 w-full"><div id="comment-anchor" class="w-full h-2.5"></div><div class="comment-area-title w-full my-1.5 md:my-2.5 text-xl md:text-3xl font-bold">评论</div><div id="gitalk-container"></div><script data-swup-reload-script src="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js"></script><script data-swup-reload-script>function loadGitalk(){let e=decodeURI(location.pathname);e.length>50&&(e=e.substring(0,47)+"...");try{Gitalk&&new Gitalk({clientID:"4d4a8706873446558037",clientSecret:"72c9ec35e05bda1ed29e5321f912397811568e0a",repo:"GitalkStorehouse",owner:"Freshwlnd",admin:["Freshwlnd"],id:e,language:"zh-CN",proxy:"https://strong-caramel-969805.netlify.app/github_access_token"}).render("gitalk-container")}catch(e){window.Gitalk=null}}{const e=setTimeout(()=>{loadGitalk(),clearTimeout(e)},1e3)}</script></div></div></div><div class="toc-content-container"><div class="post-toc-wrap"><div class="post-toc"><div class="toc-title">目录</div><div class="page-title">【AI】AI-Infra框架初识：从vLLM到SGLang、Aibrix与Mooncake的性能革命</div><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%83%8C%E6%99%AF%E7%AE%80%E4%BB%8B%EF%BC%9A%E4%B8%BA%E4%BD%95%E9%9C%80%E8%A6%81AI-Infra%E6%A1%86%E6%9E%B6%EF%BC%9F%E4%BB%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E7%9A%84%E7%97%9B%E7%82%B9%E8%AF%B4%E8%B5%B7"><span class="nav-number">1.</span> <span class="nav-text">背景简介：为何需要AI-Infra框架？从大模型推理的痛点说起</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-GPU%E6%98%BE%E5%AD%98%E6%8C%91%E6%88%98%EF%BC%9A%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E4%B8%8E%E5%86%85%E5%AD%98%E7%A2%8E%E7%89%87%E5%8C%96"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 GPU显存挑战：显存占用与内存碎片化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E5%90%9E%E5%90%90%E9%87%8F%E6%8C%91%E6%88%98%EF%BC%9A%E5%A4%84%E7%90%86%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%B9%B6%E5%8F%91%E8%AF%B7%E6%B1%82%E7%9A%84%E6%8C%91%E6%88%98"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 吞吐量挑战：处理大规模并发请求的挑战</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-%E5%A4%8D%E6%9D%82%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E6%8C%91%E6%88%98%EF%BC%9A%E4%BB%8E%E7%AE%80%E5%8D%95%E5%AF%B9%E8%AF%9D%E5%88%B0%E7%A8%8B%E5%BA%8F%E5%8C%96%E8%B0%83%E7%94%A8"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 复杂应用场景挑战：从简单对话到程序化调用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%84%89%E7%BB%9C%E6%A2%B3%E7%90%86%EF%BC%9AAI-Infra%E6%A1%86%E6%9E%B6%E7%9A%84%E6%BC%94%E8%BF%9B%E8%84%89%E7%BB%9C%E4%B8%8E%E6%A0%B8%E5%BF%83%E8%A7%A3%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">脉络梳理：AI-Infra框架的演进脉络与核心解法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-vLLM%EF%BC%9A%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B8%8A%E7%9A%84%E5%88%9B%E6%96%B0"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 vLLM：内存管理上的创新</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-SGLang%EF%BC%9A%E9%9D%A2%E5%90%91%E5%A4%8D%E6%9D%82%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E7%9A%84%E7%BC%96%E7%A8%8B%E8%8C%83%E5%BC%8F"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 SGLang：面向复杂应用场景的编程范式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-Aibrix%E4%B8%8EMooncake%EF%BC%9A%E9%9D%A2%E5%90%91%E5%A4%A7%E8%A7%84%E6%A8%A1%E9%83%A8%E7%BD%B2%E7%9A%84%E7%B3%BB%E7%BB%9F%E7%BA%A7%E5%88%9B%E6%96%B0"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 Aibrix与Mooncake：面向大规模部署的系统级创新</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A1%86%E6%9E%B6%E6%A8%AA%E5%90%91%E5%AF%B9%E6%AF%94%EF%BC%9A%E5%90%84%E8%87%AA%E7%9A%84%E5%AE%9A%E4%BD%8D%E4%B8%8E%E4%BC%98%E5%8A%A3"><span class="nav-number">3.</span> <span class="nav-text">框架横向对比：各自的定位与优劣</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E6%B5%81AI-Infra%E6%A1%86%E6%9E%B6%E8%83%BD%E5%8A%9B%E5%AF%B9%E6%AF%94"><span class="nav-number">3.1.</span> <span class="nav-text">主流AI-Infra框架能力对比</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%F0%9F%97%BA%EF%B8%8F%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">4.</span> <span class="nav-text">🗺️参考文献</span></a></li></ol></div></div></div></div></div></div><div class="main-content-footer"><footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color"><div class="info-container py-3 text-center"><div class="text-center">&copy; <span>2018</span> - 2025&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration:0.5s;color:#f54545"></i>&nbsp;&nbsp;<a href="/">Fre5h1nd</a><p class="post-count space-x-0.5"><span>共撰写了 161 篇文章 </span><span>共 392.6k 字</span></p></div><script data-swup-reload-script src="https://cn.vercount.one/js"></script><div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right"><span id="busuanzi_container_site_uv" class="lg:!block"><span class="text-sm">访问人数</span> <span id="busuanzi_value_site_uv"></span> </span><span id="busuanzi_container_site_pv" class="lg:!block"><span class="text-sm">总访问量</span> <span id="busuanzi_value_site_pv"></span></span></div><div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left"><span class="lg:block text-sm">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a> 驱动</span> <span class="text-sm lg:block">主题&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.8.5</a></span></div><div>博客已运行 <span class="odometer" id="runtime_days"></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒</div><script data-swup-reload-script>try{function odometer_init(){document.querySelectorAll(".odometer").forEach(e=>{new Odometer({el:e,format:"( ddd).dd",duration:200})})}odometer_init()}catch(e){}</script><script data-swup-reload-script src="//code.tidio.co/f96gbctyu6loroasujsehm9vxttuvcnw.js" async></script></div></footer></div></div><div class="post-tools"><div class="post-tools-container"><ul class="article-tools-list"><li class="right-bottom-tools page-aside-toggle"><i class="fa-regular fa-outdent"></i></li><li class="go-comment"><i class="fa-regular fa-comments"></i></li></ul></div></div><div class="right-side-tools-container"><div class="side-tools-container"><ul class="hidden-tools-list"><li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center"><i class="fa-regular fa-magnifying-glass-plus"></i></li><li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center"><i class="fa-regular fa-magnifying-glass-minus"></i></li><li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center"><i class="fa-regular fa-moon"></i></li><li class="right-bottom-tools rss flex justify-center items-center"><a class="flex justify-center items-center" href="/atom.xml" target="_blank"><i class="fa-regular fa-rss"></i></a></li><li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center"><i class="fa-regular fa-arrow-down"></i></li></ul><ul class="visible-tools-list"><li class="right-bottom-tools toggle-tools-list flex justify-center items-center"><i class="fa-regular fa-cog fa-spin"></i></li><li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center"><i class="arrow-up fas fa-arrow-up"></i> <span class="percent"></span></li></ul></div></div><div class="image-viewer-container"><img src=""></div><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-input-field-pre"><i class="fa-solid fa-keyboard"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="站内搜索您需要的内容..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa-solid fa-times"></i></span></div><div id="search-result"><div id="no-result"><i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i></div></div></div></div></main><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/Swup.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupSlideTheme.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupScriptsPlugin.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupProgressPlugin.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupScrollPlugin.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupPreloadPlugin.min.js"></script><script>const swup=new Swup({plugins:[new SwupScriptsPlugin({optin:!0}),new SwupProgressPlugin,new SwupScrollPlugin({offset:80}),new SwupSlideTheme({mainElement:".main-content-body"}),new SwupPreloadPlugin],containers:["#swup"]})</script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/imageViewer.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/utils.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/main.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/navbarShrink.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/scrollTopBottom.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/lightDarkSwitch.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/categoryList.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/localSearch.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/codeBlock.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/lazyload.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/runtime.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/odometer.min.js"></script><link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/assets/odometer-theme-minimal.css"><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/Typed.min.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/typed.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/mermaid.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/mermaid.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/minimasonry.min.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/masonry.js"></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/tocToggle.js" data-swup-reload-script></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/toc.js" data-swup-reload-script></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/tabs.js" data-swup-reload-script></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/moment-with-locales.min.js" data-swup-reload-script></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/essays.js" data-swup-reload-script></script><div id="aplayer"></div><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/APlayer.min.js"></script><script src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/aplayer.js"></script></body></html>