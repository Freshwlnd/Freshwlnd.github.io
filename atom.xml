<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Fre5h1nd&#39;s Blog</title>
  
  
  <link href="https://freshwlnd.github.io/atom.xml" rel="self"/>
  
  <link href="https://freshwlnd.github.io/"/>
  <updated>2025-08-26T14:28:01.046Z</updated>
  <id>https://freshwlnd.github.io/</id>
  
  <author>
    <name>Fre5h1nd</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【集群】云原生批调度实战：Volcano 深度解析（四）：CREATE 阶段瓶颈追踪与优化思考</title>
    <link href="https://freshwlnd.github.io/2025/08/26/k8s/k8s-volcano-create-analysis/"/>
    <id>https://freshwlnd.github.io/2025/08/26/k8s/k8s-volcano-create-analysis/</id>
    <published>2025-08-26T11:45:21.000Z</published>
    <updated>2025-08-26T14:28:01.046Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 深度解析》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/05/26/k8s/k8s-volcano-1/" title="云原生批调度实战：Volcano 深度解析（一）批处理背景需求与Volcano特点">云原生批调度实战：Volcano 深度解析（一）批处理背景需求与Volcano特点</a></li><li><a href="/2025/05/27/k8s/k8s-volcano-2/" title="云原生批调度实战：Volcano 深度解析（二）Volcano调度流程与调度状态">云原生批调度实战：Volcano 深度解析（二）Volcano调度流程与调度状态</a></li><li><a href="/2025/06/22/k8s/k8s-volcano-install/" title="云原生批调度实战：Volcano 安装与初试">云原生批调度实战：Volcano 安装与初试</a></li><li><a href="/2025/08/25/k8s/k8s-volcano-core-flow/" title="云原生批调度实战：Volcano 深度解析（三）核心流程解析与架构设计">云原生批调度实战：Volcano 深度解析（三）核心流程解析与架构设计</a></li><li><a href="/2025/08/26/k8s/k8s-volcano-create-analysis/" title="云原生批调度实战：Volcano 深度解析（四）Webhook 机制深度解析">云原生批调度实战：Volcano 深度解析（四）Webhook 机制深度解析</a></li></ol></blockquote><p>本文承接《Volcano 深度解析（三）：核心流程解析与架构设计》，聚焦 <strong>CREATED 阶段</strong> 的性能瓶颈。实验环境及测试方法延续前文，不再赘述。</p><h1 id="0️⃣-背景回顾"><a href="#0️⃣-背景回顾" class="headerlink" title="0️⃣ 背景回顾"></a>0️⃣ 背景回顾</h1><p>在 <a href="/2025/08/24/k8s/k8s-scheduler-performance-volcano-hypothesis-verification/" title="Webhook 禁用实验">Webhook 禁用实验</a> 中，我们已确认：即使禁用 Webhook，<strong>CREATED 曲线仍呈阶梯式“突增 / 突停”</strong>。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/4-disable-webhook/a.NoGang-10KJob/output/panel-5.png?raw=true" alt="CREATED 阶梯示例 Benchmark-1：10K Jobs × 1 Pod"><figcaption>CREATED 阶梯示例 Benchmark-1：10K Jobs × 1 Pod</figcaption></figure><br><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/Freshwlnd/image/refs/heads/blog/kube-scheduling-perf-image/4-disable-webhook/b.NoGang-500Job-no/output/panel-5.png" alt="CREATED 阶梯示例 Benchmark-2：500 Jobs × 20 Pods"><figcaption>CREATED 阶梯示例 Benchmark-2：500 Jobs × 20 Pods</figcaption></figure></p><p>本文尝试回答两个问题：</p><ol><li>阶梯为何产生？</li><li>有哪些“调得动”的参数能够缓解？</li></ol><h1 id="1️⃣-实验现象重现"><a href="#1️⃣-实验现象重现" class="headerlink" title="1️⃣ 实验现象重现"></a>1️⃣ 实验现象重现</h1><table><thead><tr><th>Benchmark</th><th>Job×Pod</th><th>现象</th><th>备注</th></tr></thead><tbody><tr><td>benchmark-1</td><td>10K×1</td><td>CREATE 与 SCHEDULE 几乎重叠，整体速度四组中最慢</td><td><strong>CREATE = 主要瓶颈</strong></td></tr><tr><td>benchmark-2</td><td>500×20</td><td>阶梯最清晰，阶段性出现 CREATE 阻塞 → SCHEDULE 停顿</td><td>CREATE &amp; SCHEDULE 交替受阻</td></tr><tr><td>benchmark-3</td><td>20×500</td><td>CREATE 有阶梯但速度明显快于 SCHEDULE</td><td>SCHEDULE 成瓶颈</td></tr><tr><td>benchmark-4</td><td>1×10K</td><td>同上，CREATE 不是主瓶颈</td><td></td></tr></tbody></table><p><strong>猜想</strong>：JobController 对 Pod 的“批量同步创建”导致单批全部结束前无法进入下一批，从而表现为突停；批量完成后瞬时放量，表现为突增。</p><h1 id="2️⃣-代码走读：JobController-批量创建逻辑"><a href="#2️⃣-代码走读：JobController-批量创建逻辑" class="headerlink" title="2️⃣ 代码走读：JobController 批量创建逻辑"></a>2️⃣ 代码走读：JobController 批量创建逻辑</h1><h2 id="2-1-Worker-线程来源"><a href="#2-1-Worker-线程来源" class="headerlink" title="2.1 Worker 线程来源"></a>2.1 Worker 线程来源</h2><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cmd/controller-manager/app/server.go:134-139</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">startControllers</span><span class="params">(config *rest.Config, opt *options.ServerOption)</span></span> <span class="function"><span class="keyword">func</span><span class="params">(ctx context.Context)</span></span> {</span><br><span class="line">    ...</span><br><span class="line">    controllerOpt.WorkerNum = opt.WorkerThreads</span><br><span class="line">    ...</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p><code>--worker-threads</code> 默认为 <strong>50</strong>（不指定则使用该值），决定 JobController 并发消费 <strong>Job 请求</strong> 的 goroutine 数。</p><blockquote><p>⚠️ <strong>注意</strong>：这里的线程只决定 <em>Job</em> 并行数，跟 <em>Pod</em> 并行数并非一回事。</p></blockquote><h2 id="2-2-哈希分片与队列"><a href="#2-2-哈希分片与队列" class="headerlink" title="2.2 哈希分片与队列"></a>2.2 哈希分片与队列</h2><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/controllers/job/job_controller.go:318-333</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cc *jobcontroller)</span></span> belongsToThisRoutine(key <span class="type">string</span>, count <span class="type">uint32</span>) <span class="type">bool</span> {</span><br><span class="line">    val := cc.genHash(key)</span><br><span class="line">    <span class="keyword">return</span> val % cc.workers == count</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cc *jobcontroller)</span></span> getWorkerQueue(key <span class="type">string</span>) workqueue.TypedRateLimitingInterface[any] {</span><br><span class="line">val := cc.genHash(key)</span><br><span class="line">queue := cc.queueList[val%cc.workers]</span><br><span class="line"><span class="keyword">return</span> queue</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// genHash 源码</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cc *jobcontroller)</span></span> genHash(key <span class="type">string</span>) <span class="type">uint32</span> {</span><br><span class="line">    hashVal := fnv.New32() <span class="comment">// FNV-1a 非加密散列</span></span><br><span class="line">    hashVal.Write([]<span class="type">byte</span>(key))</span><br><span class="line">    <span class="keyword">return</span> hashVal.Sum32()</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p>FNV（Fowler–Noll–Vo）是一种速度快、冲突率低的非加密散列函数，它在 Volcano 中承担 <strong>一致分片</strong> 的角色：</p><ol><li><strong>单 Job 串行化</strong>：保证相同的 JobKey 永远路由到同一 worker，避免多线程并发修改同一 Job 状态导致的竞态（如版本冲突、重复创建 Pod 等）。</li><li><strong>负载均衡</strong>：不同 Job 均匀散落到 <code>workers</code> 个队列，提升并行度。</li></ol><blockquote><p>❓ 如果不保证“同一Job → 同一线程”？</p><ul><li>多线程可能同时进入同一 Job 的状态机，导致 <strong>Status 冲突</strong>（ResourceVersion 不匹配重试、乐观锁失败）。</li><li>重复创建 Pod / PodGroup，产生 <strong>资源泄漏</strong> 与 <strong>Gang 调度失败</strong>。</li><li>如不使用该机制，则需要加全局锁或精细乐观重试，得不偿失。</li></ul></blockquote><h2 id="3️⃣-CREATE-批量创建流程"><a href="#3️⃣-CREATE-批量创建流程" class="headerlink" title="3️⃣ CREATE 批量创建流程"></a>3️⃣ CREATE 批量创建流程</h2><h3 id="PodGroup-创建"><a href="#PodGroup-创建" class="headerlink" title="PodGroup 创建"></a>PodGroup 创建</h3><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/controllers/job/job_controller_actions.go:190-214</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cc *jobcontroller)</span></span> createOrUpdatePodGroup(job *batch.Job) <span class="type">error</span> {</span><br><span class="line">    ...</span><br><span class="line">    pg := &amp;scheduling.PodGroup{ ... }</span><br><span class="line">    vcClient.SchedulingV1beta1().PodGroups(...).Create(..., pg, ...)</span><br><span class="line">    ...</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p>一次 API 调用即可完成，且一个 Job 只建一个 PodGroup；创建本身比较简单（仅是逻辑单元），可能不是主要瓶颈：</p><ul><li>PodGroup 本质是一个 CRD 对象（仅几十字节的 Spec &amp; Metadata，见<code>pkg/controllers/job/job_controller_actions.go</code>定义部分），创建过程只是 kube-apiserver → etcd 的一次写操作。</li><li>不涉及调度决策、节点通信或资源计算；成功后即可返回，无后续长耗时流程。</li></ul><p>但需注意：若 <code>MinMember</code> 设置过大或 Queue 资源不足，调度器在 <strong>后续阶段</strong> 仍可能因 PodGroup 不满足条件而阻塞 Job 启动，这属于调度环节而非 CREATE 环节。</p><h3 id="Pod-创建"><a href="#Pod-创建" class="headerlink" title="Pod 创建"></a>Pod 创建</h3><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/controllers/job/job_controller_actions.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cc *jobcontroller)</span></span> syncJob(jobInfo *apis.JobInfo, updateStatus state.UpdateStatusFn) <span class="type">error</span> {</span><br><span class="line">    ...</span><br><span class="line">    waitCreationGroup := sync.WaitGroup{}</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">var</span> podToCreateEachTask []*v1.Pod</span><br><span class="line"><span class="keyword">for</span> _, ts := <span class="keyword">range</span> job.Spec.Tasks {</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="type">int</span>(ts.Replicas); i++ {           <span class="comment">// 收集待建 Pod</span></span><br><span class="line">            ...</span><br><span class="line">            newPod := createJobPod(job, tc, ts.TopologyPolicy, i, jobForwarding)</span><br><span class="line">            ...</span><br><span class="line">            podToCreateEachTask = <span class="built_in">append</span>(podToCreateEachTask, newPod)</span><br><span class="line">            waitCreationGroup.Add(<span class="number">1</span>)</span><br><span class="line">            ...</span><br><span class="line">        }</span><br><span class="line">        podToCreate[ts.Name] = podToCreateEachTask</span><br><span class="line">    }</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">for</span> taskName, podToCreateEachTask := <span class="keyword">range</span> podToCreate {</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(taskName <span class="type">string</span>, podToCreateEachTask []*v1.Pod)</span></span> {</span><br><span class="line">            ...</span><br><span class="line">            <span class="keyword">for</span> _, pod := <span class="keyword">range</span> podToCreateEachTask {</span><br><span class="line">                <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(pod *v1.Pod)</span></span> {</span><br><span class="line">                    <span class="keyword">defer</span> waitCreationGroup.Done()</span><br><span class="line">                    kubeClient.CoreV1().Pods(...).Create(...)</span><br><span class="line">                }(pod)</span><br><span class="line">            }</span><br><span class="line">            ...</span><br><span class="line">        }(taskName, podToCreateEachTask)</span><br><span class="line">    }</span><br><span class="line">    ...</span><br><span class="line">    waitCreationGroup.Wait()  <span class="comment">// ⬅ 阻塞：一批全部完成前不返回</span></span><br><span class="line">    ...</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><blockquote><p><strong>观察</strong>：每个 Job 中的所有 Pod 都必须在同一批完成，<code>Wait()</code> 阻塞期间该 worker 线程无法服务其他 Job，形成“突停”。</p></blockquote><p>与上述PodGroup有所差异，由于Pod是K8s原生对象，涉及到的字段极多且复杂，既需要默认填充大量字段、也需要花更多时间写入etcd，因此更容易成为瓶颈。</p><h2 id="4️⃣-CREATE-阶段可能的瓶颈链路"><a href="#4️⃣-CREATE-阶段可能的瓶颈链路" class="headerlink" title="4️⃣  CREATE 阶段可能的瓶颈链路"></a>4️⃣  CREATE 阶段可能的瓶颈链路</h2><ol><li><strong>ControllerManager – PodGroup 创建</strong>：单请求，理论影响小；仅在 CRD 校验或 etcd 压力大时显现。</li><li><strong>ControllerManager – Pod 创建并发</strong>：瞬时并发高且字段复杂，容易受 kube-apiserver QPS/TPS 限流影响。</li><li><strong>kube-apiserver – etcd 写入</strong>：大批量对象持久化；etcd IOPS 饱和时延长请求时长。</li><li><strong>网络 / TLS 握手</strong>：每 Pod 一次 HTTPS；高并发下握手耗时占比提升。</li><li><strong>Webhook</strong>（若开启）：Mutating/Validating 延时或超时。</li><li><strong>Worker 线程饱和</strong>：<code>--worker-threads</code> 阈值被占满后，新 Job 无法 dequeue，外部观察即“突停”。</li></ol><blockquote><p>在四组 Benchmark 中，总 Pod 数一致（10K），但 <strong>Job 数量越多，Worker 越容易饱和</strong>，因此出现瓶颈的并非 “单 Job 内 Pod 数” 而是 “Cluster 同时活跃的 Job 数”。</p></blockquote><h2 id="5️⃣-关键对象关系"><a href="#5️⃣-关键对象关系" class="headerlink" title="5️⃣ 关键对象关系"></a>5️⃣ 关键对象关系</h2><table><thead><tr><th>对象</th><th>层级</th><th>作用</th><th>与其他对象关系</th></tr></thead><tbody><tr><td><strong>Job</strong></td><td>Volcano CRD</td><td>用户提交的批处理作业</td><td>一个 Job <strong>拥有</strong> 1 PodGroup &amp; N Tasks</td></tr><tr><td><strong>PodGroup</strong></td><td>Volcano CRD</td><td>Gang 调度边界，决定最小可运行成员数</td><td>Job 创建时同步生成；Scheduler 以 PG 维度做满足性判断</td></tr><tr><td><strong>Task</strong></td><td>Job 内部元素</td><td>Job 的逻辑分片，可用不同镜像/参数</td><td>Task <strong>生成</strong> 多个 Pod(Replicas)</td></tr><tr><td><strong>Pod</strong></td><td>K8s 原生</td><td>实际运行单元</td><td>由 Task 模板实例化，归属同一 PodGroup</td></tr></tbody></table><blockquote><p>PodGroup ≠ Task：个人理解前者是 Job 的化身，后者是 Job 内的子对象。<br>层级关系为：1 Job（PodGroup） → n Task → n*m Pod。</p></blockquote><h2 id="6️⃣-相关参数与理论影响"><a href="#6️⃣-相关参数与理论影响" class="headerlink" title="6️⃣ 相关参数与理论影响"></a>6️⃣ 相关参数与理论影响</h2><table><thead><tr><th>参数</th><th>默认</th><th>预期影响</th><th>实测结论</th></tr></thead><tbody><tr><td><code>--worker-threads</code></td><td>50</td><td>决定可同时被处理的 Job 数</td><td>✅ 提高可缩短突停时长，但系统整体压力增大</td></tr><tr><td><code>task.replicas</code></td><td>用户输入</td><td>决定单 Job 内批量大小</td><td>⚠️ 非根因；更改只影响单 batch 时长</td></tr><tr><td>新增参数</td><td>N/A</td><td>控制每次并发 Pod 数（而非局限于一个 Job 内的 Pod 数）</td><td>🚧 需进一步设计</td></tr></tbody></table><blockquote><p>结论：</p><ul><li><strong>Job 数</strong> → Worker 饱和度 → 是否突停。</li><li><strong>Replica 数</strong> → 单 worker 持续时间 → 阶梯宽度。</li></ul></blockquote><h2 id="7️⃣-优化方向"><a href="#7️⃣-优化方向" class="headerlink" title="7️⃣ 优化方向"></a>7️⃣ 优化方向</h2><table><thead><tr><th>方向</th><th>复杂度</th><th>收益</th><th>说明</th></tr></thead><tbody><tr><td>智能调整 <code>--worker-threads</code></td><td>低</td><td>高</td><td>观察到出现瓶颈（或观察到 replicas 普遍较小）时，自动提高并行线程数，削弱同步阻塞</td></tr><tr><td>引入新参数，或调整线程Wait阻塞逻辑</td><td>中</td><td>高</td><td>分片提交 Pods（支持跨Job并行），削弱同步阻塞</td></tr></tbody></table><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/volcano-sh/volcano">[1] Volcano GitHub 仓库<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://volcano.sh/zh/">[2] Volcano 官方文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/kube-scheduler/">[3] Kubernetes 调度器设计<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://volcano.sh/zh/docs/architecture/">[4] Volcano 架构设计<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/admission-controllers/">[5] Kubernetes Webhook 机制<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://volcano.sh/zh/docs/actions/">[6] Volcano 调度器 Actions<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/concepts/architecture/controller/">[7] Kubernetes 控制器<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://volcano.sh/zh/docs/schduler_introduction/">[8] Volcano 调度器<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">深入剖析禁用 Webhook 后仍旧存在的 CREATED 阶段性能瓶颈，通过代码略读定位 JobController 中的批量创建逻辑，分析关键相关参数影响并提出可行的优化方向。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="Kubernetes" scheme="https://freshwlnd.github.io/tags/Kubernetes/"/>
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="性能优化" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
    <category term="批处理" scheme="https://freshwlnd.github.io/tags/%E6%89%B9%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：Volcano 深度解析（三）：核心流程解析与架构设计</title>
    <link href="https://freshwlnd.github.io/2025/08/25/k8s/k8s-volcano-core-flow/"/>
    <id>https://freshwlnd.github.io/2025/08/25/k8s/k8s-volcano-core-flow/</id>
    <published>2025-08-25T12:39:54.000Z</published>
    <updated>2025-08-26T12:50:24.785Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 深度解析》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/05/26/k8s/k8s-volcano-1/" title="云原生批调度实战：Volcano 深度解析（一）批处理背景需求与Volcano特点">云原生批调度实战：Volcano 深度解析（一）批处理背景需求与Volcano特点</a></li><li><a href="/2025/05/27/k8s/k8s-volcano-2/" title="云原生批调度实战：Volcano 深度解析（二）Volcano调度流程与调度状态">云原生批调度实战：Volcano 深度解析（二）Volcano调度流程与调度状态</a></li><li><a href="/2025/06/22/k8s/k8s-volcano-install/" title="云原生批调度实战：Volcano 安装与初试">云原生批调度实战：Volcano 安装与初试</a></li><li><a href="/2025/08/25/k8s/k8s-volcano-core-flow/" title="云原生批调度实战：Volcano 深度解析（三）核心流程解析与架构设计">云原生批调度实战：Volcano 深度解析（三）核心流程解析与架构设计</a></li><li><a href="/2025/08/26/k8s/k8s-volcano-create-analysis/" title="云原生批调度实战：Volcano 深度解析（四）Webhook 机制深度解析">云原生批调度实战：Volcano 深度解析（四）Webhook 机制深度解析</a></li></ol></blockquote><h1 id="💡简介"><a href="#💡简介" class="headerlink" title="💡简介"></a>💡简介</h1><p>在<a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="调度器性能对比分析">调度器性能对比分析</a>中，我们发现Volcano在大规模Job创建时存在性能瓶颈，特别是与Webhook相关的限制。为了深入理解这一现象并提供有效的优化方案，我们需要从代码层面深入分析Volcano的核心流程。</p><p>本文将从Volcano的整体架构出发，详细解析从Job创建到Pod调度的完整流程，通过代码分析揭示Volcano如何实现高效的批处理调度，以及与原生Kubernetes调度器的关键差异。</p><h1 id="🖼️背景"><a href="#🖼️背景" class="headerlink" title="🖼️背景"></a>🖼️背景</h1><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>在性能测试中，我们发现Volcano在以下场景下存在性能瓶颈：</p><ol><li><strong>大规模Job创建</strong>：同时创建大量Job时，会出现阶段性阻塞</li><li><strong>Webhook QPS限制</strong>：Webhook的QPS限制可能影响Job创建速度</li><li><strong>批量处理机制</strong>：可能存在批量处理策略，按批创建，导致创建成为瓶颈</li></ol><p>为了理解这些问题的根本原因，我们需要深入分析Volcano的核心流程。在这篇博客中，我们对代码进行初步分析，从Job创建到Pod调度的完整流程开始，初步了解各组件的作用和交互关系。</p><h1 id="🏗️Volcano整体架构概览"><a href="#🏗️Volcano整体架构概览" class="headerlink" title="🏗️Volcano整体架构概览"></a>🏗️Volcano整体架构概览</h1><h2 id="核心组件架构"><a href="#核心组件架构" class="headerlink" title="核心组件架构"></a>核心组件架构</h2><p>Volcano作为Kubernetes的批处理调度系统，主要由以下几个核心组件组成：</p><pre class="mermaid">graph TB    A[用户] --&gt; B[kube-apiserver]    B --&gt; C[Volcano Controller Manager]    B --&gt; D[Volcano Scheduler]    B --&gt; E[Volcano Webhook Manager]        C --&gt; F[Job Controller]    C --&gt; G[PodGroup Controller]    C --&gt; H[Queue Controller]        D --&gt; I[Cache]    D --&gt; J[Actions]    D --&gt; K[Plugins]        E --&gt; L[Admission Webhooks]    E --&gt; M[Validating Webhooks]    E --&gt; N[Mutating Webhooks]        F --&gt; O[Pod Creation]    G --&gt; P[PodGroup Management]    H --&gt; Q[Queue Management]        I --&gt; R[Node Cache]    I --&gt; S[Pod Cache]    I --&gt; T[Job Cache]        J --&gt; U[Enqueue]    J --&gt; V[Allocate]    J --&gt; W[Preempt]    J --&gt; X[Reclaim]    J --&gt; Y[Backfill]</pre><h2 id="组件职责分析"><a href="#组件职责分析" class="headerlink" title="组件职责分析"></a>组件职责分析</h2><h3 id="1-Controller-Manager"><a href="#1-Controller-Manager" class="headerlink" title="1. Controller Manager"></a>1. Controller Manager</h3><ul><li><strong>Job Controller</strong>：管理Volcano Job的生命周期</li><li><strong>PodGroup Controller</strong>：管理PodGroup的创建和状态</li><li><strong>Queue Controller</strong>：管理队列资源和配额</li></ul><h3 id="2-Scheduler"><a href="#2-Scheduler" class="headerlink" title="2. Scheduler"></a>2. Scheduler</h3><ul><li><strong>Cache</strong>：维护集群状态快照</li><li><strong>Actions</strong>：执行调度操作（入队、分配、抢占等）</li><li><strong>Plugins</strong>：提供调度算法和策略</li></ul><h3 id="3-Webhook-Manager"><a href="#3-Webhook-Manager" class="headerlink" title="3. Webhook Manager"></a>3. Webhook Manager</h3><ul><li><strong>Admission Webhooks</strong>：准入控制</li><li><strong>Validating Webhooks</strong>：验证资源</li><li><strong>Mutating Webhooks</strong>：修改资源</li></ul><h1 id="🔄Job创建到Pod调度的完整流程"><a href="#🔄Job创建到Pod调度的完整流程" class="headerlink" title="🔄Job创建到Pod调度的完整流程"></a>🔄Job创建到Pod调度的完整流程</h1><h2 id="流程概览"><a href="#流程概览" class="headerlink" title="流程概览"></a>流程概览</h2><pre class="mermaid">sequenceDiagram    participant User as 用户    participant API as kube-apiserver    participant Webhook as Webhook Manager    participant Controller as Controller Manager    participant Scheduler as Volcano Scheduler    participant Cache as Scheduler Cache        User-&gt;&gt;API: 创建Volcano Job    API-&gt;&gt;Webhook: 调用准入Webhook    Webhook-&gt;&gt;API: 验证/修改Job    API-&gt;&gt;Controller: 触发Job Controller    Controller-&gt;&gt;API: 创建PodGroup    API-&gt;&gt;Webhook: 调用PodGroup Webhook    Webhook-&gt;&gt;API: 验证PodGroup    Controller-&gt;&gt;API: 创建Pod    API-&gt;&gt;Webhook: 调用Pod Webhook    Webhook-&gt;&gt;API: 验证Pod    API-&gt;&gt;Scheduler: 触发调度    Scheduler-&gt;&gt;Cache: 获取集群快照    Scheduler-&gt;&gt;Scheduler: 执行调度算法    Scheduler-&gt;&gt;API: 绑定Pod到节点</pre><h2 id="详细流程分析"><a href="#详细流程分析" class="headerlink" title="详细流程分析"></a>详细流程分析</h2><h3 id="阶段1：Job创建与验证"><a href="#阶段1：Job创建与验证" class="headerlink" title="阶段1：Job创建与验证"></a>阶段1：Job创建与验证</h3><h4 id="1-1-用户提交Job"><a href="#1-1-用户提交Job" class="headerlink" title="1.1 用户提交Job"></a>1.1 用户提交Job</h4><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch.volcano.sh/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-job</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">minAvailable:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">schedulerName:</span> <span class="string">volcano</span></span><br><span class="line">  <span class="attr">queue:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">tasks:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">task-1</span></span><br><span class="line">      <span class="attr">template:</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">containers:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">container-1</span></span><br><span class="line">              <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">              <span class="attr">command:</span> [<span class="string">"sleep"</span>, <span class="string">"100"</span>]</span><br></pre></td></tr></table></figure></div><h4 id="1-2-Webhook验证"><a href="#1-2-Webhook验证" class="headerlink" title="1.2 Webhook验证"></a>1.2 Webhook验证</h4><p>当用户提交Job时，kube-apiserver会调用Volcano的Webhook进行验证。AdmitJobs函数通过HTTP路由系统被kube-apiserver调用，而不是直接的函数调用：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/webhooks/admission/jobs/validate/admit_job.go</span></span><br><span class="line"><span class="keyword">var</span> service = &amp;router.AdmissionService{</span><br><span class="line">    Path: <span class="string">"/jobs/validate"</span>,</span><br><span class="line">    Func: AdmitJobs,  <span class="comment">// 注册到HTTP路由</span></span><br><span class="line">    ValidatingConfig: &amp;whv1.ValidatingWebhookConfiguration{...},</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">AdmitJobs</span><span class="params">(ar admissionv1.AdmissionReview)</span></span> *admissionv1.AdmissionResponse {</span><br><span class="line">    <span class="keyword">switch</span> ar.Request.Operation {</span><br><span class="line">    <span class="keyword">case</span> admissionv1.Create:</span><br><span class="line">        msg = validateJobCreate(job, &amp;reviewResponse)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p><strong>实际验证内容</strong>：根据代码分析，<code>validateJobCreate</code>函数主要验证：</p><ul><li><strong>基础参数校验</strong>：minAvailable、maxRetry、replicas等参数范围</li><li><strong>任务模板验证</strong>：Pod模板的合法性和K8s资源规范</li><li><strong>MPI依赖检查</strong>：验证master/worker任务配置</li><li><strong>任务间依赖</strong>：检查依赖关系是否形成有向无环图(DAG)</li><li><strong>队列状态验证</strong>：检查Queue是否存在、状态是否为Open、是否为叶子队列</li><li><strong>插件配置验证</strong>：验证Job插件是否存在</li></ul><p><strong>调用机制</strong>：Webhook通过以下机制被调用：</p><ol><li>webhook-manager启动时注册HTTP路由 (<code>http.HandleFunc(service.Path, service.Handler)</code>)</li><li>kube-apiserver根据ValidatingWebhookConfiguration向Volcano发送HTTP POST请求</li><li>请求路径为 <code>/jobs/validate</code>，由router.Serve处理并调用AdmitJobs函数</li></ol><h4 id="1-3-Job-Controller处理"><a href="#1-3-Job-Controller处理" class="headerlink" title="1.3 Job Controller处理"></a>1.3 Job Controller处理</h4><p>Job Controller监听到Job创建事件后，开始处理：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/controllers/job/job_controller.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(jc *jobcontroller)</span></span> syncJob(job *vcbatch.Job) <span class="type">error</span> {</span><br><span class="line">    <span class="comment">// 1. 创建PodGroup</span></span><br><span class="line">    <span class="comment">// 2. 创建Pod</span></span><br><span class="line">    <span class="comment">// 3. 更新Job状态</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><h3 id="阶段2：PodGroup创建"><a href="#阶段2：PodGroup创建" class="headerlink" title="阶段2：PodGroup创建"></a>阶段2：PodGroup创建</h3><h4 id="2-1-PodGroup的作用"><a href="#2-1-PodGroup的作用" class="headerlink" title="2.1 PodGroup的作用"></a>2.1 PodGroup的作用</h4><p>PodGroup是Volcano的核心概念，用于实现Gang调度：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/controllers/job/job_controller.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(jc *jobcontroller)</span></span> createPodGroup(job *vcbatch.Job) <span class="type">error</span> {</span><br><span class="line">    podGroup := &amp;vcscheduling.PodGroup{</span><br><span class="line">        ObjectMeta: metav1.ObjectMeta{</span><br><span class="line">            Name:      job.Name,</span><br><span class="line">            Namespace: job.Namespace,</span><br><span class="line">        },</span><br><span class="line">        Spec: vcscheduling.PodGroupSpec{</span><br><span class="line">            MinMember: job.Spec.MinAvailable,</span><br><span class="line">            Queue:     job.Spec.Queue,</span><br><span class="line">        },</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> jc.vcClient.SchedulingV1beta1().PodGroups(job.Namespace).Create(podGroup)</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><h4 id="2-2-PodGroup状态管理"><a href="#2-2-PodGroup状态管理" class="headerlink" title="2.2 PodGroup状态管理"></a>2.2 PodGroup状态管理</h4><p>PodGroup的状态转换：</p><pre class="mermaid">stateDiagram-v2    [*] --&gt; Pending    Pending --&gt; Inqueue    Inqueue --&gt; Running    Running --&gt; Completed    Running --&gt; Failed    Completed --&gt; [*]    Failed --&gt; [*]</pre><h3 id="阶段3：Pod创建"><a href="#阶段3：Pod创建" class="headerlink" title="阶段3：Pod创建"></a>阶段3：Pod创建</h3><h4 id="3-1-批量Pod创建"><a href="#3-1-批量Pod创建" class="headerlink" title="3.1 批量Pod创建"></a>3.1 批量Pod创建</h4><p>Job Controller会根据Job配置创建多个Pod：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/controllers/job/job_controller.go</span></span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="type">int</span>(ts.Replicas); i++ {</span><br><span class="line">    podName := fmt.Sprintf(jobhelpers.PodNameFmt, job.Name, name, i)</span><br><span class="line">    <span class="keyword">if</span> _, found := pods[podName]; !found {</span><br><span class="line">        newPod := createJobPod(job, tc, ts.TopologyPolicy, i, jobForwarding)</span><br><span class="line">        <span class="comment">// 收集待创建的 Pod，并登记到 WaitGroup</span></span><br><span class="line">        podToCreateEachTask = <span class="built_in">append</span>(podToCreateEachTask, newPod)</span><br><span class="line">        waitCreationGroup.Add(<span class="number">1</span>)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p>随后使用 goroutine + WaitGroup 并发向 API Server 创建 Pod：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/controllers/job/job_controller.go</span></span><br><span class="line"><span class="keyword">for</span> _, pod := <span class="keyword">range</span> podToCreateEachTask {</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(pod *v1.Pod)</span></span> {</span><br><span class="line">        <span class="keyword">defer</span> waitCreationGroup.Done()</span><br><span class="line">        _, err := kubeClient.CoreV1().Pods(pod.Namespace).Create(ctx, pod, metav1.CreateOptions{})</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &amp;&amp; !apierrors.IsAlreadyExists(err) {</span><br><span class="line">            <span class="comment">// 错误处理略</span></span><br><span class="line">        }</span><br><span class="line">    }(pod)</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><h4 id="3-2-Pod-模板展开"><a href="#3-2-Pod-模板展开" class="headerlink" title="3.2 Pod 模板展开"></a>3.2 Pod 模板展开</h4><p>单个 Pod 的构造细节在 <code>createJobPod</code>（<code>pkg/controllers/job/job_controller_util.go</code>）：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/controllers/job/job_controller_util.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">createJobPod</span><span class="params">(job *batch.Job, template *v1.PodTemplateSpec,</span></span></span><br><span class="line"><span class="params"><span class="function">    topologyPolicy batch.NumaPolicy, ix <span class="type">int</span>, jobForwarding <span class="type">bool</span>)</span></span> *v1.Pod {</span><br><span class="line"></span><br><span class="line">    pod := &amp;v1.Pod{</span><br><span class="line">        ObjectMeta: metav1.ObjectMeta{</span><br><span class="line">            Name:      jobhelpers.MakePodName(job.Name, template.Name, ix),</span><br><span class="line">            Namespace: job.Namespace,</span><br><span class="line">            OwnerReferences: []metav1.OwnerReference{</span><br><span class="line">                *metav1.NewControllerRef(job, helpers.JobKind),</span><br><span class="line">            },</span><br><span class="line">        },</span><br><span class="line">        Spec: template.Spec,</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 省略 SchedulerName、Volume 等附加字段填充</span></span><br><span class="line">    <span class="keyword">return</span> pod</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><h3 id="阶段4：调度处理"><a href="#阶段4：调度处理" class="headerlink" title="阶段4：调度处理"></a>阶段4：调度处理</h3><h4 id="4-1-调度器触发"><a href="#4-1-调度器触发" class="headerlink" title="4.1 调度器触发"></a>4.1 调度器触发</h4><p>当Pod创建后，调度器开始工作：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/scheduler/framework/session.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(ssn *Session)</span></span> Open() {</span><br><span class="line">    <span class="comment">// 1. 获取集群快照</span></span><br><span class="line">    <span class="comment">// 2. 执行调度Actions</span></span><br><span class="line">    <span class="comment">// 3. 更新调度结果</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><h4 id="4-2-调度Actions执行"><a href="#4-2-调度Actions执行" class="headerlink" title="4.2 调度Actions执行"></a>4.2 调度Actions执行</h4><p>调度器按顺序执行Actions：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/scheduler/actions/allocate/allocate.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(alloc *Action)</span></span> Execute(ssn *framework.Session) {</span><br><span class="line">    <span class="comment">// 1. Enqueue: 将Job加入调度队列</span></span><br><span class="line">    <span class="comment">// 2. Allocate: 为Pod分配节点</span></span><br><span class="line">    <span class="comment">// 3. Preempt: 处理抢占</span></span><br><span class="line">    <span class="comment">// 4. Reclaim: 处理资源回收</span></span><br><span class="line">    <span class="comment">// 5. Backfill: 处理回填</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><h4 id="4-3-节点选择算法"><a href="#4-3-节点选择算法" class="headerlink" title="4.3 节点选择算法"></a>4.3 节点选择算法</h4><p>调度器使用多种算法选择最优节点：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/scheduler/plugins/predicates/predicates.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *predicatePlugin)</span></span> OnNodeAdd(node *v1.Node) {</span><br><span class="line">    <span class="comment">// 节点预选：过滤不满足条件的节点</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *predicatePlugin)</span></span> OnNodeUpdate(oldNode, newNode *v1.Node) {</span><br><span class="line">    <span class="comment">// 节点优选：为节点打分</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><h1 id="🔍关键组件深度解析"><a href="#🔍关键组件深度解析" class="headerlink" title="🔍关键组件深度解析"></a>🔍关键组件深度解析</h1><h2 id="Controller-Manager组件"><a href="#Controller-Manager组件" class="headerlink" title="Controller Manager组件"></a>Controller Manager组件</h2><h3 id="Job-Controller"><a href="#Job-Controller" class="headerlink" title="Job Controller"></a>Job Controller</h3><p>Job Controller是Volcano的核心控制器，负责管理Job的生命周期：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/controllers/job/job_controller.go</span></span><br><span class="line"><span class="keyword">type</span> jobcontroller <span class="keyword">struct</span> {</span><br><span class="line">    vcClient    vcclientset.Interface</span><br><span class="line">    kubeClient  kubernetes.Interface</span><br><span class="line">    jobInformer vcinformer.JobInformer</span><br><span class="line">    podInformer corev1informer.PodInformer</span><br><span class="line">    <span class="comment">// ... 其他字段</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(jc *jobcontroller)</span></span> syncJob(job *vcbatch.Job) <span class="type">error</span> {</span><br><span class="line">    <span class="comment">// 1. 检查Job状态</span></span><br><span class="line">    <span class="comment">// 2. 创建/更新PodGroup</span></span><br><span class="line">    <span class="comment">// 3. 创建/更新Pod</span></span><br><span class="line">    <span class="comment">// 4. 更新Job状态</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><h3 id="PodGroup-Controller"><a href="#PodGroup-Controller" class="headerlink" title="PodGroup Controller"></a>PodGroup Controller</h3><p>PodGroup Controller管理PodGroup的状态转换：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/controllers/podgroup/podgroup_controller.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(pgc *podgroupcontroller)</span></span> syncPodGroup(pg *vcscheduling.PodGroup) <span class="type">error</span> {</span><br><span class="line">    <span class="comment">// 1. 检查PodGroup状态</span></span><br><span class="line">    <span class="comment">// 2. 计算满足条件的Pod数量</span></span><br><span class="line">    <span class="comment">// 3. 更新PodGroup状态</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><h2 id="Scheduler组件"><a href="#Scheduler组件" class="headerlink" title="Scheduler组件"></a>Scheduler组件</h2><h3 id="Cache机制"><a href="#Cache机制" class="headerlink" title="Cache机制"></a>Cache机制</h3><p>调度器的Cache维护集群状态快照：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/scheduler/cache/cache.go</span></span><br><span class="line"><span class="keyword">type</span> Cache <span class="keyword">struct</span> {</span><br><span class="line">    nodes <span class="keyword">map</span>[<span class="type">string</span>]*api.NodeInfo</span><br><span class="line">    jobs  <span class="keyword">map</span>[api.JobID]*api.JobInfo</span><br><span class="line">    pods  <span class="keyword">map</span>[<span class="type">string</span>]*api.PodInfo</span><br><span class="line">    <span class="comment">// ... 其他字段</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Cache)</span></span> Snapshot() *api.ClusterInfo {</span><br><span class="line">    <span class="comment">// 创建集群快照</span></span><br><span class="line">    <span class="keyword">return</span> &amp;api.ClusterInfo{</span><br><span class="line">        Nodes: c.nodes,</span><br><span class="line">        Jobs:  c.jobs,</span><br><span class="line">        <span class="comment">// ... 其他信息</span></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><h3 id="Actions机制"><a href="#Actions机制" class="headerlink" title="Actions机制"></a>Actions机制</h3><p>Actions定义了调度器的核心操作：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/scheduler/actions/allocate/allocate.go</span></span><br><span class="line"><span class="keyword">type</span> Action <span class="keyword">struct</span> {</span><br><span class="line">    <span class="comment">// Action实现</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(alloc *Action)</span></span> Execute(ssn *framework.Session) {</span><br><span class="line">    <span class="comment">// 1. Enqueue: 入队操作</span></span><br><span class="line">    <span class="comment">// 2. Allocate: 分配操作</span></span><br><span class="line">    <span class="comment">// 3. Preempt: 抢占操作</span></span><br><span class="line">    <span class="comment">// 4. Reclaim: 回收操作</span></span><br><span class="line">    <span class="comment">// 5. Backfill: 回填操作</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><h2 id="Webhook-Manager组件"><a href="#Webhook-Manager组件" class="headerlink" title="Webhook Manager组件"></a>Webhook Manager组件</h2><h3 id="Admission-Webhooks"><a href="#Admission-Webhooks" class="headerlink" title="Admission Webhooks"></a>Admission Webhooks</h3><p>准入控制器处理资源创建和修改：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/webhooks/admission/job/admit.go</span></span><br><span class="line"><span class="keyword">type</span> jobAdmit <span class="keyword">struct</span> {</span><br><span class="line">    <span class="comment">// Webhook实现</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(job *jobAdmit)</span></span> Admit(ar *admissionv1.AdmissionReview) *admissionv1.AdmissionResponse {</span><br><span class="line">    <span class="comment">// 1. 解析请求</span></span><br><span class="line">    <span class="comment">// 2. 验证资源</span></span><br><span class="line">    <span class="comment">// 3. 返回结果</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/volcano-sh/volcano">[1] Volcano GitHub 仓库<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://volcano.sh/zh/">[2] Volcano 官方文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/kube-scheduler/">[3] Kubernetes 调度器设计<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://volcano.sh/zh/docs/architecture/">[4] Volcano 架构设计<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/admission-controllers/">[5] Kubernetes Webhook 机制<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://volcano.sh/zh/docs/actions/">[6] Volcano 调度器 Actions<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/concepts/architecture/controller/">[7] Kubernetes 控制器<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://volcano.sh/zh/docs/schduler_introduction/">[8] Volcano 调度器<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">深入解析Volcano的核心架构和Job创建到Pod调度的完整流程，通过代码分析揭示Volcano如何实现高效的批处理调度。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="Kubernetes" scheme="https://freshwlnd.github.io/tags/Kubernetes/"/>
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
    <category term="批处理" scheme="https://freshwlnd.github.io/tags/%E6%89%B9%E5%A4%84%E7%90%86/"/>
    
    <category term="架构设计" scheme="https://freshwlnd.github.io/tags/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结</title>
    <link href="https://freshwlnd.github.io/2025/08/24/k8s/k8s-scheduler-performance-volcano-hypothesis-verification/"/>
    <id>https://freshwlnd.github.io/2025/08/24/k8s/k8s-scheduler-performance-volcano-hypothesis-verification/</id>
    <published>2025-08-24T07:11:32.000Z</published>
    <updated>2025-08-26T14:47:33.196Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li><li><a href="/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/" title="监控与测试环境解析：测试流程拆解篇">监控与测试环境解析：测试流程拆解篇</a></li><li><a href="/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/" title="监控与测试环境解析：指标采集与可视化篇">监控与测试环境解析：指标采集与可视化篇</a></li><li><a href="/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/" title="监控与测试环境解析：自定义镜像性能回归测试">监控与测试环境解析：自定义镜像性能回归测试</a></li><li><a href="/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/" title="监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题">监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题</a></li><li><a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试">云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试</a></li><li><a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复">云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</a></li><li><a href="/2025/08/20/k8s/k8s-scheduler-performance-volcano-version/" title="云原生批调度实战：Volcano版本修改与性能测试优化">云原生批调度实战：Volcano版本修改与性能测试优化</a></li><li><a href="/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/" title="云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析">云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析</a></li><li><a href="/2025/08/24/k8s/k8s-scheduler-performance-volcano-hypothesis-verification/" title="云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结">云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结</a></li></ol></blockquote><h1 id="💡简介"><a href="#💡简介" class="headerlink" title="💡简介"></a>💡简介</h1><p>在<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="本地环境测试结果与视频对比分析">本地环境测试结果与视频对比分析</a>中，我们发现本地测试结果与KubeCon技术分享视频中的结果存在显著差异。虽然整体趋势基本一致，但在某些测试场景下，本地测试的CREATED事件曲线、SCHEDULED事件表现与视频预期不符。</p><p>为了深入分析这些差异的原因，我们提出了五种可能影响实验效果的猜想，并依次进行了系统性的实验验证。本文总结了这些猜想的验证过程、实验结果和最终结论，为Volcano调度器的性能优化提供了重要参考。</p><h1 id="🔍问题背景回顾"><a href="#🔍问题背景回顾" class="headerlink" title="🔍问题背景回顾"></a>🔍问题背景回顾</h1><h2 id="1-本地测试与视频结果差异"><a href="#1-本地测试与视频结果差异" class="headerlink" title="1. 本地测试与视频结果差异"></a>1. 本地测试与视频结果差异</h2><p>根据<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="前期分析">前期分析</a>，我们发现了以下主要差异：</p><table><thead><tr><th>测试场景</th><th>视频预期</th><th>本地实际</th><th>差异程度</th></tr></thead><tbody><tr><td><strong>10K Jobs × 1 Pod</strong></td><td>CREATED阶段瓶颈严重</td><td>✅ 符合预期</td><td>基本一致</td></tr><tr><td><strong>500 Jobs × 20 Pods</strong></td><td>CREATED阶段性突变</td><td>⚠️ 部分符合</td><td>中等差异</td></tr><tr><td><strong>20 Jobs × 500 Pods</strong></td><td>调度速度平稳</td><td>❌ 出现突变</td><td>显著差异</td></tr><tr><td><strong>1 Job × 10K Pods</strong></td><td>调度速度平稳</td><td>❌ 出现突变</td><td>显著差异</td></tr></tbody></table><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/Freshwlnd/image/refs/heads/blog/2025-KubeCon-A-Comparative-Analysis-5.png" alt="图1：性能测试中的参数配置"><figcaption>图1：性能测试中的参数配置</figcaption></figure></p><h2 id="2-差异现象分析"><a href="#2-差异现象分析" class="headerlink" title="2. 差异现象分析"></a>2. 差异现象分析</h2><p>这些差异主要表现为：</p><ol><li><strong>CREATED事件异常</strong>：在benchmark3和benchmark4中，CREATED事件出现阶段性突变，与视频中的平稳增长不符</li><li><strong>Pod创建数量不足</strong>：在某些测试中，实际创建的Pod数量远少于预期的10,000个</li><li><strong>调度性能瓶颈</strong>：调度器性能表现与预期存在较大差距</li></ol><h1 id="🧪五种猜想及其验证实验"><a href="#🧪五种猜想及其验证实验" class="headerlink" title="🧪五种猜想及其验证实验"></a>🧪五种猜想及其验证实验</h1><h2 id="猜想1：enqueue功能可能是性能瓶颈"><a href="#猜想1：enqueue功能可能是性能瓶颈" class="headerlink" title="猜想1：enqueue功能可能是性能瓶颈"></a>猜想1：enqueue功能可能是性能瓶颈</h2><h3 id="1-1-猜想依据"><a href="#1-1-猜想依据" class="headerlink" title="1.1 猜想依据"></a>1.1 猜想依据</h3><p>基于<a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="视频分析">视频分析</a>，我们猜测enqueue阶段可能会：</p><ul><li><strong>提前判断资源</strong>：在Pod创建前就判断资源是否充足</li><li><strong>限制Pod创建</strong>：当资源不足时，限制新Pod的创建速度</li><li><strong>影响CREATED事件</strong>：导致CREATED事件出现阶段性突变</li></ul><h3 id="1-2-实验设计"><a href="#1-2-实验设计" class="headerlink" title="1.2 实验设计"></a>1.2 实验设计</h3><p>我们通过<a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="禁用enqueue功能">禁用enqueue功能</a>来验证这一猜想：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改调度器配置，移除enqueue阶段</span></span><br><span class="line">actions: <span class="string">"allocate, backfill, reclaim"</span>  <span class="comment"># 原始：actions: "enqueue, allocate, backfill"</span></span><br></pre></td></tr></table></figure></div><h3 id="1-3-实验结果"><a href="#1-3-实验结果" class="headerlink" title="1.3 实验结果"></a>1.3 实验结果</h3><h4 id="第一种-Benchmark：10K-Jobs-×-1-Pod"><a href="#第一种-Benchmark：10K-Jobs-×-1-Pod" class="headerlink" title="第一种 Benchmark：10K Jobs × 1 Pod"></a>第一种 Benchmark：10K Jobs × 1 Pod</h4><p><strong>测试参数</strong>：每个Job只有1个Pod，共10K个Job，共10kPod</p><p><strong>实际结果</strong>：<strong>无明显变化</strong>。如下图所示，与<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="本地测试">本地测试</a>时的结果几乎一致。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/1-no-enqueue/a.NoGang-10KJob/output/panel-5.png?raw=true" alt="图2：对于猜想1，第一种benchmark测试结果"><figcaption>图2：对于猜想1，第一种benchmark测试结果</figcaption></figure></p><h4 id="第二种-Benchmark：500-Jobs-×-20-Pods"><a href="#第二种-Benchmark：500-Jobs-×-20-Pods" class="headerlink" title="第二种 Benchmark：500 Jobs × 20 Pods"></a>第二种 Benchmark：500 Jobs × 20 Pods</h4><p><strong>测试参数</strong>：每个Job有20个Pod，共500个Job，共10kPod</p><p><strong>实际结果</strong>：<strong>无明显变化</strong>。如下图所示，与<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="本地测试">本地测试</a>时的结果几乎一致。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/1-no-enqueue/b.NoGang-500Job-no/output/panel-5.png?raw=true" alt="图3：对于猜想1，第二种benchmark测试结果"><figcaption>图3：对于猜想1，第二种benchmark测试结果</figcaption></figure></p><h4 id="第三种-Benchmark：20-Jobs-×-500-Pods"><a href="#第三种-Benchmark：20-Jobs-×-500-Pods" class="headerlink" title="第三种 Benchmark：20 Jobs × 500 Pods"></a>第三种 Benchmark：20 Jobs × 500 Pods</h4><p><strong>测试参数</strong>：每个Job有500Pod，共20个Job，共10kPod</p><p><strong>实际结果</strong>：<strong>无明显变化</strong>。如下图所示，与<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="本地测试">本地测试</a>时的结果几乎一致，仍然存在“仅创建1000Pod”的bug。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/1-no-enqueue/c.NoGang-20Job-no/output/panel-5.png?raw=true" alt="图4：对于猜想1，第三种benchmark测试结果"><figcaption>图4：对于猜想1，第三种benchmark测试结果</figcaption></figure></p><h4 id="第四种-Benchmark：1-Job-×-10K-Pods"><a href="#第四种-Benchmark：1-Job-×-10K-Pods" class="headerlink" title="第四种 Benchmark：1 Job × 10K Pods"></a>第四种 Benchmark：1 Job × 10K Pods</h4><p><strong>实际结果</strong>：<strong>无明显变化</strong>。如下图所示，与<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="本地测试">本地测试</a>时的结果几乎一致。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/1-no-enqueue/c.NoGang-20Job-no/output/panel-5.png?raw=true" alt="图5：对于猜想1，第四种benchmark测试结果"><figcaption>图5：对于猜想1，第四种benchmark测试结果</figcaption></figure></p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><strong>实验结论</strong>：enqueue阶段不是性能瓶颈</p><p><strong>关键发现</strong>：</p><ul><li>禁用enqueue后，测试结果与前期本地测试结果基本一致</li><li>CREATED事件仍然出现阶段性突变</li><li>调度性能没有显著改善</li></ul><p><strong>分析说明</strong>：enqueue主要负责任务入队和优先级排序，对Pod创建和调度的直接影响有限。CREATED事件的异常现象可能源于其他因素。</p><h2 id="猜想2：webhook超时可能导致性能测试异常"><a href="#猜想2：webhook超时可能导致性能测试异常" class="headerlink" title="猜想2：webhook超时可能导致性能测试异常"></a>猜想2：webhook超时可能导致性能测试异常</h2><h3 id="2-1-猜想依据"><a href="#2-1-猜想依据" class="headerlink" title="2.1 猜想依据"></a>2.1 猜想依据</h3><p>在<a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="问题排查过程">问题排查过程</a>中，我们发现了严重的webhook超时问题：</p><ul><li><strong>Pod创建失败率</strong>：98.7%的Pod创建请求因超时而失败</li><li><strong>超时配置</strong>：webhook超时时间设置为10秒</li><li><strong>日志证据</strong>：4.9GB的审计日志记录了大量超时错误</li></ul><h3 id="2-2-实验设计"><a href="#2-2-实验设计" class="headerlink" title="2.2 实验设计"></a>2.2 实验设计</h3><p>我们通过修改webhook超时时间从10秒增加到30秒来验证这一猜想：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 批量修改所有webhook配置文件的超时时间</span></span><br><span class="line">sed -i <span class="string">'s/timeoutSeconds: 10/timeoutSeconds: 30/g'</span> schedulers/volcano/admission-service-*.yaml</span><br></pre></td></tr></table></figure></div><h3 id="2-3-实验结果"><a href="#2-3-实验结果" class="headerlink" title="2.3 实验结果"></a>2.3 实验结果</h3><h4 id="第一种-Benchmark：10K-Jobs-×-1-Pod-1"><a href="#第一种-Benchmark：10K-Jobs-×-1-Pod-1" class="headerlink" title="第一种 Benchmark：10K Jobs × 1 Pod"></a>第一种 Benchmark：10K Jobs × 1 Pod</h4><p><strong>测试参数</strong>：每个Job只有1个Pod，共10K个Job，共10kPod</p><p><strong>实际结果</strong>：<strong>无明显变化</strong>。如下图所示，与<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="本地测试">本地测试</a>时的结果几乎一致。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/2-long-webhook-timeout/a.NoGang-10KJob/output/panel-5.png?raw=true" alt="图6：对于猜想2，第一种benchmark测试结果"><figcaption>图6：对于猜想2，第一种benchmark测试结果</figcaption></figure></p><h4 id="第二种-Benchmark：500-Jobs-×-20-Pods-1"><a href="#第二种-Benchmark：500-Jobs-×-20-Pods-1" class="headerlink" title="第二种 Benchmark：500 Jobs × 20 Pods"></a>第二种 Benchmark：500 Jobs × 20 Pods</h4><p><strong>测试参数</strong>：每个Job有20个Pod，共500个Job，共10kPod</p><p><strong>实际结果</strong>：<strong>无明显变化</strong>。如下图所示，与<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="本地测试">本地测试</a>时的结果几乎一致。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/2-long-webhook-timeout/b.NoGang-500Job-no/output/panel-5.png?raw=true" alt="图7：对于猜想2，第二种benchmark测试结果"><figcaption>图7：对于猜想2，第二种benchmark测试结果</figcaption></figure></p><h4 id="第三种-Benchmark：20-Jobs-×-500-Pods-1"><a href="#第三种-Benchmark：20-Jobs-×-500-Pods-1" class="headerlink" title="第三种 Benchmark：20 Jobs × 500 Pods"></a>第三种 Benchmark：20 Jobs × 500 Pods</h4><p><strong>测试参数</strong>：每个Job有500Pod，共20个Job，共10kPod</p><p><strong>实际结果</strong>：<strong>✅有显著变化</strong>。如下图所示，与<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="本地测试">本地测试</a>时的结果完全不同，结果恢复为符合预期的正常状态“创建Pod数量达10000”。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/2-long-webhook-timeout/c.NoGang-20Job-no/output/panel-5.png?raw=true" alt="图8：对于猜想2，第三种benchmark测试结果"><figcaption>图8：对于猜想2，第三种benchmark测试结果</figcaption></figure></p><h4 id="第四种-Benchmark：1-Job-×-10K-Pods-1"><a href="#第四种-Benchmark：1-Job-×-10K-Pods-1" class="headerlink" title="第四种 Benchmark：1 Job × 10K Pods"></a>第四种 Benchmark：1 Job × 10K Pods</h4><p><strong>实际结果</strong>：<strong>✅有显著变化</strong>。如下图所示，与<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="本地测试">本地测试</a>时的结果完全不同，结果恢复为符合预期的正常状态“创建Pod数量达10000”。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/2-long-webhook-timeout/d.NoGang-1Job-no/output/panel-5.png?raw=true" alt="图9：对于猜想2，第四种benchmark测试结果"><figcaption>图9：对于猜想2，第四种benchmark测试结果</figcaption></figure></p><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p><strong>实验结论</strong>：webhook超时确实是导致性能测试异常的主要原因</p><p><strong>关键发现</strong>：</p><ul><li><strong>benchmark3和benchmark4</strong>：从”创建Pod数量不到1000”恢复为正常状态”创建Pod数量达10000”</li><li><strong>性能恢复</strong>：Pod创建成功率从1.3%提升到接近100%</li><li><strong>测试稳定性</strong>：大规模Pod创建测试能够正常完成</li></ul><p><strong>分析说明</strong>：webhook超时时间过短（10秒）无法处理大量并发Pod创建请求，导致请求堆积和失败。将超时时间延长到30秒后，系统能够正常处理高并发负载。这也为我们指出了在大规模下需要注意的配置问题。</p><h2 id="猜想3：Volcano版本较低可能导致性能测试结果异常"><a href="#猜想3：Volcano版本较低可能导致性能测试结果异常" class="headerlink" title="猜想3：Volcano版本较低可能导致性能测试结果异常"></a>猜想3：Volcano版本较低可能导致性能测试结果异常</h2><h3 id="3-1-猜想依据"><a href="#3-1-猜想依据" class="headerlink" title="3.1 猜想依据"></a>3.1 猜想依据</h3><p>基于版本差异可能带来的影响，我们猜测：</p><ul><li><strong>性能优化差异</strong>：新版本可能包含重要的性能优化</li><li><strong>算法改进差异</strong>：调度算法可能在新版本中有显著改进</li><li><strong>配置默认值差异</strong>：新版本的默认配置可能更适合大规模测试</li></ul><h3 id="3-2-实验设计"><a href="#3-2-实验设计" class="headerlink" title="3.2 实验设计"></a>3.2 实验设计</h3><p>我们通过将Volcano版本从v1.11.0升级到v1.12.0-alpha.0来验证这一猜想：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 批量替换版本号</span></span><br><span class="line">sed -i <span class="string">'s/v1\.11\.0/v1.12.0-alpha.0/g'</span> schedulers/volcano/*/deployment.yaml</span><br><span class="line">sed -i <span class="string">'s/v1\.11\.0/v1.12.0-alpha.0/g'</span> schedulers/volcano/*/job.yaml</span><br></pre></td></tr></table></figure></div><h3 id="3-3-实验结果"><a href="#3-3-实验结果" class="headerlink" title="3.3 实验结果"></a>3.3 实验结果</h3><h4 id="第一种-Benchmark：10K-Jobs-×-1-Pod-2"><a href="#第一种-Benchmark：10K-Jobs-×-1-Pod-2" class="headerlink" title="第一种 Benchmark：10K Jobs × 1 Pod"></a>第一种 Benchmark：10K Jobs × 1 Pod</h4><p><strong>测试参数</strong>：每个Job只有1个Pod，共10K个Job，共10kPod</p><p><strong>实际结果</strong>：<strong>无明显变化</strong>。如下图所示，与验证猜想2时的结果几乎一致。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/3-update-volcano-version-1.12.0-alpha.0/a.NoGang-10KJob/output/panel-5.png?raw=true" alt="图10：对于猜想3，第一种benchmark测试结果"><figcaption>图10：对于猜想3，第一种benchmark测试结果</figcaption></figure></p><h4 id="第二种-Benchmark：500-Jobs-×-20-Pods-2"><a href="#第二种-Benchmark：500-Jobs-×-20-Pods-2" class="headerlink" title="第二种 Benchmark：500 Jobs × 20 Pods"></a>第二种 Benchmark：500 Jobs × 20 Pods</h4><p><strong>测试参数</strong>：每个Job有20个Pod，共500个Job，共10kPod</p><p><strong>实际结果</strong>：<strong>无明显变化</strong>。如下图所示，与验证猜想2时的结果几乎一致。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/3-update-volcano-version-1.12.0-alpha.0/b.NoGang-500Job-no/output/panel-5.png?raw=true" alt="图11：对于猜想3，第二种benchmark测试结果"><figcaption>图11：对于猜想3，第二种benchmark测试结果</figcaption></figure></p><h4 id="第三种-Benchmark：20-Jobs-×-500-Pods-2"><a href="#第三种-Benchmark：20-Jobs-×-500-Pods-2" class="headerlink" title="第三种 Benchmark：20 Jobs × 500 Pods"></a>第三种 Benchmark：20 Jobs × 500 Pods</h4><p><strong>测试参数</strong>：每个Job有500Pod，共20个Job，共10kPod</p><p><strong>实际结果</strong>：<strong>无明显变化</strong>。如下图所示，与验证猜想2时的结果几乎一致。但CREATE速度似乎更快了些（也有可能只是随机波动）。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/3-update-volcano-version-1.12.0-alpha.0/c.NoGang-20Job-no/output/panel-5.png?raw=true" alt="图12：对于猜想3，第三种benchmark测试结果"><figcaption>图12：对于猜想3，第三种benchmark测试结果</figcaption></figure></p><h4 id="第四种-Benchmark：1-Job-×-10K-Pods-2"><a href="#第四种-Benchmark：1-Job-×-10K-Pods-2" class="headerlink" title="第四种 Benchmark：1 Job × 10K Pods"></a>第四种 Benchmark：1 Job × 10K Pods</h4><p><strong>实际结果</strong>：<strong>无明显变化</strong>。如下图所示，与验证猜想2时的结果几乎一致。但CREATE速度似乎更快了些（也有可能只是随机波动）。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/3-update-volcano-version-1.12.0-alpha.0/d.NoGang-1Job-no/output/panel-5.png?raw=true" alt="图13：对于猜想3，第四种benchmark测试结果"><figcaption>图13：对于猜想3，第四种benchmark测试结果</figcaption></figure></p><h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h4><p><strong>实验结论</strong>：Volcano版本不是导致性能测试结果差异的主要原因</p><p><strong>关键发现</strong>：</p><ul><li>升级到v1.12.0-alpha.0后，测试结果与前期本地测试结果基本一致</li><li>CREATED事件的异常现象仍然存在</li><li>调度性能没有显著改善</li></ul><p><strong>分析说明</strong>：虽然版本升级可能带来一些改进，但核心的性能瓶颈问题仍然存在。这表明性能差异主要源于配置和架构层面的问题，而非版本本身。</p><h2 id="猜想4：webhook处理可能是性能瓶颈"><a href="#猜想4：webhook处理可能是性能瓶颈" class="headerlink" title="猜想4：webhook处理可能是性能瓶颈"></a>猜想4：webhook处理可能是性能瓶颈</h2><h3 id="4-1-猜想依据"><a href="#4-1-猜想依据" class="headerlink" title="4.1 猜想依据"></a>4.1 猜想依据</h3><p>基于webhook系统的复杂性，我们猜测webhook处理本身可能成为性能瓶颈：</p><ul><li><strong>多次判断开销</strong>：每个Pod创建请求需要经过多个webhook验证</li><li><strong>TLS证书验证</strong>：每次webhook调用都需要进行TLS验证</li><li><strong>网络延迟</strong>：webhook服务调用可能引入额外的网络延迟</li></ul><h3 id="4-2-实验设计"><a href="#4-2-实验设计" class="headerlink" title="4.2 实验设计"></a>4.2 实验设计</h3><p>我们通过<a href="/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/" title="完全禁用webhook功能">完全禁用webhook功能</a>来验证这一猜想：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除所有webhook配置，保留admission服务</span></span><br><span class="line">make disable-volcano-webhooks</span><br></pre></td></tr></table></figure></div><h3 id="4-3-实验结果"><a href="#4-3-实验结果" class="headerlink" title="4.3 实验结果"></a>4.3 实验结果</h3><h4 id="第一种-Benchmark：10K-Jobs-×-1-Pod-3"><a href="#第一种-Benchmark：10K-Jobs-×-1-Pod-3" class="headerlink" title="第一种 Benchmark：10K Jobs × 1 Pod"></a>第一种 Benchmark：10K Jobs × 1 Pod</h4><p><strong>测试参数</strong>：每个Job只有1个Pod，共10K个Job，共10kPod</p><p><strong>实际结果</strong>：<strong>✅性能明显上升</strong>。如下两图对比所示，整体斜率比前期测试结果更大，说明CREATED和SCHEDULE的速度更快。</p><p>优化前：<br><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/3-update-volcano-version-1.12.0-alpha.0/a.NoGang-10KJob/output/panel-5.png?raw=true" alt="图10：对于猜想3，第一种benchmark测试结果"><figcaption>图10：对于猜想3，第一种benchmark测试结果</figcaption></figure></p><p>优化后：<br><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/4-disable-webhook/a.NoGang-10KJob/output/panel-5.png?raw=true" alt="图14：对于猜想4，第一种benchmark测试结果"><figcaption>图14：对于猜想4，第一种benchmark测试结果</figcaption></figure></p><h4 id="第二种-Benchmark：500-Jobs-×-20-Pods-3"><a href="#第二种-Benchmark：500-Jobs-×-20-Pods-3" class="headerlink" title="第二种 Benchmark：500 Jobs × 20 Pods"></a>第二种 Benchmark：500 Jobs × 20 Pods</h4><p><strong>测试参数</strong>：每个Job有20个Pod，共500个Job，共10kPod</p><p><strong>实际结果</strong>：<strong>✅性能略有上升</strong>。如下两图对比所示，CREATE斜率比前期测试结果更大（甚至好几段近乎直线上升），说明CREATED的速度显著上升；但与此同时需要注意的是，<strong>CREATE仍然存在阶梯状</strong>突变，证明CREATE瓶颈仍然需要通过其他方式解决。</p><p>优化前：<br><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/3-update-volcano-version-1.12.0-alpha.0/b.NoGang-500Job-no/output/panel-5.png?raw=true" alt="图11：对于猜想3，第二种benchmark测试结果"><figcaption>图11：对于猜想3，第二种benchmark测试结果</figcaption></figure></p><p>优化后：<br><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/4-disable-webhook/b.NoGang-500Job-no/output/panel-5.png?raw=true" alt="图15：对于猜想4，第二种benchmark测试结果"><figcaption>图15：对于猜想4，第二种benchmark测试结果</figcaption></figure></p><h4 id="第三种-Benchmark：20-Jobs-×-500-Pods-3"><a href="#第三种-Benchmark：20-Jobs-×-500-Pods-3" class="headerlink" title="第三种 Benchmark：20 Jobs × 500 Pods"></a>第三种 Benchmark：20 Jobs × 500 Pods</h4><p><strong>测试参数</strong>：每个Job有500Pod，共20个Job，共10kPod</p><p><strong>✅性能明显上升</strong>。如下两图对比所示，整体斜率比前期测试结果更大，说明CREATED和SCHEDULE的速度更快。同时也注意到，即便如此也还是比另外两种调度器更慢些，意味着SCHEDULE部分调度性能本身也还有优化空间。</p><p>优化前：<br><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/3-update-volcano-version-1.12.0-alpha.0/c.NoGang-20Job-no/output/panel-5.png?raw=true" alt="图12：对于猜想3，第三种benchmark测试结果"><figcaption>图12：对于猜想3，第三种benchmark测试结果</figcaption></figure></p><p>优化后：<br><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/4-disable-webhook/c.NoGang-20Job-no/output/panel-5.png?raw=true" alt="图16：对于猜想4，第三种benchmark测试结果"><figcaption>图16：对于猜想4，第三种benchmark测试结果</figcaption></figure></p><h4 id="第四种-Benchmark：1-Job-×-10K-Pods-3"><a href="#第四种-Benchmark：1-Job-×-10K-Pods-3" class="headerlink" title="第四种 Benchmark：1 Job × 10K Pods"></a>第四种 Benchmark：1 Job × 10K Pods</h4><p><strong>✅性能明显上升</strong>。如下两图对比所示，整体斜率比前期测试结果更大，说明CREATED和SCHEDULE的速度更快。同时和benchmark3一样，也注意到，即便如此也还是比另外两种调度器更慢些，意味着SCHEDULE部分调度性能本身也还有优化空间。</p><p>优化前：<br><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/3-update-volcano-version-1.12.0-alpha.0/d.NoGang-1Job-no/output/panel-5.png?raw=true" alt="图13：对于猜想3，第四种benchmark测试结果"><figcaption>图13：对于猜想3，第四种benchmark测试结果</figcaption></figure></p><p>优化后：<br><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/4-disable-webhook/d.NoGang-1Job-no/output/panel-5.png?raw=true" alt="图17：对于猜想4，第四种benchmark测试结果"><figcaption>图17：对于猜想4，第四种benchmark测试结果</figcaption></figure></p><h4 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h4><p><strong>实验结论</strong>：webhook处理确实是重要的性能瓶颈</p><p><strong>关键发现</strong>：禁用webhook后，性能获得较大提升，Pod创建/调度速度显著加快，调度器整体性能表现改善</p><p><strong>分析说明</strong>：webhook系统虽然提供了重要的验证和修改功能，但在大规模Pod创建场景下，其处理开销成为了性能瓶颈。禁用webhook后，系统能够更直接地处理Pod创建请求，从而提升整体性能。</p><h2 id="猜想5：Volcano批处理机制可能导致CREATED阶段瓶颈"><a href="#猜想5：Volcano批处理机制可能导致CREATED阶段瓶颈" class="headerlink" title="猜想5：Volcano批处理机制可能导致CREATED阶段瓶颈"></a>猜想5：Volcano批处理机制可能导致CREATED阶段瓶颈</h2><h3 id="5-1-猜想依据"><a href="#5-1-猜想依据" class="headerlink" title="5.1 猜想依据"></a>5.1 猜想依据</h3><p>基于<a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="视频分析">视频分析</a>，我们注意到在benchmark1和benchmark2下，CREATED阶段成为性能瓶颈。视频中提到Volcano会”create pod in Batch”，即分批处理Job，当一批Job处理完后才会继续处理下一批Job。</p><p>这种批处理机制可能导致：</p><ul><li><strong>CREATED事件阶段性突变</strong>：批处理完成后，下一批Job的Pod创建会出现集中爆发</li><li><strong>资源浪费</strong>：批处理期间资源可能被闲置，且批处理完成后资源竞争激烈</li></ul><h3 id="5-2-实验验证"><a href="#5-2-实验验证" class="headerlink" title="5.2 实验验证"></a>5.2 实验验证</h3><p>虽然我们没有针对这一猜想进行专门的测试，但从前面所有实验结果都能证明这一点（尤其是benchmark1和benchmark2）：</p><ol><li><strong>enqueue实验</strong>：禁用enqueue后，CREATED事件仍然出现阶段性突变，说明问题不在enqueue阶段</li><li><strong>webhook超时实验</strong>：修复超时问题后，Pod创建数量恢复正常，但CREATED的阶段性特征仍然存在</li><li><strong>版本升级实验</strong>：升级到v1.12.0-alpha.0后，CREATED事件的异常现象仍然存在</li><li><strong>webhook禁用实验</strong>：即使禁用webhook，在Job数量较多时仍然存在Pod创建瓶颈</li></ol><p>这些实验结果的一致性表明，CREATED阶段的瓶颈问题源于更深层的架构设计，即Volcano的批处理机制。</p><h3 id="5-3-实验结论"><a href="#5-3-实验结论" class="headerlink" title="5.3 实验结论"></a>5.3 实验结论</h3><p><strong>实验结论</strong>：Volcano的批处理机制确实是CREATED阶段性能瓶颈的根本原因</p><p><strong>关键发现</strong>：</p><ul><li>批处理机制导致Pod创建出现阶段性突变</li><li>这种瓶颈无法通过调整配置参数完全解决</li><li>需要从架构层面进行优化</li></ul><p><strong>分析说明</strong>：Volcano的批处理设计虽然在某些场景下有利于资源管理和调度优化，但在大规模、高并发的Pod创建场景下，这种同步批处理机制成为了性能瓶颈。系统需要等待当前批次完成才能开始下一批次的处理，无法实现真正的并行流水线。</p><h1 id="📊实验结果综合分析"><a href="#📊实验结果综合分析" class="headerlink" title="📊实验结果综合分析"></a>📊实验结果综合分析</h1><table><thead><tr><th>猜想</th><th>验证方法</th><th>实验结果</th><th>结论</th></tr></thead><tbody><tr><td><strong>enqueue功能瓶颈</strong></td><td>禁用enqueue阶段</td><td>性能无显著改善</td><td>❌ 不是主要瓶颈</td></tr><tr><td><strong>webhook超时问题</strong></td><td>延长超时时间</td><td>Pod创建恢复正常</td><td>✅ 是重要瓶颈</td></tr><tr><td><strong>版本差异影响</strong></td><td>升级到v1.12.0-alpha.0</td><td>性能无显著改善</td><td>❌ 不是主要瓶颈</td></tr><tr><td><strong>webhook处理瓶颈</strong></td><td>完全禁用webhook</td><td>性能大幅提升</td><td>✅ 是主要瓶颈</td></tr><tr><td><strong>批处理机制瓶颈</strong></td><td>综合分析所有实验结果</td><td>CREATED阶段性突变持续存在</td><td>✅ 是根本瓶颈</td></tr></tbody></table><h1 id="🚀未来方向"><a href="#🚀未来方向" class="headerlink" title="🚀未来方向"></a>🚀未来方向</h1><h2 id="1-短期优化方案"><a href="#1-短期优化方案" class="headerlink" title="1. 短期优化方案"></a>1. 短期优化方案</h2><h3 id="1-1-调整webhook超时时间"><a href="#1-1-调整webhook超时时间" class="headerlink" title="1.1 调整webhook超时时间"></a>1.1 调整webhook超时时间</h3><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将webhook超时时间从10秒增加到30秒</span></span><br><span class="line"><span class="attr">timeoutSeconds:</span> <span class="number">30</span>  <span class="comment"># 原始：timeoutSeconds: 10</span></span><br></pre></td></tr></table></figure></div><p><strong>适用场景</strong>：需要保持webhook功能完整性的生产环境<br><strong>优化效果</strong>：解决超时导致的测试异常问题</p><h3 id="1-2-优化webhook资源配置"><a href="#1-2-优化webhook资源配置" class="headerlink" title="1.2 优化webhook资源配置"></a>1.2 优化webhook资源配置</h3><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加webhook服务的资源限制</span></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line">  <span class="attr">requests:</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">"512Mi"</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">"500m"</span></span><br><span class="line">  <span class="attr">limits:</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">"1Gi"</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">"1000m"</span></span><br></pre></td></tr></table></figure></div><p><strong>适用场景</strong>：资源受限但需要webhook功能的环境<br><strong>优化效果</strong>：提升webhook处理能力</p><h2 id="2-长期优化方向"><a href="#2-长期优化方向" class="headerlink" title="2. 长期优化方向"></a>2. 长期优化方向</h2><h3 id="2-1-替代webhook的验证机制"><a href="#2-1-替代webhook的验证机制" class="headerlink" title="2.1 替代webhook的验证机制"></a>2.1 替代webhook的验证机制</h3><p>基于<a href="/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/" title="我们的实验发现">我们的实验发现</a>，未来可以考虑：</p><ol><li><strong>CRD验证规则</strong>：使用通用表达式语言（CEL）来实现K8s准入校验规则，验证CRD的值（使用 K8s v1.29 [stabe] x-kubernetes-validations 扩展），实现利用 Kubernetes CRD 的验证功能替代部分 validating webhook</li><li><strong>预配置模板</strong>：通过提前配置减少运行时的修改需求</li></ol><h3 id="2-2-优化CREATED批处理阻塞瓶颈"><a href="#2-2-优化CREATED批处理阻塞瓶颈" class="headerlink" title="2.2 优化CREATED批处理阻塞瓶颈"></a>2.2 优化CREATED批处理阻塞瓶颈</h3><p>基于第五个猜想的验证结果，针对Volcano批处理机制的根本性瓶颈，未来可以考虑：</p><ol><li><strong>动态批次调整</strong>：根据系统负载动态调整批次大小，避免资源闲置和突发负载</li><li><strong>异步批处理</strong>：将同步批处理改为异步处理，不阻塞下一批Job的Pod创建</li><li><strong>流水线优化</strong>：设计真正的流水线机制，实现Pod创建、验证、调度的并行处理</li></ol><!-- ### 2.3 异步处理机制设计异步的Pod验证和修改机制：1. **异步验证**：Pod创建后异步进行验证，不阻塞创建流程2. **批量处理**：将多个验证请求批量处理，提高效率3. **缓存机制**：缓存验证结果，减少重复计算--><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[1] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a href="!--swig%EF%BF%BC36--">[2] <a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></a></p><p><a href="!--swig%EF%BF%BC38--">[3] <a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></a></p><p><a href="!--swig%EF%BF%BC40--">[4] <a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试">云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试</a></a></p><p><a href="!--swig%EF%BF%BC42--">[5] <a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复">云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</a></a></p><p><a href="!--swig%EF%BF%BC44--">[6] <a href="/2025/08/20/k8s/k8s-scheduler-performance-volcano-version/" title="云原生批调度实战：Volcano版本修改与性能测试优化">云原生批调度实战：Volcano版本修改与性能测试优化</a></a></p><p><a href="!--swig%EF%BF%BC46--">[7] <a href="/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/" title="云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析">云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析</a></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/admission-controllers/">[8] Kubernetes 文档 - Admission Controllers 准入控制 <i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/">[9] Kubernetes 文档 - Kubernetes 1.25: CustomResourceDefinition Validation Rules Graduate to Beta<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions">[10] Kubernetes 文档 - 使用 CustomResourceDefinition 扩展 Kubernetes API<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation">[11] Kubernetes 文档 - CRD Validation 合法性检查<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">[12] Kubernetes 文档 - CRD Validation Rules 合法性检查规则<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">[13] Kubernetes 文档 - CRD Validation Rules 合法性检查规则（K8s v1.29 [stabe]）<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/validating-admission-policy/">[14] Kubernetes 文档 - 验证准入策略（ValidatingAdmissionPolicy）（K8s v1.30 [stable]）<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://liangyuanpeng.com/post/k8s-admissionregistration-with-cel/">[15] 用cel表达式来实现k8s准入校验<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">本文总结了针对Volcano调度器性能测试结果异常的四种猜想及其验证实验，包括enqueue功能、webhook超时、版本差异和webhook处理瓶颈。通过系统性的实验验证，我们成功识别了webhook超时和webhook处理是主要的性能瓶颈，为调度器优化提供了重要参考。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
    <category term="性能瓶颈" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88/"/>
    
    <category term="实验验证" scheme="https://freshwlnd.github.io/tags/%E5%AE%9E%E9%AA%8C%E9%AA%8C%E8%AF%81/"/>
    
    <category term="系统优化" scheme="https://freshwlnd.github.io/tags/%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析</title>
    <link href="https://freshwlnd.github.io/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/"/>
    <id>https://freshwlnd.github.io/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/</id>
    <published>2025-08-22T14:55:54.000Z</published>
    <updated>2025-08-26T14:47:24.021Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li><li><a href="/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/" title="监控与测试环境解析：测试流程拆解篇">监控与测试环境解析：测试流程拆解篇</a></li><li><a href="/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/" title="监控与测试环境解析：指标采集与可视化篇">监控与测试环境解析：指标采集与可视化篇</a></li><li><a href="/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/" title="监控与测试环境解析：自定义镜像性能回归测试">监控与测试环境解析：自定义镜像性能回归测试</a></li><li><a href="/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/" title="监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题">监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题</a></li><li><a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试">云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试</a></li><li><a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复">云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</a></li><li><a href="/2025/08/20/k8s/k8s-scheduler-performance-volcano-version/" title="云原生批调度实战：Volcano版本修改与性能测试优化">云原生批调度实战：Volcano版本修改与性能测试优化</a></li><li><a href="/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/" title="云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析">云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析</a></li><li><a href="/2025/08/24/k8s/k8s-scheduler-performance-volcano-hypothesis-verification/" title="云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结">云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结</a></li></ol></blockquote><h1 id="💡简介"><a href="#💡简介" class="headerlink" title="💡简介"></a>💡简介</h1><p>在<a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="上一篇博客">上一篇博客</a>中，我们通过修改webhook超时时间解决了Pod创建数量不足的问题。然而，在实际测试过程中，我们发现webhook本身可能成为性能瓶颈，特别是在大规模Job提交的场景下。</p><p>本文详细介绍了如何通过修改<code>kube-scheduling-perf</code>的Makefile来完全禁用Volcano的webhook功能，分析了webhook对调度性能的影响，并探讨了未来通过CRD替代webhook的可能性。通过实验发现，即使禁用webhook，在Job数量较多时仍然存在Pod创建瓶颈，这为后续的调度器优化提供了重要参考。</p><h1 id="🖼️背景需求分析"><a href="#🖼️背景需求分析" class="headerlink" title="🖼️背景需求分析"></a>🖼️背景需求分析</h1><h2 id="1-Webhook性能瓶颈问题"><a href="#1-Webhook性能瓶颈问题" class="headerlink" title="1. Webhook性能瓶颈问题"></a>1. Webhook性能瓶颈问题</h2><h3 id="1-1-多次判断的性能开销"><a href="#1-1-多次判断的性能开销" class="headerlink" title="1.1 多次判断的性能开销"></a>1.1 多次判断的性能开销</h3><p>Volcano的webhook系统包含多个组件，每个Pod创建请求都需要经过：</p><ol><li><strong>Mutating Webhook</strong>：修改Pod配置（如添加<code>maxRetry</code>、<code>minAvailable</code>等字段）</li><li><strong>Validating Webhook</strong>：验证Pod配置的合法性</li><li><strong>Admission Service</strong>：处理webhook请求的服务</li><li><strong>TLS证书验证</strong>：确保webhook调用的安全性</li></ol><p>在<a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="之前的测试">之前的测试</a>中，我们发现webhook超时成为了Pod创建失败的主要原因，98.7%的请求因超时而失败。</p><h3 id="1-2-替代方案的可能性"><a href="#1-2-替代方案的可能性" class="headerlink" title="1.2 替代方案的可能性"></a>1.2 替代方案的可能性</h3><p>考虑到现代Kubernetes版本的发展趋势，webhook的某些功能可以通过以下方式替代：</p><ol><li><strong>CRD验证</strong>：在Kubernetes较新版本中，可以通过CRD的验证规则替代部分validating webhook</li><li><strong>准入策略</strong>：使用Pod Security Standards等内置策略替代部分验证逻辑</li><li><strong>预配置模板</strong>：通过提前配置Job模板，减少运行时的修改需求</li></ol><h3 id="1-3-性能测试需求"><a href="#1-3-性能测试需求" class="headerlink" title="1.3 性能测试需求"></a>1.3 性能测试需求</h3><p>为了准确评估webhook对调度性能的影响，我们需要：</p><ul><li><strong>基准测试</strong>：在有webhook的情况下进行性能测试</li><li><strong>对比测试</strong>：在禁用webhook的情况下进行相同的性能测试</li><li><strong>瓶颈分析</strong>：识别webhook是否是真正的性能瓶颈</li></ul><h1 id="🔧方案设计与实现"><a href="#🔧方案设计与实现" class="headerlink" title="🔧方案设计与实现"></a>🔧方案设计与实现</h1><h2 id="1-整体设计思路"><a href="#1-整体设计思路" class="headerlink" title="1. 整体设计思路"></a>1. 整体设计思路</h2><h3 id="1-1-保留核心组件"><a href="#1-1-保留核心组件" class="headerlink" title="1.1 保留核心组件"></a>1.1 保留核心组件</h3><p>我们采用<strong>选择性禁用</strong>的策略，只删除webhook配置，保留其他重要组件：</p><ul><li>✅ <strong>保留</strong>：<code>volcano-admission</code> deployment、<code>volcano-admission-service</code>、<code>volcano-admission-init</code> job</li><li>❌ <strong>删除</strong>：所有<code>MutatingWebhookConfiguration</code>和<code>ValidatingWebhookConfiguration</code></li></ul><h3 id="1-2-实现方式"><a href="#1-2-实现方式" class="headerlink" title="1.2 实现方式"></a>1.2 实现方式</h3><p>通过修改<code>./clusters/volcano/Makefile</code>，在Volcano部署完成后自动删除webhook配置：</p><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: disable-volcano-webhooks</span></span><br><span class="line"><span class="section">disable-volcano-webhooks:</span></span><br><span class="line"><span class="comment"># 只删除webhook配置，保留admission服务和初始化组件</span></span><br><span class="line"><span class="comment"># 删除所有MutatingWebhookConfiguration</span></span><br><span class="line">-KUBECONFIG=<span class="variable">$(KUBECONFIG)</span> kubectl delete mutatingwebhookconfiguration volcano-admission-service-jobs-mutate</span><br><span class="line">-KUBECONFIG=<span class="variable">$(KUBECONFIG)</span> kubectl delete mutatingwebhookconfiguration volcano-admission-service-podgroups-mutate</span><br><span class="line">-KUBECONFIG=<span class="variable">$(KUBECONFIG)</span> kubectl delete mutatingwebhookconfiguration volcano-admission-service-pods-mutate</span><br><span class="line">-KUBECONFIG=<span class="variable">$(KUBECONFIG)</span> kubectl delete mutatingwebhookconfiguration volcano-admission-service-queues-mutate</span><br><span class="line"><span class="comment"># 删除所有ValidatingWebhookConfiguration</span></span><br><span class="line">-KUBECONFIG=<span class="variable">$(KUBECONFIG)</span> kubectl delete validatingwebhookconfiguration volcano-admission-service-jobs-validate</span><br><span class="line">-KUBECONFIG=<span class="variable">$(KUBECONFIG)</span> kubectl delete validatingwebhookconfiguration volcano-admission-service-pods-validate</span><br><span class="line">-KUBECONFIG=<span class="variable">$(KUBECONFIG)</span> kubectl delete validatingwebhookconfiguration volcano-admission-service-queues-validate</span><br><span class="line">@echo <span class="string">"Volcano webhook configurations have been disabled, but admission service and init components are preserved"</span></span><br></pre></td></tr></table></figure></div><h2 id="2-具体实现步骤"><a href="#2-具体实现步骤" class="headerlink" title="2. 具体实现步骤"></a>2. 具体实现步骤</h2><h3 id="2-1-修改Makefile"><a href="#2-1-修改Makefile" class="headerlink" title="2.1 修改Makefile"></a>2.1 修改Makefile</h3><p>在<code>clusters/volcano/Makefile</code>中，我们添加了多个webhook控制选项：</p><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 完全禁用webhook</span></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: disable-volcano-webhooks</span></span><br><span class="line"><span class="section">disable-volcano-webhooks:</span></span><br><span class="line"><span class="comment"># 删除所有webhook配置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只禁用mutating webhook</span></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: disable-volcano-mutating-webhooks</span></span><br><span class="line"><span class="section">disable-volcano-mutating-webhooks:</span></span><br><span class="line"><span class="comment"># 只删除MutatingWebhookConfiguration</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只禁用validating webhook</span></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: disable-volcano-validating-webhooks</span></span><br><span class="line"><span class="section">disable-volcano-validating-webhooks:</span></span><br><span class="line"><span class="comment"># 只删除ValidatingWebhookConfiguration</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新启用webhook</span></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: enable-volcano-webhooks</span></span><br><span class="line"><span class="section">enable-volcano-webhooks:</span></span><br><span class="line"><span class="comment"># 重新创建所有webhook配置</span></span><br></pre></td></tr></table></figure></div><h3 id="2-2-集成到部署流程"><a href="#2-2-集成到部署流程" class="headerlink" title="2.2 集成到部署流程"></a>2.2 集成到部署流程</h3><p>在<code>up</code>目标中集成webhook禁用：</p><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: up</span></span><br><span class="line"><span class="section">up: \</span></span><br><span class="line">create-cluster</span><br><span class="line">make wait</span><br><span class="line">make -j 2 \</span><br><span class="line">create-kwok \</span><br><span class="line">create-volcano</span><br><span class="line">make disable-volcano-webhooks  <span class="comment"># 自动禁用webhook</span></span><br><span class="line">make limit-built-controller-manager</span><br><span class="line">make uncordon</span><br></pre></td></tr></table></figure></div><h2 id="3-技术细节说明"><a href="#3-技术细节说明" class="headerlink" title="3. 技术细节说明"></a>3. 技术细节说明</h2><h3 id="3-1-Webhook配置类型"><a href="#3-1-Webhook配置类型" class="headerlink" title="3.1 Webhook配置类型"></a>3.1 Webhook配置类型</h3><p>我们禁用的webhook配置包括：</p><table><thead><tr><th>类型</th><th>配置名称</th><th>功能描述</th></tr></thead><tbody><tr><td><strong>Mutating</strong></td><td><code>volcano-admission-service-jobs-mutate</code></td><td>修改Job配置，添加<code>maxRetry</code>等字段</td></tr><tr><td><strong>Mutating</strong></td><td><code>volcano-admission-service-podgroups-mutate</code></td><td>修改PodGroup配置</td></tr><tr><td><strong>Mutating</strong></td><td><code>volcano-admission-service-pods-mutate</code></td><td>修改Pod配置</td></tr><tr><td><strong>Mutating</strong></td><td><code>volcano-admission-service-queues-mutate</code></td><td>修改Queue配置</td></tr><tr><td><strong>Validating</strong></td><td><code>volcano-admission-service-jobs-validate</code></td><td>验证Job配置合法性</td></tr><tr><td><strong>Validating</strong></td><td><code>volcano-admission-service-pods-validate</code></td><td>验证Pod配置合法性</td></tr><tr><td><strong>Validating</strong></td><td><code>volcano-admission-service-queues-validate</code></td><td>验证Queue配置合法性</td></tr></tbody></table><h3 id="3-2-保留组件的原因"><a href="#3-2-保留组件的原因" class="headerlink" title="3.2 保留组件的原因"></a>3.2 保留组件的原因</h3><p>我们选择保留以下组件的原因：</p><ol><li><strong><code>volcano-admission</code> deployment</strong>：保持服务架构完整性，避免其他组件依赖问题</li><li>**<code>volcano-admission-service</code>**：维持网络服务，避免服务发现失败</li><li><strong><code>volcano-admission-init</code> job</strong>：继续生成TLS证书，确保系统安全性</li></ol><h1 id="🚀未来展望与优化方向"><a href="#🚀未来展望与优化方向" class="headerlink" title="🚀未来展望与优化方向"></a>🚀未来展望与优化方向</h1><h2 id="1-CRD替代Webhook的可能性"><a href="#1-CRD替代Webhook的可能性" class="headerlink" title="1. CRD替代Webhook的可能性"></a>1. CRD替代Webhook的可能性</h2><h3 id="1-1-Kubernetes版本演进"><a href="#1-1-Kubernetes版本演进" class="headerlink" title="1.1 Kubernetes版本演进"></a>1.1 Kubernetes版本演进</h3><p>从Kubernetes 1.16开始，CRD支持更强大的验证和默认值设置：</p><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apiextensions.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CustomResourceDefinition</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">validation:</span></span><br><span class="line">    <span class="attr">openAPIV3Schema:</span></span><br><span class="line">      <span class="comment"># 可以定义复杂的验证规则</span></span><br><span class="line">  <span class="attr">defaulting:</span></span><br><span class="line">    <span class="comment"># 可以设置默认值</span></span><br></pre></td></tr></table></figure></div><h3 id="1-2-替代方案设计"><a href="#1-2-替代方案设计" class="headerlink" title="1.2 替代方案设计"></a>1.2 替代方案设计</h3><p>未来可以考虑通过以下方式替代webhook：</p><ol><li><strong>CRD验证规则</strong>：在Job CRD中定义验证规则，替代validating webhook</li><li><strong>默认值设置</strong>：通过CRD的defaulting功能，替代mutating webhook</li><li><strong>准入策略</strong>：使用Pod Security Standards等内置策略</li></ol><h3 id="1-3-实现路径"><a href="#1-3-实现路径" class="headerlink" title="1.3 实现路径"></a>1.3 实现路径</h3><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例：通过CRD实现Job验证</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apiextensions.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CustomResourceDefinition</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">jobs.batch.volcano.sh</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">validation:</span></span><br><span class="line">    <span class="attr">openAPIV3Schema:</span></span><br><span class="line">      <span class="attr">properties:</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">properties:</span></span><br><span class="line">            <span class="attr">maxRetry:</span></span><br><span class="line">              <span class="attr">type:</span> <span class="string">integer</span></span><br><span class="line">              <span class="attr">minimum:</span> <span class="number">1</span></span><br><span class="line">              <span class="attr">maximum:</span> <span class="number">10</span></span><br><span class="line">            <span class="attr">minAvailable:</span></span><br><span class="line">              <span class="attr">type:</span> <span class="string">integer</span></span><br><span class="line">              <span class="attr">minimum:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">defaulting:</span></span><br><span class="line">    <span class="attr">openAPIV3Schema:</span></span><br><span class="line">      <span class="attr">properties:</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">properties:</span></span><br><span class="line">            <span class="attr">maxRetry:</span></span><br><span class="line">              <span class="attr">default:</span> <span class="number">3</span></span><br></pre></td></tr></table></figure></div><h2 id="2-Controller调度逻辑优化"><a href="#2-Controller调度逻辑优化" class="headerlink" title="2. Controller调度逻辑优化"></a>2. Controller调度逻辑优化</h2><p>基于测试结果，我们发现前期确定的优化点仍然存在。具体而言：</p><ol><li><strong>实验结果发现</strong>：即使禁用了webhook，在Job数量很多时仍然存在<a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="前期测试">前期测试</a>中提到的Pod CREATE 瓶颈。</li><li><strong>可能原因</strong>：Volcano会分批处理Job并CREATE Pod，当Job数量多时CREATE速度比SCHEDULE速度慢而成为瓶颈。</li><li><strong>后续思路</strong>：可以通过检查和修改controller的调度逻辑来进行优化。</li></ol><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[1] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a href="!--swig%EF%BF%BC22--">[2] <a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复">云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</a></a></p><p><a href="!--swig%EF%BF%BC24--">[3] <a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></a></p><p><a class="link" href="https://volcano.sh/zh/docs/architecture/">[4] Volcano 文档 - 架构<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/">[5] Kubernetes 文档 - Kubernetes 1.25: CustomResourceDefinition Validation Rules Graduate to Beta<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions">[6] Kubernetes 文档 - 使用 CustomResourceDefinition 扩展 Kubernetes API<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation">[7] Kubernetes 文档 - CRD Validation 合法性检查<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">[8] Kubernetes 文档 - CRD Validation Rules 合法性检查规则<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">[9] Kubernetes 文档 - CRD Validation Rules 合法性检查规则（K8s v1.29 [stabe]）<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/validating-admission-policy/">[10] Kubernetes 文档 - 验证准入策略（ValidatingAdmissionPolicy）（K8s v1.30 [stable]）<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://liangyuanpeng.com/post/k8s-admissionregistration-with-cel/">[11] 用cel表达式来实现k8s准入校验<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a href="!--swig%EF%BF%BC26--">[12] <a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></a> </p>]]></content>
    
    
    <summary type="html">本文详细介绍了如何通过修改kube-scheduling-perf的Makefile来禁用Volcano的webhook功能，分析了webhook对调度性能的影响，并探讨了未来通过CRD替代webhook的可能性。通过实验发现，即使禁用webhook，在Job数量较多时仍然存在Pod创建瓶颈，这为后续的调度器优化提供了重要参考。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="Webhook" scheme="https://freshwlnd.github.io/tags/Webhook/"/>
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="性能优化" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
    <category term="CRD" scheme="https://freshwlnd.github.io/tags/CRD/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：Volcano版本修改与性能测试优化</title>
    <link href="https://freshwlnd.github.io/2025/08/20/k8s/k8s-scheduler-performance-volcano-version/"/>
    <id>https://freshwlnd.github.io/2025/08/20/k8s/k8s-scheduler-performance-volcano-version/</id>
    <published>2025-08-19T17:25:19.000Z</published>
    <updated>2025-08-24T07:22:28.703Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li><li><a href="/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/" title="监控与测试环境解析：测试流程拆解篇">监控与测试环境解析：测试流程拆解篇</a></li><li><a href="/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/" title="监控与测试环境解析：指标采集与可视化篇">监控与测试环境解析：指标采集与可视化篇</a></li><li><a href="/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/" title="监控与测试环境解析：自定义镜像性能回归测试">监控与测试环境解析：自定义镜像性能回归测试</a></li><li><a href="/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/" title="监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题">监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题</a></li><li><a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试">云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试</a></li><li><a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复">云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</a></li><li><a href="/2025/08/20/k8s/k8s-scheduler-performance-volcano-version/" title="云原生批调度实战：Volcano版本修改与性能测试优化">云原生批调度实战：Volcano版本修改与性能测试优化</a></li><li><a href="/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/" title="云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析">云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析</a></li><li><a href="/2025/08/24/k8s/k8s-scheduler-performance-volcano-hypothesis-verification/" title="云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结">云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结</a></li></ol></blockquote><h1 id="💡简介"><a href="#💡简介" class="headerlink" title="💡简介"></a>💡简介</h1><p>在<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="本地环境测试结果与视频对比分析">本地环境测试结果与视频对比分析</a>中，我们发现本地测试结果与KubeCon技术分享视频中的结果存在显著差异。虽然整体趋势基本一致，但在某些测试场景下，本地测试的CREATED事件曲线、SCHEDULED事件表现与视频预期不符。</p><p>通过深入分析，我们猜测这些差异可能源于<strong>Volcano调度器版本不同</strong>。视频中使用的可能是较新的版本（如1.12.0-alpha.0），而本地环境使用的是较旧版本（v1.11.0）。本文详细介绍了如何查看当前测试所用的Volcano版本，如何自动化升级到目标版本，以及如何验证版本升级的效果。</p><h1 id="🔍问题回顾与分析"><a href="#🔍问题回顾与分析" class="headerlink" title="🔍问题回顾与分析"></a>🔍问题回顾与分析</h1><h2 id="1-本地测试与视频结果差异"><a href="#1-本地测试与视频结果差异" class="headerlink" title="1. 本地测试与视频结果差异"></a>1. 本地测试与视频结果差异</h2><h3 id="1-1-差异现象总结"><a href="#1-1-差异现象总结" class="headerlink" title="1.1 差异现象总结"></a>1.1 差异现象总结</h3><p>根据<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="本地测试结果">本地测试结果</a>，我们发现了以下主要差异：</p><table><thead><tr><th>测试场景</th><th>视频预期</th><th>本地实际</th><th>差异分析</th></tr></thead><tbody><tr><td><strong>10K Jobs × 1 Pod</strong></td><td>YuniKorn吞吐量最高</td><td>✅ 符合预期</td><td>基本一致</td></tr><tr><td><strong>500 Jobs × 20 Pods</strong></td><td>CREATED阶段性突变</td><td>⚠️ 部分符合</td><td>可能版本差异</td></tr><tr><td><strong>20 Jobs × 500 Pods</strong></td><td>CREATED成为瓶颈</td><td>❌ 出现突变</td><td>瓶颈效应不明显</td></tr><tr><td><strong>1 Job × 10K Pods</strong></td><td>调度速度平稳</td><td>❌ 出现突变</td><td>版本兼容性问题</td></tr></tbody></table><h3 id="1-2-差异原因猜测"><a href="#1-2-差异原因猜测" class="headerlink" title="1.2 差异原因猜测"></a>1.2 差异原因猜测</h3><p>基于测试结果分析，我们猜测差异可能源于：</p><ol><li><strong>调度器版本差异</strong>：本地使用v1.11.0，视频可能使用更新版本</li><li><strong>性能优化差异</strong>：新版本可能包含重要的性能优化</li><li><strong>算法改进差异</strong>：调度算法可能在新版本中有显著改进</li><li><strong>配置默认值差异</strong>：新版本的默认配置可能更适合大规模测试</li></ol><h1 id="🔍如何查看测试所用的Volcano版本"><a href="#🔍如何查看测试所用的Volcano版本" class="headerlink" title="🔍如何查看测试所用的Volcano版本"></a>🔍如何查看测试所用的Volcano版本</h1><h2 id="1-查看部署文件中的版本信息"><a href="#1-查看部署文件中的版本信息" class="headerlink" title="1. 查看部署文件中的版本信息"></a>1. 查看部署文件中的版本信息</h2><h3 id="1-1-直接查看配置文件"><a href="#1-1-直接查看配置文件" class="headerlink" title="1.1 直接查看配置文件"></a>1.1 直接查看配置文件</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看admission webhook manager版本</span></span><br><span class="line">grep <span class="string">"image:"</span> schedulers/volcano/volcano-admission/deployment.yaml | grep volcano</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看scheduler版本</span></span><br><span class="line">grep <span class="string">"image:"</span> schedulers/volcano/volcano-scheduler/deployment.yaml | grep volcano</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看controller manager版本</span></span><br><span class="line">grep <span class="string">"image:"</span> schedulers/volcano/volcano-controller/deployment.yaml | grep volcano</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看初始化job版本</span></span><br><span class="line">grep <span class="string">"image:"</span> schedulers/volcano/volcano-admission-init/job.yaml | grep volcano</span><br></pre></td></tr></table></figure></div><h3 id="1-2-批量查看所有版本"><a href="#1-2-批量查看所有版本" class="headerlink" title="1.2 批量查看所有版本"></a>1.2 批量查看所有版本</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一次性查看所有Volcano组件版本</span></span><br><span class="line">grep -r <span class="string">"image:"</span> schedulers/volcano/ | grep volcano | grep -E <span class="string">"v[0-9]+\.[0-9]+\.[0-9]+"</span></span><br></pre></td></tr></table></figure></div><p><strong>输出示例</strong>：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">schedulers/volcano/volcano-admission/deployment.yaml:        image: kind-registry:5000/docker.io/volcanosh/vc-webhook-manager:v1.11.0</span><br><span class="line">schedulers/volcano/volcano-admission-init/job.yaml:        image: kind-registry:5000/docker.io/volcanosh/vc-webhook-manager:v1.11.0</span><br><span class="line">schedulers/volcano/volcano-scheduler/deployment.yaml:        image: kind-registry:5000/docker.io/volcanosh/vc-scheduler:v1.11.0</span><br><span class="line">schedulers/volcano/volcano-controller/deployment.yaml:        image: kind-registry:5000/docker.io/volcanosh/vc-controller-manager:v1.11.0</span><br></pre></td></tr></table></figure></div><h2 id="2-查看运行中的集群版本"><a href="#2-查看运行中的集群版本" class="headerlink" title="2. 查看运行中的集群版本"></a>2. 查看运行中的集群版本</h2><h3 id="2-1-检查已部署的Pod版本"><a href="#2-1-检查已部署的Pod版本" class="headerlink" title="2.1 检查已部署的Pod版本"></a>2.1 检查已部署的Pod版本</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置kubeconfig</span></span><br><span class="line"><span class="built_in">export</span> KUBECONFIG=./clusters/volcano/kubeconfig.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看运行中的Pod镜像版本</span></span><br><span class="line">kubectl get pods -n volcano-system -o jsonpath=<span class="string">'{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].image}{"\n"}{end}'</span></span><br></pre></td></tr></table></figure></div><h3 id="2-2-查看ConfigMap中的配置"><a href="#2-2-查看ConfigMap中的配置" class="headerlink" title="2.2 查看ConfigMap中的配置"></a>2.2 查看ConfigMap中的配置</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看调度器配置</span></span><br><span class="line">kubectl get configmap -n volcano-system volcano-scheduler-configmap -o yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看admission webhook配置</span></span><br><span class="line">kubectl get validatingwebhookconfigurations -o yaml | grep -A 5 -B 5 volcano</span><br></pre></td></tr></table></figure></div><h2 id="3-版本信息汇总"><a href="#3-版本信息汇总" class="headerlink" title="3. 版本信息汇总"></a>3. 版本信息汇总</h2><p>通过上述方法，我们确认了当前环境使用的Volcano版本：</p><table><thead><tr><th>组件</th><th>当前版本</th><th>镜像名称</th></tr></thead><tbody><tr><td><strong>Webhook Manager</strong></td><td>v1.11.0</td><td><code>volcanosh/vc-webhook-manager:v1.11.0</code></td></tr><tr><td><strong>Scheduler</strong></td><td>v1.11.0</td><td><code>volcanosh/vc-scheduler:v1.11.0</code></td></tr><tr><td><strong>Controller Manager</strong></td><td>v1.11.0</td><td><code>volcanosh/vc-controller-manager:v1.11.0</code></td></tr></tbody></table><h1 id="🚀如何自动化升级Volcano版本"><a href="#🚀如何自动化升级Volcano版本" class="headerlink" title="🚀如何自动化升级Volcano版本"></a>🚀如何自动化升级Volcano版本</h1><h2 id="1-目标版本选择"><a href="#1-目标版本选择" class="headerlink" title="1. 目标版本选择"></a>1. 目标版本选择</h2><p>我们选择升级到<code>v1.12.0-alpha.0</code>，原因主要是KubeCon视频中说明使用了该版本。推测新版本中可能包含性能改进、旧版本中可能存在部分bug被修复，甚至调度算法可能也有所更新。</p><h2 id="2-版本升级脚本"><a href="#2-版本升级脚本" class="headerlink" title="2. 版本升级脚本"></a>2. 版本升级脚本</h2><h3 id="2-1-批量替换版本"><a href="#2-1-批量替换版本" class="headerlink" title="2.1 批量替换版本"></a>2.1 批量替换版本</h3><p>如果不想使用脚本，也可以手动执行：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 备份原文件</span></span><br><span class="line"><span class="built_in">cp</span> -r schedulers/volcano schedulers/volcano.backup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批量替换版本</span></span><br><span class="line">sed -i <span class="string">'s/v1\.11\.0/v1.12.0-alpha.0/g'</span> schedulers/volcano/*/deployment.yaml</span><br><span class="line">sed -i <span class="string">'s/v1\.11\.0/v1.12.0-alpha.0/g'</span> schedulers/volcano/*/job.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证修改结果</span></span><br><span class="line">grep -r <span class="string">"image:"</span> schedulers/volcano/ | grep volcano | grep -E <span class="string">"v[0-9]+\.[0-9]+\.[0-9]+"</span></span><br></pre></td></tr></table></figure></div><h3 id="3-2-逐个文件修改"><a href="#3-2-逐个文件修改" class="headerlink" title="3.2 逐个文件修改"></a>3.2 逐个文件修改</h3><p>也可以逐个文件进行修改：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改admission webhook</span></span><br><span class="line">sed -i <span class="string">'s/v1\.11\.0/v1.12.0-alpha.0/g'</span> schedulers/volcano/volcano-admission/deployment.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改scheduler</span></span><br><span class="line">sed -i <span class="string">'s/v1\.11\.0/v1.12.0-alpha.0/g'</span> schedulers/volcano/volcano-scheduler/deployment.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改controller</span></span><br><span class="line">sed -i <span class="string">'s/v1\.11\.0/v1.12.0-alpha.0/g'</span> schedulers/volcano/volcano-controller/deployment.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改初始化job</span></span><br><span class="line">sed -i <span class="string">'s/v1\.11\.0/v1.12.0-alpha.0/g'</span> schedulers/volcano/volcano-admission-init/job.yaml</span><br></pre></td></tr></table></figure></div><h1 id="✅验证版本升级效果"><a href="#✅验证版本升级效果" class="headerlink" title="✅验证版本升级效果"></a>✅验证版本升级效果</h1><h2 id="1-配置文件验证"><a href="#1-配置文件验证" class="headerlink" title="1. 配置文件验证"></a>1. 配置文件验证</h2><h3 id="1-1-检查版本是否已更新"><a href="#1-1-检查版本是否已更新" class="headerlink" title="1.1 检查版本是否已更新"></a>1.1 检查版本是否已更新</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证所有文件是否已更新</span></span><br><span class="line">grep -r <span class="string">"image:"</span> schedulers/volcano/ | grep volcano | grep -E <span class="string">"v[0-9]+\.[0-9]+\.[0-9]+"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 期望输出：所有文件都显示 v1.12.0-alpha.0</span></span><br></pre></td></tr></table></figure></div><h3 id="1-2-检查文件完整性"><a href="#1-2-检查文件完整性" class="headerlink" title="1.2 检查文件完整性"></a>1.2 检查文件完整性</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查是否有遗漏的文件</span></span><br><span class="line">find schedulers/volcano/ -name <span class="string">"*.yaml"</span> -<span class="built_in">exec</span> grep -l <span class="string">"v1\.11\.0"</span> {} \;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果没有输出，说明所有文件都已更新</span></span><br></pre></td></tr></table></figure></div><h2 id="2-部署验证"><a href="#2-部署验证" class="headerlink" title="2. 部署验证"></a>2. 部署验证</h2><h3 id="2-1-重新部署Volcano"><a href="#2-1-重新部署Volcano" class="headerlink" title="2.1 重新部署Volcano"></a>2.1 重新部署Volcano</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 清理旧环境</span></span><br><span class="line">make down-volcano</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新部署</span></span><br><span class="line">make up-volcano</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待服务就绪</span></span><br><span class="line">make wait-volcano</span><br></pre></td></tr></table></figure></div><h3 id="2-2-验证Pod镜像版本"><a href="#2-2-验证Pod镜像版本" class="headerlink" title="2.2 验证Pod镜像版本"></a>2.2 验证Pod镜像版本</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置kubeconfig</span></span><br><span class="line"><span class="built_in">export</span> KUBECONFIG=./clusters/volcano/kubeconfig.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查Pod是否使用新版本镜像</span></span><br><span class="line">kubectl get pods -n volcano-system -o jsonpath=<span class="string">'{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].image}{"\n"}{end}'</span> | grep volcano</span><br></pre></td></tr></table></figure></div><p><strong>期望输出</strong>：所有Pod都应该显示<code>v1.12.0-alpha.0</code>版本</p><h3 id="2-3-检查服务状态"><a href="#2-3-检查服务状态" class="headerlink" title="2.3 检查服务状态"></a>2.3 检查服务状态</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查所有Pod状态</span></span><br><span class="line">kubectl get pods -n volcano-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查服务日志</span></span><br><span class="line">kubectl logs -n volcano-system deployment/volcano-scheduler --since=1m</span><br><span class="line">kubectl logs -n volcano-system deployment/volcano-controller-manager --since=1m</span><br></pre></td></tr></table></figure></div><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[1] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://github.com/volcano-sh/volcano/releases">[2] Volcano GitHub - Releases<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a href="!--swig%EF%BF%BC28--">[3] <a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></a></p><p><a href="!--swig%EF%BF%BC30--">[4] <a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复">云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</a></a></p><p><a href="!--swig%EF%BF%BC32--">[5] <a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></a> </p>]]></content>
    
    
    <summary type="html">本文回顾了本地测试与视频测试结果差异的问题，发现可能的原因在于Volcano调度器版本不同。文章详细介绍了如何查看测试所用的Volcano版本，如何将测试版本更换到1.12.0-alpha.0版本，以及如何验证版本升级效果。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="性能优化" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
    <category term="版本升级" scheme="https://freshwlnd.github.io/tags/%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7/"/>
    
    <category term="自动化脚本" scheme="https://freshwlnd.github.io/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%84%9A%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</title>
    <link href="https://freshwlnd.github.io/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/"/>
    <id>https://freshwlnd.github.io/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/</id>
    <published>2025-08-18T15:19:30.000Z</published>
    <updated>2025-08-24T07:21:47.908Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li><li><a href="/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/" title="监控与测试环境解析：测试流程拆解篇">监控与测试环境解析：测试流程拆解篇</a></li><li><a href="/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/" title="监控与测试环境解析：指标采集与可视化篇">监控与测试环境解析：指标采集与可视化篇</a></li><li><a href="/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/" title="监控与测试环境解析：自定义镜像性能回归测试">监控与测试环境解析：自定义镜像性能回归测试</a></li><li><a href="/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/" title="监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题">监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题</a></li><li><a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试">云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试</a></li><li><a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复">云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</a></li><li><a href="/2025/08/20/k8s/k8s-scheduler-performance-volcano-version/" title="云原生批调度实战：Volcano版本修改与性能测试优化">云原生批调度实战：Volcano版本修改与性能测试优化</a></li><li><a href="/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/" title="云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析">云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析</a></li><li><a href="/2025/08/24/k8s/k8s-scheduler-performance-volcano-hypothesis-verification/" title="云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结">云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结</a></li></ol></blockquote><h1 id="💡简介"><a href="#💡简介" class="headerlink" title="💡简介"></a>💡简介</h1><p>在<a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="上一篇博客">上一篇博客</a>中，我们介绍了如何禁用Volcano调度器的enqueue功能。然而，在实际测试过程中，我们发现了一个更严重的问题：<strong>Pod创建数量始终少于10000，仅达到1000左右</strong>，这严重影响了测试结果的准确性。</p><p>本文详细记录了问题排查的完整过程，从发现异常现象到分析4.9GB的审计日志，最终定位到Webhook超时是导致Pod创建失败的根本原因。通过修改Webhook超时时间，我们成功解决了这个问题。</p><h1 id="🚨问题现象描述"><a href="#🚨问题现象描述" class="headerlink" title="🚨问题现象描述"></a>🚨问题现象描述</h1><h2 id="测试环境配置"><a href="#测试环境配置" class="headerlink" title="测试环境配置"></a>测试环境配置</h2><p>我们使用以下参数进行测试：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">make serial-test \</span><br><span class="line">    RESULT_RECENT_DURATION_SECONDS=60 TEST_TIMEOUT_SECONDS=160 \</span><br><span class="line">    NODES_SIZE=1000 \</span><br><span class="line">    QUEUES_SIZE=1  JOBS_SIZE_PER_QUEUE=20     PODS_SIZE_PER_JOB=500</span><br><span class="line"></span><br><span class="line">make serial-test \</span><br><span class="line">    RESULT_RECENT_DURATION_SECONDS=90 TEST_TIMEOUT_SECONDS=190 \</span><br><span class="line">    NODES_SIZE=1000 \</span><br><span class="line">    QUEUES_SIZE=1 JOBS_SIZE_PER_QUEUE=1 PODS_SIZE_PER_JOB=10000</span><br></pre></td></tr></table></figure></div><p><strong>预期结果</strong>：应该创建10,000个Pod<br><strong>实际结果</strong>：仅创建了不到1,000个Pod，成功率不到10%</p><h1 id="🔍问题排查过程"><a href="#🔍问题排查过程" class="headerlink" title="🔍问题排查过程"></a>🔍问题排查过程</h1><h2 id="1-初步分析"><a href="#1-初步分析" class="headerlink" title="1. 初步分析"></a>1. 初步分析</h2><p>首先，我们检查了测试环境的基本状态：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查Pod数量</span></span><br><span class="line">kubectl get pods -A | grep volcano-job | <span class="built_in">wc</span> -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查Job状态</span></span><br><span class="line">kubectl get vcjob -A</span><br></pre></td></tr></table></figure></div><p>发现确实只有少量Pod被创建，大部分Pod创建请求似乎失败了。</p><h2 id="2-日志文件分析"><a href="#2-日志文件分析" class="headerlink" title="2. 日志文件分析"></a>2. 日志文件分析</h2><h3 id="2-1-日志文件大小"><a href="#2-1-日志文件大小" class="headerlink" title="2.1 日志文件大小"></a>2.1 日志文件大小</h3><p>测试完成后，我们发现审计日志文件异常巨大：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> -lh results/1755448387/logs/kube-apiserver-audit.volcano.log</span><br><span class="line"><span class="comment"># 输出：-rw-rw-rw- 1 root root 4.9G  8月 18 00:22 kube-apiserver-audit.volcano.log</span></span><br></pre></td></tr></table></figure></div><p><strong>4.9GB的日志文件</strong>表明系统产生了大量的审计记录，远大于其它测试的审计记录，猜测这意味着存在大量失败的操作。</p><h3 id="2-2-高效日志分析方法"><a href="#2-2-高效日志分析方法" class="headerlink" title="2.2 高效日志分析方法"></a>2.2 高效日志分析方法</h3><p>由于日志文件过大，我们采用了高效的分析方法：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统计错误数量</span></span><br><span class="line">grep -c <span class="string">"context deadline exceeded"</span> kube-apiserver-audit.volcano.log</span><br><span class="line"><span class="comment"># 输出：520120</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计Webhook调用数量</span></span><br><span class="line">grep -c <span class="string">"validatepod.volcano.sh"</span> kube-apiserver-audit.volcano.log</span><br><span class="line"><span class="comment"># 输出：515531</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计Pod创建成功/失败数量</span></span><br><span class="line">grep -c <span class="string">"ResponseComplete.*pods.*create.*Success"</span> kube-apiserver-audit.volcano.log</span><br><span class="line"><span class="comment"># 输出：712</span></span><br><span class="line">grep -c <span class="string">"ResponseComplete.*pods.*create.*Failure"</span> kube-apiserver-audit.volcano.log</span><br><span class="line"><span class="comment"># 输出：520518</span></span><br></pre></td></tr></table></figure></div><h2 id="3-根本原因定位"><a href="#3-根本原因定位" class="headerlink" title="3. 根本原因定位"></a>3. 根本原因定位</h2><h3 id="3-1-错误模式分析"><a href="#3-1-错误模式分析" class="headerlink" title="3.1 错误模式分析"></a>3.1 错误模式分析</h3><p>通过分析日志中的错误信息，我们发现了统一的错误模式：</p><div class="code-container" data-rel="Json"><figure class="iseeu highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">  <span class="attr">"status"</span><span class="punctuation">:</span> <span class="string">"Failure"</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">"message"</span><span class="punctuation">:</span> <span class="string">"Internal error occurred: failed calling webhook \"validatepod.volcano.sh\": failed to call webhook: Post \"https://volcano-admission-service.volcano-system.svc:443/pods/validate?timeout=10s\": context deadline exceeded"</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">"reason"</span><span class="punctuation">:</span> <span class="string">"InternalError"</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">"code"</span><span class="punctuation">:</span> <span class="number">500</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure></div><p><strong>关键信息</strong>：</p><ul><li><strong>错误类型</strong>：Webhook调用超时</li><li><strong>超时时间</strong>：10秒</li><li><strong>影响范围</strong>：98.7%的Pod创建请求失败</li></ul><h3 id="3-2-统计数据汇总"><a href="#3-2-统计数据汇总" class="headerlink" title="3.2 统计数据汇总"></a>3.2 统计数据汇总</h3><table><thead><tr><th>指标</th><th>数量</th><th>占比</th></tr></thead><tbody><tr><td><strong>总Pod创建请求</strong></td><td>526,767</td><td>100%</td></tr><tr><td><strong>成功创建</strong></td><td>712</td><td>0.13%</td></tr><tr><td><strong>失败创建</strong></td><td>520,518</td><td>98.7%</td></tr><tr><td><strong>Webhook超时错误</strong></td><td>520,120</td><td>98.7%</td></tr></tbody></table><h2 id="4-问题根因分析"><a href="#4-问题根因分析" class="headerlink" title="4. 问题根因分析"></a>4. 问题根因分析</h2><h3 id="4-1-Webhook超时机制"><a href="#4-1-Webhook超时机制" class="headerlink" title="4.1 Webhook超时机制"></a>4.1 Webhook超时机制</h3><p>Volcano使用admission webhook来验证和修改Pod创建请求：</p><ol><li><p><strong>Pod创建流程</strong>：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create pod → API Server → Admission Webhook → Volcano Controller → Pod创建</span><br></pre></td></tr></table></figure></div></li><li><p><strong>超时配置</strong>：</p><ul><li>当前配置：<code>timeoutSeconds: 10</code></li><li>问题：10秒内无法处理大量并发Pod创建请求</li></ul></li></ol><h3 id="4-2-性能瓶颈分析"><a href="#4-2-性能瓶颈分析" class="headerlink" title="4.2 性能瓶颈分析"></a>4.2 性能瓶颈分析</h3><p>当设置<code>PODS_SIZE_PER_JOB=10000</code>时：</p><ol><li><strong>并发压力</strong>：系统需要同时处理10,000个Pod创建请求</li><li><strong>Webhook负载</strong>：每个请求都需要经过admission webhook验证</li><li><strong>超时触发</strong>：大量请求堆积导致处理时间超过10秒</li><li><strong>失败连锁</strong>：超时失败导致Pod创建失败，影响整体测试结果</li></ol><h1 id="🔧问题修复方案"><a href="#🔧问题修复方案" class="headerlink" title="🔧问题修复方案"></a>🔧问题修复方案</h1><h2 id="1-修改Webhook超时时间"><a href="#1-修改Webhook超时时间" class="headerlink" title="1. 修改Webhook超时时间"></a>1. 修改Webhook超时时间</h2><h3 id="1-1-手动修改方法"><a href="#1-1-手动修改方法" class="headerlink" title="1.1 手动修改方法"></a>1.1 手动修改方法</h3><p>我们需要将所有webhook配置文件的超时时间从10秒增加到30秒：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找所有webhook配置文件</span></span><br><span class="line">find schedulers/volcano/ -name <span class="string">"*webhook*.yaml"</span> -<span class="built_in">exec</span> grep -l <span class="string">"timeoutSeconds: 10"</span> {} \;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批量修改超时时间</span></span><br><span class="line">sed -i <span class="string">'s/timeoutSeconds: 10/timeoutSeconds: 30/g'</span> schedulers/volcano/admission-service-*.yaml</span><br></pre></td></tr></table></figure></div><p><strong>重要说明</strong>：Kubernetes对webhook超时时间有严格限制，必须在1到30秒之间。我们最初尝试设置为60秒，但在部署时遇到了验证错误。</p><h2 id="1-3-Kubernetes超时时间限制"><a href="#1-3-Kubernetes超时时间限制" class="headerlink" title="1.3 Kubernetes超时时间限制"></a>1.3 Kubernetes超时时间限制</h2><p>在修复过程中，我们发现了一个重要的Kubernetes限制：</p><p>当我们尝试将webhook超时时间设置为60秒时，在部署过程中遇到了以下错误：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Error from server (Invalid): error when creating <span class="string">"../../schedulers/volcano"</span>: </span><br><span class="line">MutatingWebhookConfiguration.admissionregistration.k8s.io <span class="string">"volcano-admission-service-jobs-mutate"</span> </span><br><span class="line">is invalid: webhooks[0].timeoutSeconds: Invalid value: 60: </span><br><span class="line">the <span class="built_in">timeout</span> value must be between 1 and 30 seconds</span><br></pre></td></tr></table></figure></div><p>这个错误表明：</p><ul><li><strong>Kubernetes限制</strong>：webhook超时时间必须在1到30秒之间</li><li><strong>我们之前的设置</strong>：60秒超出了允许范围</li><li><strong>影响范围</strong>：所有7个webhook配置文件都无法部署</li></ul><h3 id="1-2-修改的文件列表"><a href="#1-2-修改的文件列表" class="headerlink" title="1.2 修改的文件列表"></a>1.2 修改的文件列表</h3><p>需要修改的7个webhook配置文件：</p><ol><li><code>admission-service-jobs-validate_validatingwebhookconfiguration.yaml</code></li><li><code>admission-service-queues-mutate_mutatingwebhookconfiguration.yaml</code></li><li><code>admission-service-jobs-mutate_mutatingwebhookconfiguration.yaml</code></li><li><code>admission-service-podgroups-mutate_mutatingwebhookconfiguration.yaml</code></li><li><code>admission-service-pods-mutate_mutatingwebhookconfiguration.yaml</code></li><li><code>admission-service-queues-validate_validatingwebhookconfiguration.yaml</code></li><li><code>admission-service-pods-validate_validatingwebhookconfiguration.yaml</code></li></ol><h2 id="2-修复实现"><a href="#2-修复实现" class="headerlink" title="2. 修复实现"></a>2. 修复实现</h2><p>为了修改配置，可以按照以下步骤进行：</p><h3 id="步骤1：备份原配置文件"><a href="#步骤1：备份原配置文件" class="headerlink" title="步骤1：备份原配置文件"></a>步骤1：备份原配置文件</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建备份目录</span></span><br><span class="line"><span class="built_in">mkdir</span> -p schedulers/volcano/backup-$(<span class="built_in">date</span> +%Y%m%d-%H%M%S)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 备份所有webhook配置文件</span></span><br><span class="line"><span class="built_in">cp</span> schedulers/volcano/admission-service-*.yaml schedulers/volcano/backup-$(<span class="built_in">date</span> +%Y%m%d-%H%M%S)/</span><br></pre></td></tr></table></figure></div><h3 id="步骤2：查找需要修改的文件"><a href="#步骤2：查找需要修改的文件" class="headerlink" title="步骤2：查找需要修改的文件"></a>步骤2：查找需要修改的文件</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找所有包含timeoutSeconds的文件</span></span><br><span class="line">find schedulers/volcano/ -name <span class="string">"*webhook*.yaml"</span> -<span class="built_in">exec</span> grep -l <span class="string">"timeoutSeconds:"</span> {} \;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前超时设置</span></span><br><span class="line">grep -r <span class="string">"timeoutSeconds:"</span> schedulers/volcano/ | grep -E <span class="string">"[0-9]+"</span></span><br></pre></td></tr></table></figure></div><h3 id="步骤3：逐个修改文件"><a href="#步骤3：逐个修改文件" class="headerlink" title="步骤3：逐个修改文件"></a>步骤3：逐个修改文件</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改validating webhook配置文件</span></span><br><span class="line">sed -i <span class="string">'s/timeoutSeconds: 10/timeoutSeconds: 30/g'</span> schedulers/volcano/admission-service-jobs-validate_validatingwebhookconfiguration.yaml</span><br><span class="line">sed -i <span class="string">'s/timeoutSeconds: 10/timeoutSeconds: 30/g'</span> schedulers/volcano/admission-service-pods-validate_validatingwebhookconfiguration.yaml</span><br><span class="line">sed -i <span class="string">'s/timeoutSeconds: 10/timeoutSeconds: 30/g'</span> schedulers/volcano/admission-service-queues-validate_validatingwebhookconfiguration.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改mutating webhook配置文件</span></span><br><span class="line">sed -i <span class="string">'s/timeoutSeconds: 10/timeoutSeconds: 30/g'</span> schedulers/volcano/admission-service-jobs-mutate_mutatingwebhookconfiguration.yaml</span><br><span class="line">sed -i <span class="string">'s/timeoutSeconds: 10/timeoutSeconds: 30/g'</span> schedulers/volcano/admission-service-podgroups-mutate_mutatingwebhookconfiguration.yaml</span><br><span class="line">sed -i <span class="string">'s/timeoutSeconds: 10/timeoutSeconds: 30/g'</span> schedulers/volcano/admission-service-pods-mutate_mutatingwebhookconfiguration.yaml</span><br><span class="line">sed -i <span class="string">'s/timeoutSeconds: 10/timeoutSeconds: 30/g'</span> schedulers/volcano/admission-service-queues-mutate_mutatingwebhookconfiguration.yaml</span><br></pre></td></tr></table></figure></div><h3 id="步骤4：批量修改（推荐）"><a href="#步骤4：批量修改（推荐）" class="headerlink" title="步骤4：批量修改（推荐）"></a>步骤4：批量修改（推荐）</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一次性修改所有文件</span></span><br><span class="line">sed -i <span class="string">'s/timeoutSeconds: 10/timeoutSeconds: 30/g'</span> schedulers/volcano/admission-service-*.yaml</span><br></pre></td></tr></table></figure></div><h2 id="3-验证修复效果"><a href="#3-验证修复效果" class="headerlink" title="3. 验证修复效果"></a>3. 验证修复效果</h2><h3 id="3-1-检查配置修改"><a href="#3-1-检查配置修改" class="headerlink" title="3.1 检查配置修改"></a>3.1 检查配置修改</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查是否所有文件都已修改</span></span><br><span class="line">grep -r <span class="string">"timeoutSeconds:"</span> schedulers/volcano/ | grep -E <span class="string">"[0-9]+"</span></span><br><span class="line"><span class="comment"># 期望输出：所有文件都显示 timeoutSeconds: 30</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 确认没有遗漏的文件</span></span><br><span class="line">find schedulers/volcano/ -name <span class="string">"*webhook*.yaml"</span> -<span class="built_in">exec</span> grep -l <span class="string">"timeoutSeconds: 10"</span> {} \;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果上述命令有输出，说明还有文件未修改</span></span><br></pre></td></tr></table></figure></div><h3 id="3-2-重新部署测试"><a href="#3-2-重新部署测试" class="headerlink" title="3.2 重新部署测试"></a>3.2 重新部署测试</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新部署Volcano</span></span><br><span class="line">make up-volcano</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新运行测试</span></span><br><span class="line">make serial-test \</span><br><span class="line">    RESULT_RECENT_DURATION_SECONDS=90 TEST_TIMEOUT_SECONDS=190 \</span><br><span class="line">    NODES_SIZE=1000 \</span><br><span class="line">    QUEUES_SIZE=1 JOBS_SIZE_PER_QUEUE=1 PODS_SIZE_PER_JOB=10000</span><br></pre></td></tr></table></figure></div><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[1] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://volcano.sh/zh/docs/architecture/">[2] Volcano 文档 - 架构<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/admission-controllers/">[3] Kubernetes 文档 - Admission Controllers 准入控制 <i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a href="!--swig%EF%BF%BC29--">[4] <a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试">云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试</a></a></p><p><a href="!--swig%EF%BF%BC31--">[5] <a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></a> </p>]]></content>
    
    
    <summary type="html">本文详细记录了在测试Volcano调度器时发现Pod创建数量始终少于10000，仅达到1000的问题排查过程。通过分析4.9GB的审计日志，发现大量Pod创建请求因Webhook超时而失败。文章介绍了如何修改Webhook超时时间。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="Webhook" scheme="https://freshwlnd.github.io/tags/Webhook/"/>
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
    <category term="问题排查" scheme="https://freshwlnd.github.io/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"/>
    
    <category term="超时修复" scheme="https://freshwlnd.github.io/tags/%E8%B6%85%E6%97%B6%E4%BF%AE%E5%A4%8D/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试</title>
    <link href="https://freshwlnd.github.io/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/"/>
    <id>https://freshwlnd.github.io/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/</id>
    <published>2025-08-17T15:06:07.000Z</published>
    <updated>2025-08-24T07:22:55.991Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li><li><a href="/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/" title="监控与测试环境解析：测试流程拆解篇">监控与测试环境解析：测试流程拆解篇</a></li><li><a href="/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/" title="监控与测试环境解析：指标采集与可视化篇">监控与测试环境解析：指标采集与可视化篇</a></li><li><a href="/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/" title="监控与测试环境解析：自定义镜像性能回归测试">监控与测试环境解析：自定义镜像性能回归测试</a></li><li><a href="/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/" title="监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题">监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题</a></li><li><a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试">云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试</a></li><li><a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复">云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</a></li><li><a href="/2025/08/20/k8s/k8s-scheduler-performance-volcano-version/" title="云原生批调度实战：Volcano版本修改与性能测试优化">云原生批调度实战：Volcano版本修改与性能测试优化</a></li><li><a href="/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/" title="云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析">云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析</a></li><li><a href="/2025/08/24/k8s/k8s-scheduler-performance-volcano-hypothesis-verification/" title="云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结">云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结</a></li></ol></blockquote><h1 id="💡简介"><a href="#💡简介" class="headerlink" title="💡简介"></a>💡简介</h1><p>在<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="上一篇博客">上一篇博客</a>中，我们分析了本地环境下的调度器性能测试结果，发现了一些有趣的现象：在某些测试场景下，CREATED事件会出现阶段性突变，而SCHEDULED事件则相对平稳。这种现象可能与调度器的enqueue机制有关。</p><p>本文详细介绍了如何禁用Volcano调度器的enqueue功能，通过对比分析来验证我们的猜测：<strong>enqueue可能会提前判断资源是否充足，从而在资源不足时限制Pod的创建，进而影响调度性能</strong>。</p><p>通过禁用enqueue功能，我们可以观察调度器在资源分配阶段的纯粹性能表现，为调度器性能优化提供重要参考。</p><h1 id="🧠Volcano调度器enqueue功能简介"><a href="#🧠Volcano调度器enqueue功能简介" class="headerlink" title="🧠Volcano调度器enqueue功能简介"></a>🧠Volcano调度器enqueue功能简介</h1><h2 id="什么是enqueue？"><a href="#什么是enqueue？" class="headerlink" title="什么是enqueue？"></a>什么是enqueue？</h2><p><strong>enqueue</strong>是Volcano调度器调度流程中的一个重要阶段，主要负责：</p><ol><li><strong>作业入队管理</strong>：将提交的Job添加到调度队列中</li><li><strong>优先级排序</strong>：根据Job的优先级、提交时间等因素进行排序</li><li><strong>资源预检查</strong>：提前判断集群资源是否满足Job需求</li><li><strong>队列容量控制</strong>：管理队列的容量限制和准入控制</li></ol><h2 id="enqueue对调度性能的影响"><a href="#enqueue对调度性能的影响" class="headerlink" title="enqueue对调度性能的影响"></a>enqueue对调度性能的影响</h2><p>基于<a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="前期测试结果">前期测试结果</a>的分析，我们猜测enqueue可能会：</p><ul><li><strong>提前资源判断</strong>：在Pod创建前就判断资源是否充足</li><li><strong>限制Pod创建</strong>：当资源不足时，限制新Pod的创建速度</li><li><strong>影响CREATED事件</strong>：导致CREATED事件出现阶段性突变</li><li><strong>调度性能瓶颈</strong>：在某些场景下成为整体性能的瓶颈</li></ul><h2 id="Volcano调度器的完整调度流程"><a href="#Volcano调度器的完整调度流程" class="headerlink" title="Volcano调度器的完整调度流程"></a>Volcano调度器的完整调度流程</h2><pre class="mermaid">graph LR    A[Job提交] --&gt; B[enqueue]    B --&gt; C[allocate]    C --&gt; D[backfill]    D --&gt; E[reclaim]    E --&gt; F[preempt]        B1[enqueue阶段] --&gt; B2[队列管理]    B1 --&gt; B3[优先级排序]    B1 --&gt; B4[资源预检查]        C1[allocate阶段] --&gt; C2[资源分配]    C1 --&gt; C3[节点选择]    C1 --&gt; C4[Pod绑定]</pre><h1 id="🔧如何禁用Volcano的enqueue功能"><a href="#🔧如何禁用Volcano的enqueue功能" class="headerlink" title="🔧如何禁用Volcano的enqueue功能"></a>🔧如何禁用Volcano的enqueue功能</h1><h2 id="1-修改调度器配置文件"><a href="#1-修改调度器配置文件" class="headerlink" title="1. 修改调度器配置文件"></a>1. 修改调度器配置文件</h2><h3 id="1-1-修改主配置文件"><a href="#1-1-修改主配置文件" class="headerlink" title="1.1 修改主配置文件"></a>1.1 修改主配置文件</h3><p>编辑 <code>schedulers/volcano/scheduler.conf</code> 文件：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原始配置</span></span><br><span class="line">actions: <span class="string">"enqueue, allocate, backfill"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改为（移除enqueue）</span></span><br><span class="line">actions: <span class="string">"allocate, backfill"</span></span><br></pre></td></tr></table></figure></div><h3 id="1-2-修改测试配置文件"><a href="#1-2-修改测试配置文件" class="headerlink" title="1.2 修改测试配置文件"></a>1.2 修改测试配置文件</h3><p>编辑 <code>test/volcano/init.yaml</code> 文件：</p><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原始配置</span></span><br><span class="line"><span class="attr">actions:</span> <span class="string">"enqueue, allocate,#<span class="template-variable">{{ if .preemption }}</span> preempt,#<span class="template-variable">{{ end }}</span> backfill, reclaim"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改为（移除enqueue）</span></span><br><span class="line"><span class="attr">actions:</span> <span class="string">"allocate,#<span class="template-variable">{{ if .preemption }}</span> preempt,#<span class="template-variable">{{ end }}</span> backfill, reclaim"</span></span><br></pre></td></tr></table></figure></div><h2 id="2-重新部署Volcano调度器"><a href="#2-重新部署Volcano调度器" class="headerlink" title="2. 重新部署Volcano调度器"></a>2. 重新部署Volcano调度器</h2><p>修改配置后，需要重新部署调度器：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新构建和部署</span></span><br><span class="line">make up-volcano</span><br></pre></td></tr></table></figure></div><h1 id="🚀如何手动配置测试环境"><a href="#🚀如何手动配置测试环境" class="headerlink" title="🚀如何手动配置测试环境"></a>🚀如何手动配置测试环境</h1><h2 id="1-启动Volcano测试环境"><a href="#1-启动Volcano测试环境" class="headerlink" title="1. 启动Volcano测试环境"></a>1. 启动Volcano测试环境</h2><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动完整的Volcano测试环境</span></span><br><span class="line">make prepare-volcano</span><br></pre></td></tr></table></figure></div><p>这个命令会依次执行：</p><ul><li><code>make up-volcano</code>：创建Kind集群并部署Volcano</li><li><code>make wait-volcano</code>：等待所有服务就绪</li><li><code>make test-init-volcano</code>：初始化测试环境，创建虚拟节点</li></ul><h2 id="2-验证环境状态"><a href="#2-验证环境状态" class="headerlink" title="2. 验证环境状态"></a>2. 验证环境状态</h2><h3 id="2-1-检查集群状态"><a href="#2-1-检查集群状态" class="headerlink" title="2.1 检查集群状态"></a>2.1 检查集群状态</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl get nodes -o wide</span><br></pre></td></tr></table></figure></div><p>应该看到：</p><ul><li><code>volcano-control-plane</code>：控制平面节点</li><li><code>node-0</code>、<code>node-1</code>等：虚拟KWOK节点</li></ul><h3 id="2-2-检查Volcano服务状态"><a href="#2-2-检查Volcano服务状态" class="headerlink" title="2.2 检查Volcano服务状态"></a>2.2 检查Volcano服务状态</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl get pods -n volcano-system</span><br></pre></td></tr></table></figure></div><p>确保所有Pod都处于Running状态。</p><h3 id="2-3-检查虚拟节点标签"><a href="#2-3-检查虚拟节点标签" class="headerlink" title="2.3 检查虚拟节点标签"></a>2.3 检查虚拟节点标签</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl get nodes --show-labels | grep node-</span><br></pre></td></tr></table></figure></div><p>确保虚拟节点有正确的标签：<code>type=kwok</code></p><h1 id="🔍如何验证enqueue是否被成功禁用"><a href="#🔍如何验证enqueue是否被成功禁用" class="headerlink" title="🔍如何验证enqueue是否被成功禁用"></a>🔍如何验证enqueue是否被成功禁用</h1><h2 id="1-检查ConfigMap配置"><a href="#1-检查ConfigMap配置" class="headerlink" title="1. 检查ConfigMap配置"></a>1. 检查ConfigMap配置</h2><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl get configmap -n volcano-system volcano-scheduler-configmap -o yaml</span><br></pre></td></tr></table></figure></div><p><strong>期望结果</strong>：</p><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">volcano-scheduler.conf:</span> <span class="string">"actions: \"allocate, backfill, reclaim\"\n..."</span></span><br></pre></td></tr></table></figure></div><p><strong>关键点</strong>：配置中应该没有<code>enqueue</code>，只有<code>allocate, backfill, reclaim</code>。</p><h2 id="2-检查调度器启动日志"><a href="#2-检查调度器启动日志" class="headerlink" title="2. 检查调度器启动日志"></a>2. 检查调度器启动日志</h2><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl logs -n volcano-system deployment/volcano-scheduler --since=1h | grep <span class="string">"Successfully loaded"</span></span><br></pre></td></tr></table></figure></div><p><strong>期望结果</strong>：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Successfully loaded Scheduler conf, actions: [allocate backfill reclaim]</span><br></pre></td></tr></table></figure></div><p><strong>关键点</strong>：日志中应该显示<code>actions: [allocate backfill reclaim]</code>，没有enqueue。</p><h2 id="3-检查实时调度日志"><a href="#3-检查实时调度日志" class="headerlink" title="3. 检查实时调度日志"></a>3. 检查实时调度日志</h2><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl logs -n volcano-system deployment/volcano-scheduler -f | grep -E <span class="string">"(enqueue|allocate|backfill)"</span></span><br></pre></td></tr></table></figure></div><p><strong>期望结果</strong>：应该只看到<code>allocate</code>和<code>backfill</code>相关的日志，没有<code>enqueue</code>相关的日志。</p><h1 id="🧪如何执行性能测试"><a href="#🧪如何执行性能测试" class="headerlink" title="🧪如何执行性能测试"></a>🧪如何执行性能测试</h1><h2 id="1-运行批处理作业测试"><a href="#1-运行批处理作业测试" class="headerlink" title="1. 运行批处理作业测试"></a>1. 运行批处理作业测试</h2><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用小规模参数进行测试</span></span><br><span class="line">make start-volcano QUEUES_SIZE=1 JOBS_SIZE_PER_QUEUE=20 PODS_SIZE_PER_JOB=500</span><br></pre></td></tr></table></figure></div><p><strong>参数说明</strong>：</p><ul><li><code>QUEUES_SIZE=1</code>：创建1个队列</li><li><code>JOBS_SIZE_PER_QUEUE=20</code>：每个队列20个Job</li><li><code>PODS_SIZE_PER_JOB=500</code>：每个Job包含500个Pod</li><li><strong>总计</strong>：20个Job × 500个Pod = 10,000个Pod</li></ul><h2 id="2-测试执行过程"><a href="#2-测试执行过程" class="headerlink" title="2. 测试执行过程"></a>2. 测试执行过程</h2><p>测试程序会：</p><ol><li><strong>创建队列</strong>：<code>test-queue-long-term-research-0</code></li><li><strong>创建Job</strong>：20个Volcano Job</li><li><strong>创建Pod</strong>：每个Job创建500个Pod</li><li><strong>执行调度</strong>：Volcano调度器分配资源</li><li><strong>收集结果</strong>：记录CREATED和SCHEDULED事件</li></ol><h2 id="3-监控测试进度"><a href="#3-监控测试进度" class="headerlink" title="3. 监控测试进度"></a>3. 监控测试进度</h2><h3 id="3-1-查看Job状态"><a href="#3-1-查看Job状态" class="headerlink" title="3.1 查看Job状态"></a>3.1 查看Job状态</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看Volcano Job（注意：不是标准Kubernetes Job）</span></span><br><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl get vcjob -A</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者使用完整API</span></span><br><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl get jobs.batch.volcano.sh -A</span><br></pre></td></tr></table></figure></div><h3 id="3-2-查看Pod状态"><a href="#3-2-查看Pod状态" class="headerlink" title="3.2 查看Pod状态"></a>3.2 查看Pod状态</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看所有Pod</span></span><br><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl get pods -A | grep volcano-job</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看特定Job的Pod</span></span><br><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl get pods -l job-name=volcano-job-long-term-research-0-1</span><br></pre></td></tr></table></figure></div><h3 id="3-3-查看调度器日志"><a href="#3-3-查看调度器日志" class="headerlink" title="3.3 查看调度器日志"></a>3.3 查看调度器日志</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实时监控调度过程</span></span><br><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl logs -n volcano-system deployment/volcano-scheduler -f | grep -E <span class="string">"(Binding|allocate|task)"</span></span><br></pre></td></tr></table></figure></div><h1 id="📊如何验证测试是否正常执行"><a href="#📊如何验证测试是否正常执行" class="headerlink" title="📊如何验证测试是否正常执行"></a>📊如何验证测试是否正常执行</h1><h2 id="1-检查测试结果"><a href="#1-检查测试结果" class="headerlink" title="1. 检查测试结果"></a>1. 检查测试结果</h2><h3 id="1-1-查看测试程序输出"><a href="#1-1-查看测试程序输出" class="headerlink" title="1.1 查看测试程序输出"></a>1.1 查看测试程序输出</h3><p>测试完成后，应该看到类似输出：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">=== RUN   TestBatchJob</span><br><span class="line">--- PASS: TestBatchJob (40.05s)</span><br><span class="line">PASS</span><br></pre></td></tr></table></figure></div><h3 id="1-2-检查Job和Pod状态"><a href="#1-2-检查Job和Pod状态" class="headerlink" title="1.2 检查Job和Pod状态"></a>1.2 检查Job和Pod状态</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查Job状态</span></span><br><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl get vcjob -o wide</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查Pod状态分布</span></span><br><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl get pods -A | grep volcano-job | awk <span class="string">'{print $3}'</span> | <span class="built_in">sort</span> | <span class="built_in">uniq</span> -c</span><br></pre></td></tr></table></figure></div><p><strong>期望结果</strong>：</p><ul><li>大部分Pod应该处于<code>Completed</code>状态</li><li>少量Pod可能处于<code>Running</code>或<code>Pending</code>状态</li><li>没有Pod处于<code>Failed</code>状态</li></ul><h2 id="2-验证调度行为"><a href="#2-验证调度行为" class="headerlink" title="2. 验证调度行为"></a>2. 验证调度行为</h2><h3 id="2-1-检查Pod调度位置"><a href="#2-1-检查Pod调度位置" class="headerlink" title="2.1 检查Pod调度位置"></a>2.1 检查Pod调度位置</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看Pod被调度到哪些节点</span></span><br><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl get pods -A -o wide | grep volcano-job | awk <span class="string">'{print $1, $2, $8}'</span></span><br></pre></td></tr></table></figure></div><p><strong>期望结果</strong>：Pod应该被调度到虚拟节点（如<code>node-0</code>），而不是控制平面节点。</p><h3 id="2-2-检查资源分配"><a href="#2-2-检查资源分配" class="headerlink" title="2.2 检查资源分配"></a>2.2 检查资源分配</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看节点资源使用情况</span></span><br><span class="line">KUBECONFIG=./clusters/volcano/kubeconfig.yaml kubectl describe node node-0</span><br></pre></td></tr></table></figure></div><h2 id="3-分析调度性能"><a href="#3-分析调度性能" class="headerlink" title="3. 分析调度性能"></a>3. 分析调度性能</h2><h3 id="3-1-对比enqueue启用-禁用的差异"><a href="#3-1-对比enqueue启用-禁用的差异" class="headerlink" title="3.1 对比enqueue启用/禁用的差异"></a>3.1 对比enqueue启用/禁用的差异</h3><p><strong>启用enqueue时</strong>：</p><ul><li>CREATED事件可能出现阶段性突变</li><li>SCHEDULED事件相对平稳</li><li>整体调度时间较长</li></ul><p><strong>禁用enqueue后</strong>：</p><ul><li>CREATED事件应该更加平稳</li><li>SCHEDULED事件可能成为瓶颈</li><li>整体调度时间可能缩短</li></ul><h3 id="3-2-关键指标对比"><a href="#3-2-关键指标对比" class="headerlink" title="3.2 关键指标对比"></a>3.2 关键指标对比</h3><table><thead><tr><th>指标</th><th>启用enqueue</th><th>禁用enqueue</th><th>差异分析</th></tr></thead><tbody><tr><td>CREATED事件曲线</td><td>阶段性突变</td><td>相对平稳</td><td>enqueue的资源预检查影响</td></tr><tr><td>SCHEDULED事件曲线</td><td>相对平稳</td><td>可能成为瓶颈</td><td>直接进入资源分配阶段</td></tr><tr><td>整体调度时间</td><td>较长</td><td>可能较短</td><td>跳过队列管理阶段</td></tr><tr><td>资源利用率</td><td>较高</td><td>可能较低</td><td>缺乏资源预优化</td></tr></tbody></table><h1 id="🧹如何清理测试环境"><a href="#🧹如何清理测试环境" class="headerlink" title="🧹如何清理测试环境"></a>🧹如何清理测试环境</h1><h2 id="1-停止测试"><a href="#1-停止测试" class="headerlink" title="1. 停止测试"></a>1. 停止测试</h2><p>如果测试还在运行，可以按<code>Ctrl+C</code>停止。</p><h2 id="2-清理测试环境"><a href="#2-清理测试环境" class="headerlink" title="2. 清理测试环境"></a>2. 清理测试环境</h2><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 清理Volcano测试环境</span></span><br><span class="line">make down-volcano</span><br></pre></td></tr></table></figure></div><p>这个命令会：</p><ul><li>删除所有测试Pod和Job</li><li>销毁Kind集群</li><li>清理相关资源</li></ul><h2 id="3-验证清理结果"><a href="#3-验证清理结果" class="headerlink" title="3. 验证清理结果"></a>3. 验证清理结果</h2><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查集群是否已销毁</span></span><br><span class="line">docker ps | grep volcano</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查相关目录是否清理</span></span><br><span class="line"><span class="built_in">ls</span> -la clusters/volcano/</span><br></pre></td></tr></table></figure></div><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[1] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://volcano.sh/zh/docs/actions/">[2] Volcano Documentation - Scheduler Actions Enqueue<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://github.com/volcano-sh/volcano/blob/master/docs/user-guide/how_to_configure_scheduler.md">[3] Volcano GitHub - How to Configure Scheduler<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a href="!--swig%EF%BF%BC38--">[4] <a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></a></p><p><a href="!--swig%EF%BF%BC40--">[5] <a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></a> </p>]]></content>
    
    
    <summary type="html">本文详细介绍了如何禁用Volcano调度器的enqueue功能，包括配置修改、环境搭建、功能验证、性能测试和结果分析。通过禁用enqueue，可以观察调度器在资源分配阶段的性能表现，为调度器性能优化提供参考。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
    <category term="enqueue" scheme="https://freshwlnd.github.io/tags/enqueue/"/>
    
    <category term="调度优化" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>【论文】略读笔记86-前沿-DLRM的CPU-GPU分解调度</title>
    <link href="https://freshwlnd.github.io/2025/08/13/literature/literatureNotes86/"/>
    <id>https://freshwlnd.github.io/2025/08/13/literature/literatureNotes86/</id>
    <published>2025-08-13T04:10:32.000Z</published>
    <updated>2025-08-13T04:10:35.308Z</updated>
    
    <content type="html"><![CDATA[<h1 id="x1f4d6-《GPU-Disaggregated-Serving-for-Deep-Learning-Recommendation-Models-at-Scale》"><a href="#x1f4d6-《GPU-Disaggregated-Serving-for-Deep-Learning-Recommendation-Models-at-Scale》" class="headerlink" title="📖《GPU-Disaggregated Serving for Deep Learning Recommendation Models at Scale》"></a><span class="emoji" alias="book" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png?v8">📖</span>《GPU-Disaggregated Serving for Deep Learning Recommendation Models at Scale》</h1><p>2025 年 香港科技大学+阿里巴巴团队 发表于 CCF-A 类会议 NSDI。</p><p>作者之一<a class="link" href="https://www.zhihu.com/people/llllkkkk">刘侃<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>提到：实际工作在 n 年前就完成了，论文写作在 n-m 年前就完成了~<a href="#refer-anchor-1"><sup>[2,3]</sup></a>连这样的论文都花了这么久[/惊恐]</p><p>RTP（Real Time Prediction）<a href="#refer-anchor-1"><sup>[4]</sup></a>平台是阿里内部一个通用的在线预测平台，广泛支持淘天、本地生活、AIDC、菜鸟、大文娱等搜索和推荐业务场景的 DLRM（Deep Learning Recommendation Model）部署。自2022年起，RTP开始探索大规模GPU-Disaggregation技术的落地，运用RDMA高性能网络通信构建GPU-CPU全分离的分布式推理系统。</p><h2 id="x1f3af-需求"><a href="#x1f3af-需求" class="headerlink" title="🎯需求"></a><span class="emoji" alias="dart" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png?v8">🎯</span>需求</h2><ul><li>在线推荐系统使用深度学习推荐模型（DLRMs）提供准确、个性化的推荐以提升用户体验。<ul><li>个性化推荐系统是许多面向用户、创造收入的网络服务的关键基础设施，如内容流媒体、电子商务、社交网络和网页搜索。这些系统使用深度学习推荐模型（DLRM）提供准确、个性化的推荐，以改善客户体验并增加用户参与度。</li><li>根据Meta的数据，DLRM服务消耗了当今AI云中大部分的推理资源，顶级推荐模型占用了<strong>超过79%的AI周期</strong>。</li></ul></li></ul><h3 id="DLRM特点"><a href="#DLRM特点" class="headerlink" title="DLRM特点"></a>DLRM特点</h3><p>鉴于现在 LLM 很火，作者之一刘侃用下面的表格对比了两者在线部署角度的差异，以便更好地理解问题。</p><blockquote><p>如果想了解更多 DLRM 的信息，可以参考原文或相关博客<a href="#refer-anchor-1"><sup>[2,3]</sup></a>，或原博客推荐的“典中典 W&amp;D<a href="#refer-anchor-1"><sup>[5]</sup></a>以及系统介绍<a href="#refer-anchor-1"><sup>[6]</sup></a>”。</p></blockquote><p>模型特点对比：<br>|-| DLRM | LLM |<br>|—|—|—|<br>|Feature Engineering|    ID 化、统计、笛卡尔积、查外表…太多了|    Tokenize，字符到 int 的 ID 转换|<br>|Feature Store|    100G-10T 量级，有行为序列、商品属性等|    额，如果是说 tokenizer 表的话，那就是 M 级别。|<br>|Embedding|    10G-1T 量级，大规模稀疏|    &lt;10G|<br>|Model|    DNN + Attention 等变种结合|    Transformer/Mamba，没了|</p><p>对应算力特点对比：</p><table><thead><tr><th>-</th><th>DLRM</th><th>LLM</th></tr></thead><tbody><tr><td>CPU</td><td>负载重，大量特征查表（如 KV）和计算。不同模型之间负载差距大，有 32c:1GPU 也有 128c:1GPU。</td><td>Tokenize 开销很小，剩下还有少量 Framework 和 KernelLaunch 开销。8c 算多的。</td></tr><tr><td>GPU</td><td>模型杂，方法多，实验也多；模型输入偏小，Kernel Launch 开销较大，算力利用有限。</td><td>社区很卷，都是标准算子，接近硬件算力极限。</td></tr></tbody></table><h2 id="x1f6a7-现状"><a href="#x1f6a7-现状" class="headerlink" title="🚧现状"></a><span class="emoji" alias="construction" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6a7.png?v8">🚧</span>现状</h2><ul><li>然而，大规模高效提供DLRM服务具有挑战性。</li><li>DLRMs表现出独特的资源使用模式：它们需要<strong>大量的CPU核心和巨大的内存</strong>，但只有<strong>少量GPU</strong>。<ul><li>DLRM提供具有严格的延迟服务级别目标（SLO），通常每个请求在数十毫秒的规模。同时，DLRM提供需要处理需求的频繁波动。满足延迟SLO通常意味着为<strong>峰值</strong>负载提供资源，这可以显著高于平均水平。</li></ul></li><li>在多GPU服务器上运行它们会迅速耗尽服务器的CPU和内存资源，导致大量未分配的GPU闲置，无法被其他任务利用。<ul><li>图1说明了阿里巴巴DLRM服务在生产集群中的资源需求。我们观察到明显的日间模式，峰值与谷值之比超过6倍；在季节性促销活动中，峰值负载可以比常规峰值高1.3倍，这与之前的报告一致。在如此规模上为峰值负载提供资源会导致显著的低利用率，使其在经济上不可行。</li></ul></li><li>为了减少过度配置，更好的策略是为平均负载进行配置，并在负载高峰期间启用容量借贷。<ul><li>像阿里巴巴这样的大型公司拥有多个特定用途的基础设施：一些用于训练，其他用于推理。当DLRM服务处于高峰时段时，它可以暂时从训练集群借用GPU服务器，因为训练作业对延迟不敏感，可以容忍中断。</li><li>然而，服务器池操作之间存在不匹配。<ul><li>与需要大量GPU周期的训练任务不同，推荐模型表现出较低的计算强度，并且不依赖于GPU。相反，它们执行稀疏计算，如嵌入，这需要大量内存来存储嵌入表，以及许多CPU核心用于查找和池化操作。因此，在训练服务器上运行推荐模型会迅速耗尽服务器的CPU和内存资源，留下大量未分配的GPU闲置。</li></ul></li><li>在我们的集群中，典型的DLRM服务请求48个CPU和1个GPU，而训练服务器通常有〈96个CPU，8个GPU〉。部署两个DLRM推理实例将占用主机上的所有CPU，留下6个未分配的GPU无法被其他任务利用。</li></ul></li></ul><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/2025-NSDI-Yang-GPU-Disaggregated.png?raw=true" alt="原文图1：DLRM服务的CPU需求表现出每日和季节性变化；GPU需求遵循相同趋势。从生产集群收集的跟踪数据，包括三个（带星号）电子商务促销活动。"><figcaption>原文图1：DLRM服务的CPU需求表现出每日和季节性变化；GPU需求遵循相同趋势。从生产集群收集的跟踪数据，包括三个（带星号）电子商务促销活动。</figcaption></figure></p><h2 id="x1f6e9-创新"><a href="#x1f6e9-创新" class="headerlink" title="🛩创新"></a><span class="emoji" alias="small_airplane" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6e9.png?v8">🛩</span>创新</h2><ul><li>本文描述了Prism，一个生产级DLRM服务系统，通过<strong>资源解耦</strong>的方式消除GPU碎片化。<ul><li>Prism运行在共享基础设施上，其中一组CPU节点（CNs）通过高速RDMA网络与一组异构GPU节点（HNs）互连，形成两个可以独立扩展的资源池。<ul><li>每个CN拥有大量CPU核心和高内存，但没有GPU，而每个HN是一个典型的具有多个GPU但CPU和内存资源有限的训练服务器。</li><li>这种基础设施将具有固定配置的单体服务器集群分解为两个解耦的资源池，其中CNs提供丰富的CPU和内存资源，而HNs提供大量的GPU。这两个资源池可以独立扩展以匹配动态工作负载的变化需求。</li></ul></li><li>Prism自动将DLRMs划分为CPU密集型和GPU密集型子图，并在CNs和HNs上调度它们以实现解耦服务。<ul><li>给定一个DLRM，Prism自动将其计算图分为两个子图，一个包含CPU和内存密集型操作符，另一个GPU密集型。然后，系统将这两个子图调度到选定的CN和HN上进行解耦服务，并将结果返回给用户。</li></ul></li></ul></li><li>本文还描述了在生产规模下构建解耦的DLRM系统所面临的挑战、技术和经验教训。Prism采用各种技术来最小化由解耦引起的延迟开销，包括最优图划分、拓扑感知资源管理和SLO感知通信调度。<ul><li>首先，解耦需要对模型所有者透明。<ul><li>手动重构模型到解耦版本会增加精度下降的风险，并需要模型所有者额外的努力，因此是不理想的。</li></ul></li><li>其次，系统应扩展到数千台服务器以处理过度的负载峰值。<ul><li>鉴于流量激增，它应迅速将工作负载调度到大量服务器上，以在短时间内实现显著的总吞吐量。</li></ul></li><li>第三，系统应满足DLRM服务的严格延迟SLO，由于GPU解耦导致CN和HN之间存在非平凡的通信开销。</li></ul></li><li>Prism通过三个主要组件应对这些挑战：<ul><li>一个解耦优化的实时预测（RTP）框架，该框架在CN和HN之间最优地划分计算图（第4.1节），</li><li>一个拓扑感知的资源管理器，该管理器最小化服务器间和服务器内的通信开销（第4.2节），</li><li>以及SLO感知的通信调度，确保在目标延迟SLO内进行解耦服务（第4.3节）。</li></ul></li><li>总结而言，我们的主要贡献如下：<ul><li>• 我们在部署生产规模的弹性DLRM服务时，识别了资源配置的挑战，并激励了GPU解耦服务的需求。</li><li>• 我们设计和实现了Prism，通过解耦服务从CPU节点和异构GPU节点中收集资源，缓解了服务器配置与DLRM资源需求之间的不匹配，同时仍满足延迟SLOs。</li><li>• 我们在生产环境中评估了Prism，并证明它可以有效地减少资源碎片化，而不会损害服务性能，在促销活动期间实现高效的容量借贷。</li><li>我们将生产DLRM服务跟踪作为阿里巴巴集群跟踪计划的一部分发布<a href="#refer-anchor-1"><sup>[7]</sup></a>。</li></ul></li></ul><h2 id="x1f4ca-效果"><a href="#x1f4ca-效果" class="headerlink" title="📊效果"></a><span class="emoji" alias="bar_chart" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png?v8">📊</span>效果</h2><ul><li>评估表明，Prism在拥挤的GPU集群中有效地将CPU和GPU碎片化降低了53%和27%。在季节性促销活动中，它有效地实现了从训练集群的容量借贷，节省了超过90%的GPU（§5）。Prism已在生产集群中部署超过两年，现在运行在超过10k个GPU上。</li></ul><h2 id="⛳️未来机会"><a href="#⛳️未来机会" class="headerlink" title="⛳️未来机会"></a>⛳️未来机会</h2><ul><li>操作经验。生产集群经常采用在线和离线任务的混合部署以提高效率。但在我们的情况下，分类的DLRM服务引入了频繁的RDMA网络通信。<ul><li>我们观察到，即使获得RNIC，RDMA转移潜伏期也可以在强烈的资源争夺中增加十倍。根本原因是在容器覆盖网络下，RDMA和TCP都依赖于覆盖网络方案进行通信。混合工作负载的TCP流量会影响网卡底层的流表逻辑，从而影响RDMA流量。</li><li>我们当前的解决方法涉及监视节点资源利用率和在线服务延迟，并在指标变得异常时触发离线任务的驱逐。我们认为这是未来研究的一个开放问题。</li></ul></li></ul><!-- ## <span class="emoji" alias="brain" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f9e0.png?v8">&#x1f9e0;</span>疑问 --><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://www.usenix.org/conference/nsdi25/presentation/yang">[1] Yang L, Wang Y, Yu Y, et al. {GPU-Disaggregated} Serving for Deep Learning Recommendation Models at Scale[C]//22nd USENIX Symposium on Networked Systems Design and Implementation (NSDI 25). 2025: 847-863.<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://mp.weixin.qq.com/s/fk_x6pdu2BNdyIvnkfQRfA">[2] GPU，CPU，谁是谁的“伴侣”？—— 阿里 RTP 平台的异构资源解耦大冒险 - InfoQ<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://zhuanlan.zhihu.com/p/1892365414530516703">[3] 解读 NSDI25 GPU-Disaggregated Serving for Deep Learning Recommendation Models at Scal - 知乎<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://developer.aliyun.com/article/674182">[4] 深度预测平台RTP介绍<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://arxiv.org/abs/1606.07792">[5] Wide &amp; Deep Learning for Recommender Systems<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://blog.csdn.net/tianshuai1111/article/details/136275123">[6] 【AI.OS】深入解读阿里开源系统全图化引擎<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://github.com/alibaba/clusterdata/tree/master/cluster-trace-gpuv2025">[7] Github - alibaba/clusterdata/cluster-trace-gpuv2025<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">《GPU-Disaggregated Serving for Deep Learning Recommendation Models at Scale》，大规模深度学习推荐模型的 GPU 分解服务</summary>
    
    
    
    <category term="论文" scheme="https://freshwlnd.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    <category term="略读" scheme="https://freshwlnd.github.io/categories/%E8%AE%BA%E6%96%87/%E7%95%A5%E8%AF%BB/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E8%AE%BA%E6%96%87/%E7%95%A5%E8%AF%BB/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="AI-Infra" scheme="https://freshwlnd.github.io/tags/AI-Infra/"/>
    
    <category term="DLRM" scheme="https://freshwlnd.github.io/tags/DLRM/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：Volcano 数据收集方法深度解析与Prometheus Histogram误差问题</title>
    <link href="https://freshwlnd.github.io/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/"/>
    <id>https://freshwlnd.github.io/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/</id>
    <published>2025-08-09T16:04:35.000Z</published>
    <updated>2025-08-24T06:49:42.067Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li><li><a href="/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/" title="监控与测试环境解析：测试流程拆解篇">监控与测试环境解析：测试流程拆解篇</a></li><li><a href="/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/" title="监控与测试环境解析：指标采集与可视化篇">监控与测试环境解析：指标采集与可视化篇</a></li><li><a href="/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/" title="监控与测试环境解析：自定义镜像性能回归测试">监控与测试环境解析：自定义镜像性能回归测试</a></li><li><a href="/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/" title="监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题">监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题</a></li><li><a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试">云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试</a></li><li><a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复">云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</a></li><li><a href="/2025/08/20/k8s/k8s-scheduler-performance-volcano-version/" title="云原生批调度实战：Volcano版本修改与性能测试优化">云原生批调度实战：Volcano版本修改与性能测试优化</a></li><li><a href="/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/" title="云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析">云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析</a></li><li><a href="/2025/08/24/k8s/k8s-scheduler-performance-volcano-hypothesis-verification/" title="云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结">云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结</a></li></ol></blockquote><p>在前期的文章中，我提到了<a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="调度器性能对比分析">调度器性能对比分析</a>中一个重要论断：传统方法使用Prometheus会造成很大误差，而audit-exporter方法能够准确记录性能。当时我误以为这种差异源于数据处理阶段的Prometheus histogram统计方法，但深入分析<a href="/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/" title="指标采集链路">指标采集链路</a>后发现，<strong>两种方法的核心差异实际上在数据收集阶段，而非数据处理阶段</strong>。</p><p>本文旨在澄清这一技术细节，并深入探讨Prometheus histogram的误差机制与替代方案。</p><hr><h1 id="🎯-问题澄清：数据收集-vs-数据处理"><a href="#🎯-问题澄清：数据收集-vs-数据处理" class="headerlink" title="🎯 问题澄清：数据收集 vs 数据处理"></a>🎯 问题澄清：数据收集 vs 数据处理</h1><h2 id="1-传统Prometheus方法-vs-Audit-Exporter方法"><a href="#1-传统Prometheus方法-vs-Audit-Exporter方法" class="headerlink" title="1. 传统Prometheus方法 vs Audit-Exporter方法"></a>1. 传统Prometheus方法 vs Audit-Exporter方法</h2><h3 id="传统Prometheus方法的数据收集路径"><a href="#传统Prometheus方法的数据收集路径" class="headerlink" title="传统Prometheus方法的数据收集路径"></a>传统Prometheus方法的数据收集路径</h3><p>传统的Kubernetes调度器性能监控依赖调度器自身暴露的Prometheus指标：</p><pre class="mermaid">graph LR    A[调度器内部逻辑] --&gt; B[调度器暴露metrics端点]    B --&gt; C[Prometheus定期抓取]    C --&gt; D[存储到TSDB]    D --&gt; E[Grafana查询展示]</pre><p><strong>核心特点</strong>：</p><ul><li><strong>数据源</strong>：调度器进程内部的instrumentation代码</li><li><strong>时间精度</strong>：受限于调度器代码的埋点位置和精度</li><li><strong>数据完整性</strong>：可能遗漏调度过程中的某些阶段</li><li><strong>系统开销</strong>：对调度器性能有直接影响</li></ul><h3 id="Audit-Exporter方法的数据收集路径"><a href="#Audit-Exporter方法的数据收集路径" class="headerlink" title="Audit-Exporter方法的数据收集路径"></a>Audit-Exporter方法的数据收集路径</h3><p>而audit-exporter方法从kube-apiserver的审计日志中提取性能数据：</p><pre class="mermaid">graph LR    A[调度器向APIServer发起请求] --&gt; B[APIServer记录审计日志]    B --&gt; C[audit-exporter解析日志]    C --&gt; D[转换为Prometheus指标]    D --&gt; E[Prometheus抓取存储]    E --&gt; F[Grafana查询展示]</pre><p><strong>核心特点</strong>：</p><ul><li><strong>数据源</strong>：kube-apiserver的完整请求审计记录</li><li><strong>时间精度</strong>：基于APIServer的高精度时间戳</li><li><strong>数据完整性</strong>：涵盖从请求到响应的完整生命周期</li><li><strong>系统开销</strong>：不影响调度器性能，仅增加APIServer审计开销</li></ul><h2 id="2-本质差异分析"><a href="#2-本质差异分析" class="headerlink" title="2. 本质差异分析"></a>2. 本质差异分析</h2><h3 id="✅-数据收集阶段的差异（核心）"><a href="#✅-数据收集阶段的差异（核心）" class="headerlink" title="✅ 数据收集阶段的差异（核心）"></a>✅ 数据收集阶段的差异（核心）</h3><p><strong>传统方法的局限性</strong>：</p><ol><li><strong>可控能力差</strong>：调度器内部的metric埋点可能无法覆盖所有关键路径，受制于代码中指标记录调用的位置和频率  </li><li><strong>潜在性能影响</strong>：在高负载下，调度器本身指标统计可能会影响调度性能</li></ol><p><strong>Audit-exporter方法的优势</strong>：</p><ol><li><strong>可控能力强</strong>：可以自定义捕获数据，例如本项目能监控每个API请求的完整生命周期（RequestReceived → ResponseComplete），且因为基于外部 APIServer，所以所有组件（调度器、控制器等）的请求都被统一记录</li><li><strong>无性能影响</strong>：基于外部 APIServer，低侵入性，不影响被监控组件的性能</li></ol><p>具体来说，通过audit日志能够精确记录关键事件：</p><div class="code-container" data-rel="Json"><figure class="iseeu highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">  <span class="attr">"verb"</span><span class="punctuation">:</span> <span class="string">"create"</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">"objectRef"</span><span class="punctuation">:</span> <span class="punctuation">{</span><span class="attr">"resource"</span><span class="punctuation">:</span> <span class="string">"pods"</span><span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">"stage"</span><span class="punctuation">:</span> <span class="string">"ResponseComplete"</span><span class="punctuation">,</span> </span><br><span class="line">  <span class="attr">"stageTimestamp"</span><span class="punctuation">:</span> <span class="string">"2025-01-27T10:30:45.123456Z"</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">"requestReceivedTimestamp"</span><span class="punctuation">:</span> <span class="string">"2025-01-27T10:30:45.098123Z"</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></table></figure></div><p>通过解析这些事件，可以计算出精确的 <strong>Pod创建延迟</strong>、 <strong>调度延迟</strong> 等数据。</p><h3 id="❌-数据处理阶段的相似性"><a href="#❌-数据处理阶段的相似性" class="headerlink" title="❌ 数据处理阶段的相似性"></a>❌ 数据处理阶段的相似性</h3><p><strong>重要澄清</strong>：两种方法在数据处理阶段实际上是相同的！（之前我的博客中，知道数据处理方法中 histogram 会存在误差，但现在才发现这种误差两种方法都会有）</p><p>无论是传统方法还是audit-exporter方法，最终都会：</p><ol><li>将原始数据转换为Prometheus histogram指标</li><li>使用相同的bucket配置和histogram_quantile()函数</li><li>面临相同的线性插值误差问题</li></ol><p>因此，<strong>Prometheus histogram的误差问题在两种方法中都存在</strong>，差异并非来自数据处理阶段。</p><hr><h1 id="📊-Prometheus-Histogram误差深度解析"><a href="#📊-Prometheus-Histogram误差深度解析" class="headerlink" title="📊 Prometheus Histogram误差深度解析"></a>📊 Prometheus Histogram误差深度解析</h1><h2 id="1-Histogram工作原理"><a href="#1-Histogram工作原理" class="headerlink" title="1. Histogram工作原理"></a>1. Histogram工作原理</h2><p>Prometheus histogram通过预定义的bucket边界来统计数据分布：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 示例：调度延迟histogram配置</span></span><br><span class="line">latencyHistogram := prometheus.NewHistogram(prometheus.HistogramOpts{</span><br><span class="line">    Name: <span class="string">"scheduler_latency_seconds"</span>,</span><br><span class="line">    Buckets: []<span class="type">float64</span>{<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">10</span>}, <span class="comment">// bucket边界</span></span><br><span class="line">})</span><br></pre></td></tr></table></figure></div><p>当收集到延迟数据时，每个观测值会被分配到相应的bucket中：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">观测值: 0.3秒</span><br><span class="line">↓</span><br><span class="line">累计到bucket: le="0.5", le="1", le="5", le="10", le="+Inf"</span><br></pre></td></tr></table></figure></div><p>其中 le 指 less，即 bucket 记录“小于 x 的数字有多少个”。</p><h2 id="2-线性插值误差机制"><a href="#2-线性插值误差机制" class="headerlink" title="2. 线性插值误差机制"></a>2. 线性插值误差机制</h2><h3 id="均匀分布假设的问题"><a href="#均匀分布假设的问题" class="headerlink" title="均匀分布假设的问题"></a>均匀分布假设的问题</h3><p>Prometheus使用<code>histogram_quantile()</code>函数计算百分位数时，<strong>假设每个bucket内的数据均匀分布</strong>：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Prometheus源码中的线性插值逻辑（简化）</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">linearInterpolation</span><span class="params">(bucketStart, bucketEnd <span class="type">float64</span>, rank, count <span class="type">float64</span>)</span></span> <span class="type">float64</span> {</span><br><span class="line">    <span class="keyword">return</span> bucketStart + (bucketEnd-bucketStart)*rank/count</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p><strong>例子</strong>：假设我们有bucket配置<code>[0.1, 0.5, 1.0]</code>，观测到以下数据：</p><table><thead><tr><th>Bucket范围</th><th>累计计数</th><th>实际分布</th></tr></thead><tbody><tr><td>(0, 0.1]</td><td>10</td><td>均匀分布</td></tr><tr><td>(0.1, 0.5]</td><td>90</td><td><strong>集中在0.12秒附近</strong></td></tr><tr><td>(0.5, 1.0]</td><td>100</td><td>均匀分布</td></tr></tbody></table><p>计算P90（第90个样本）：</p><ul><li><strong>Prometheus假设</strong>：P90在(0.1, 0.5]区间内均匀分布，计算得P90 ≈ 0.5秒</li><li><strong>实际情况</strong>：如果80个样本都集中在0.12秒附近，真实P90 ≈ 0.12秒</li><li><strong>结论</strong>：存在非常大的误差（尤其当桶边界非线性时，目前看大多数情况下桶边界都是指数级增长，具体原因可能是大部分数据都非均匀分布，数据范围太大时线性分布粒度过粗）</li></ul><h2 id="3-Bucket配置的关键影响"><a href="#3-Bucket配置的关键影响" class="headerlink" title="3. Bucket配置的关键影响"></a>3. Bucket配置的关键影响</h2><h3 id="默认Bucket的问题"><a href="#默认Bucket的问题" class="headerlink" title="默认Bucket的问题"></a>默认Bucket的问题</h3><p>不同Prometheus客户端库的默认bucket配置：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Go客户端默认bucket</span></span><br><span class="line">prometheus.DefBuckets = []<span class="type">float64</span>{</span><br><span class="line">    <span class="number">.005</span>, <span class="number">.01</span>, <span class="number">.025</span>, <span class="number">.05</span>, <span class="number">.1</span>, <span class="number">.25</span>, <span class="number">.5</span>, <span class="number">1</span>, <span class="number">2.5</span>, <span class="number">5</span>, <span class="number">10</span>,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Python客户端默认bucket  </span></span><br><span class="line">DEFAULT_BUCKETS = (</span><br><span class="line">    <span class="number">.005</span>, <span class="number">.01</span>, <span class="number">.025</span>, <span class="number">.05</span>, <span class="number">.075</span>, <span class="number">.1</span>, <span class="number">.25</span>, <span class="number">.5</span>, <span class="number">.75</span>, </span><br><span class="line">    <span class="number">1.0</span>, <span class="number">2.5</span>, <span class="number">5.0</span>, <span class="number">7.5</span>, <span class="number">10.0</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure></div><p>这些默认配置的问题：</p><ol><li><strong>覆盖范围过广</strong>：从5ms到10s，对特定应用场景分辨率不足</li><li><strong>分布不均</strong>：低延迟区间密集，高延迟区间稀疏</li></ol><h3 id="可能的备选Bucket策略"><a href="#可能的备选Bucket策略" class="headerlink" title="可能的备选Bucket策略"></a>可能的备选Bucket策略</h3><p><strong>1. 基于SLO设计</strong>：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 假设SLO为P95 &lt; 500ms, P99 &lt; 1s</span></span><br><span class="line">sloBuckets := []<span class="type">float64</span>{</span><br><span class="line">    <span class="number">0.050</span>, <span class="number">0.100</span>, <span class="number">0.200</span>, <span class="number">0.350</span>, <span class="comment">// P95周围密集采样</span></span><br><span class="line">    <span class="number">0.500</span>, <span class="number">0.650</span>, <span class="number">0.800</span>, <span class="number">0.950</span>, <span class="comment">// P99周围密集采样  </span></span><br><span class="line">    <span class="number">1.000</span>, <span class="number">2.000</span>, <span class="number">5.000</span>,        <span class="comment">// 异常情况覆盖</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p><strong>2. 对数分布</strong>：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指数增长，适合跨多个数量级的指标</span></span><br><span class="line">logBuckets := []<span class="type">float64</span>{</span><br><span class="line">    <span class="number">0.001</span>, <span class="number">0.002</span>, <span class="number">0.005</span>, <span class="number">0.01</span>, <span class="number">0.02</span>, <span class="number">0.05</span>, </span><br><span class="line">    <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>,</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p><strong>3. 动态调整</strong>：某些高级方案支持根据实际数据分布动态调整bucket（VictoriaMetrics似乎有这个功能）</p><hr><h1 id="🔧-Histogram替代方案与优化"><a href="#🔧-Histogram替代方案与优化" class="headerlink" title="🔧 Histogram替代方案与优化"></a>🔧 Histogram替代方案与优化</h1><h2 id="1-Summary指标"><a href="#1-Summary指标" class="headerlink" title="1. Summary指标"></a>1. Summary指标</h2><p>Prometheus提供了Summary类型作为histogram的替代：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">summary := prometheus.NewSummary(prometheus.SummaryOpts{</span><br><span class="line">    Name: <span class="string">"latency_seconds"</span>,</span><br><span class="line">    Objectives: <span class="keyword">map</span>[<span class="type">float64</span>]<span class="type">float64</span>{</span><br><span class="line">        <span class="number">0.5</span>: <span class="number">0.05</span>,  <span class="comment">// P50误差±5%</span></span><br><span class="line">        <span class="number">0.9</span>: <span class="number">0.01</span>,  <span class="comment">// P90误差±1% </span></span><br><span class="line">        <span class="number">0.99</span>: <span class="number">0.001</span>, <span class="comment">// P99误差±0.1%</span></span><br><span class="line">    },</span><br><span class="line">})</span><br></pre></td></tr></table></figure></div><p><strong>优势</strong>：</p><ul><li>客户端精确计算百分位数，无插值误差</li><li>内存占用相对固定</li><li>查询性能好</li></ul><p><strong>劣势</strong>：</p><ul><li><strong>无法聚合</strong>：不能跨实例计算全局百分位数</li><li><strong>预定义百分位</strong>：无法在查询时动态计算其他百分位数</li><li><strong>客户端开销</strong>：需要维护滑动窗口和排序</li></ul><hr><h1 id="🔚-总结"><a href="#🔚-总结" class="headerlink" title="🔚 总结"></a>🔚 总结</h1><p>通过本文的深入分析，我们可以得出以下关键结论：</p><h2 id="核心澄清"><a href="#核心澄清" class="headerlink" title="核心澄清"></a>核心澄清</h2><ol><li><p><strong>数据收集vs数据处理</strong>：audit-exporter方法与传统Prometheus方法的主要差异在于<strong>数据收集阶段</strong>（数据源和精度），而非数据处理阶段（histogram计算）。</p></li><li><p><strong>Histogram误差本质</strong>：Prometheus histogram的误差源于bucket内均匀分布假设与实际数据分布的差异，这在两种方法中都存在。默认bucket配置可能不匹配实际应用的延迟分布，需要根据SLO定制。</p></li></ol><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="📝参考文献"><a href="#📝参考文献" class="headerlink" title="📝参考文献"></a>📝参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://cloud.tencent.com/developer/article/2210383">[1] 大规模集群仿真模拟与调度器压测方法<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://hulining.gitbook.io/prometheus/practices/histograms#errors-of-quantile-estimation">[2] Prometheus中文文档 - Histogram and Summary<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>%E3%80%82)</p><p><a class="link" href="https://mp.weixin.qq.com/s/5Y_pCPIJcRpIlqhdtb3XBw">[3] 蓝胖子编程梦 - prometheus描点原理<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.cnblogs.com/hobbybear/p/17531488.html">[4] 蓝胖子编程梦 - prometheus Histogram 统计原理<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[5] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://github.com/wzshiming/kube-apiserver-audit-exporter">[6] Github - kube-apiserver-audit-exporter<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> </p>]]></content>
    
    
    <summary type="html">深入分析audit-exporter与传统Prometheus监控方法的本质差异，澄清数据收集vs数据处理阶段的误区，并探讨Prometheus histogram的bucket分布假设与误差来源。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
    <category term="监控" scheme="https://freshwlnd.github.io/tags/%E7%9B%91%E6%8E%A7/"/>
    
    <category term="Histogram" scheme="https://freshwlnd.github.io/tags/Histogram/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：Volcano 自定义镜像与二次压测</title>
    <link href="https://freshwlnd.github.io/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/"/>
    <id>https://freshwlnd.github.io/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/</id>
    <published>2025-08-08T10:13:02.000Z</published>
    <updated>2025-08-24T06:49:46.966Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li><li><a href="/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/" title="监控与测试环境解析：测试流程拆解篇">监控与测试环境解析：测试流程拆解篇</a></li><li><a href="/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/" title="监控与测试环境解析：指标采集与可视化篇">监控与测试环境解析：指标采集与可视化篇</a></li><li><a href="/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/" title="监控与测试环境解析：自定义镜像性能回归测试">监控与测试环境解析：自定义镜像性能回归测试</a></li><li><a href="/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/" title="监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题">监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题</a></li><li><a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试">云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试</a></li><li><a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复">云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</a></li><li><a href="/2025/08/20/k8s/k8s-scheduler-performance-volcano-version/" title="云原生批调度实战：Volcano版本修改与性能测试优化">云原生批调度实战：Volcano版本修改与性能测试优化</a></li><li><a href="/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/" title="云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析">云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析</a></li><li><a href="/2025/08/24/k8s/k8s-scheduler-performance-volcano-hypothesis-verification/" title="云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结">云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结</a></li></ol></blockquote><p>（在未来）优化时，在调度器源码中加入 <strong>新算法</strong> 后，我们最关心的就是：</p><blockquote><p><em>「我改的逻辑到底是否提升了吞吐量？」</em></p></blockquote><p>本篇将手把手演示 <strong>本地构建自定义 Volcano Scheduler 镜像 → 替换到 Kind 集群 → 重跑 Benchmark</strong> 的全流程，帮助大家 <strong>快速验证改动效果</strong>。</p><hr><h1 id="1️⃣-Fork-amp-修改源码"><a href="#1️⃣-Fork-amp-修改源码" class="headerlink" title="1️⃣ Fork & 修改源码"></a>1️⃣ Fork &amp; 修改源码</h1><ol><li>Fork <a class="link" href="https://github.com/volcano-sh/volcano">Volcano<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> 仓库；</li><li>切分支 <code>feat/my-algorithm</code>；</li><li>在 <code>pkg/scheduler/plugins/</code> 新增/修改调度逻辑；</li><li>本地单元测试通过后，进入构建阶段。</li></ol><blockquote><p><strong>示例改动</strong>：在 <code>allocate.go</code> 打印每次 <code>selectBestNode</code> 结果。</p></blockquote><hr><h1 id="2️⃣-本地构建镜像-amp-推送-Registry"><a href="#2️⃣-本地构建镜像-amp-推送-Registry" class="headerlink" title="2️⃣ 本地构建镜像 & 推送 Registry"></a>2️⃣ 本地构建镜像 &amp; 推送 Registry</h1><p>项目脚本已自带 <strong>本地 5000 端口 Registry</strong>，无需额外安装。关键脚本：</p><div class="code-container" data-rel="Sh"><figure class="iseeu highlight sh"><figcaption><span>hack/local-registry-with-load-images.sh:17:33</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create local registry container if not running</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="string">"<span class="subst">$(docker inspect -f '{{.State.Running}}' <span class="string">"<span class="variable">${reg_name}</span>"</span> 2&gt;/dev/null || true)</span>"</span> != <span class="string">'true'</span> ]]; <span class="keyword">then</span></span><br><span class="line">  target_image=<span class="string">"docker.io/library/registry:2.8.3"</span></span><br><span class="line">  <span class="keyword">if</span> [[ <span class="variable">${IMAGE_PREFIX}</span> != <span class="string">""</span> ]] &amp;&amp; ! docker image inspect <span class="string">"<span class="variable">${target_image}</span>"</span> &amp;&gt;/dev/null; <span class="keyword">then</span></span><br><span class="line">    docker pull <span class="string">"<span class="variable">${IMAGE_PREFIX}</span><span class="variable">${target_image}</span>"</span></span><br><span class="line">    docker tag <span class="string">"<span class="variable">${IMAGE_PREFIX}</span><span class="variable">${target_image}</span>"</span> <span class="string">"<span class="variable">${target_image}</span>"</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line">  docker run \</span><br><span class="line">    -d \</span><br><span class="line">    --restart=always \</span><br><span class="line">    -p <span class="string">"127.0.0.1:<span class="variable">${reg_port}</span>:5000"</span> \</span><br><span class="line">    --network bridge \</span><br><span class="line">    --name <span class="string">"<span class="variable">${reg_name}</span>"</span> \</span><br><span class="line">    -v <span class="string">"<span class="variable">${ROOT_DIR}</span>/registry-data:/var/lib/registry"</span> \</span><br><span class="line">    <span class="string">"<span class="variable">${target_image}</span>"</span> || :</span><br><span class="line">  <span class="built_in">sleep</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure></div><p>构建 &amp; 推送命令：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设当前位于 volcano 仓库根目录</span></span><br><span class="line"><span class="built_in">export</span> TAG=dev</span><br><span class="line">make -C build scheduler-image \</span><br><span class="line">  IMAGE_REPO=kind-registry:5000/volcano-scheduler \</span><br><span class="line">  IMAGE_TAG=<span class="variable">${TAG}</span></span><br><span class="line"></span><br><span class="line">docker push kind-registry:5000/volcano-scheduler:<span class="variable">${TAG}</span></span><br></pre></td></tr></table></figure></div><blockquote><p>若在国内环境，可通过 <code>IMAGE_PREFIX</code> 拉取基础镜像，详见上文 Metrics 篇。</p></blockquote><p>Mermaid 流程一览：</p><pre class="mermaid">graph TD;  A[源码改动] --&gt; B("Docker build")  B --&gt; C("本地镜像 kind-registry:5000")  C --&gt; D("kustomize build → kubectl apply")  D --&gt; E("Kind 集群调度性能测试")</pre><hr><h1 id="3️⃣-替换-Deployment-中的镜像"><a href="#3️⃣-替换-Deployment-中的镜像" class="headerlink" title="3️⃣ 替换 Deployment 中的镜像"></a>3️⃣ 替换 Deployment 中的镜像</h1><p>调度器 Deployment 位于：</p><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><figcaption><span>schedulers/volcano/volcano-scheduler/deployment.yaml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">kind-registry:5000/docker.io/volcanosh/vc-scheduler:v1.11.0</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">volcano-scheduler</span></span><br></pre></td></tr></table></figure></div><p>只需把 <code>image:</code> 行替换为新镜像：</p><div class="code-container" data-rel="Diff"><figure class="iseeu highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="deletion">- image: kind-registry:5000/docker.io/volcanosh/vc-scheduler:v1.11.0</span></span><br><span class="line"><span class="addition">+ image: kind-registry:5000/volcano-scheduler:dev</span></span><br></pre></td></tr></table></figure></div><hr><h1 id="4️⃣-重新压测-amp-对比指标"><a href="#4️⃣-重新压测-amp-对比指标" class="headerlink" title="4️⃣ 重新压测 & 对比指标"></a>4️⃣ 重新压测 &amp; 对比指标</h1><ol><li><strong>启动集群 &amp; 压测</strong></li></ol><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">make prepare-volcano start-volcano \</span><br><span class="line">  IMAGE_PREFIX= \</span><br><span class="line">  NODES_SIZE=1000 QUEUES_SIZE=1 JOBS_SIZE_PER_QUEUE=500 PODS_SIZE_PER_JOB=20</span><br></pre></td></tr></table></figure></div><ol start="2"><li><strong>结束测试 &amp; 保存面板</strong></li></ol><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make end-volcano</span><br></pre></td></tr></table></figure></div><ol start="3"><li><strong>查看结果目录</strong>（假设时间戳 <code>1690300000</code>）：</li></ol><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">results/</span><br><span class="line">  └── 1690300000/</span><br><span class="line">      ├── audit.log</span><br><span class="line">      ├── metrics.json</span><br><span class="line">      └── panels/</span><br><span class="line">          ├── panel-1.png   # CREATED 曲线</span><br><span class="line">          ├── panel-2.png   # SCHEDULED 曲线</span><br><span class="line">          └── panel-3.png   # RUNNING 曲线</span><br></pre></td></tr></table></figure></div><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[1] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.youtube.com/watch?v=njT5r3JjIaA&list=PLj6h78yzYM2MP0QhYFK8HOb8UqgbIkLMc&index=226">[2] A Comparative Analysis of Kueue, Volcano, and YuniKorn - Wei Huang, Apple &amp; Shiming Zhang, DaoCloud<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/tasks/debug/debug-cluster/audit/">[3] Kubernetes官方文档 - 审计<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Policy">[4] Kubernetes官方文档 - 审计Policy配置参考<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">演示如何验证修改算法后调度器性能变化情况，包括 Fork Volcano 源码、构建本地镜像、替换 Deployment 并再次执行性能测试，实现算法改动的快速回归。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
    <category term="镜像定制" scheme="https://freshwlnd.github.io/tags/%E9%95%9C%E5%83%8F%E5%AE%9A%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：Volcano 指标采集与可视化</title>
    <link href="https://freshwlnd.github.io/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/"/>
    <id>https://freshwlnd.github.io/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/</id>
    <published>2025-08-07T14:59:25.000Z</published>
    <updated>2025-08-26T09:07:28.067Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li><li><a href="/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/" title="监控与测试环境解析：测试流程拆解篇">监控与测试环境解析：测试流程拆解篇</a></li><li><a href="/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/" title="监控与测试环境解析：指标采集与可视化篇">监控与测试环境解析：指标采集与可视化篇</a></li><li><a href="/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/" title="监控与测试环境解析：自定义镜像性能回归测试">监控与测试环境解析：自定义镜像性能回归测试</a></li><li><a href="/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/" title="监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题">监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题</a></li><li><a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试">云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试</a></li><li><a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复">云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</a></li><li><a href="/2025/08/20/k8s/k8s-scheduler-performance-volcano-version/" title="云原生批调度实战：Volcano版本修改与性能测试优化">云原生批调度实战：Volcano版本修改与性能测试优化</a></li><li><a href="/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/" title="云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析">云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析</a></li><li><a href="/2025/08/24/k8s/k8s-scheduler-performance-volcano-hypothesis-verification/" title="云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结">云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结</a></li></ol></blockquote><p>上一篇我们从 <strong>Makefile → Kind → 测试代码</strong> 串起了一次最小性能测试的全链路。本篇将回答另一个常见问题：</p><blockquote><p><em>「<code>TestBatchJob</code> 跑完后，Grafana 面板上的 CREATED / SCHEDULED / RUNNING 曲线是怎么来的？」</em></p></blockquote><p>下图给出了核心组件与数据流，阅读完本文，希望能够帮你快速实现：</p><ol><li>理解 审计日志 → Exporter → Prometheus+Grafana → 截图归档 的端到端链路；</li><li>自定义审计策略 &amp; 面板查询 &amp; 截图归档。</li></ol><pre class="mermaid">graph LR;  subgraph Control-Plane 审计日志    APIServer["Kube-APIServer(开启审计)"] --&gt;|/var/log/kubernetes/kube-apiserver-audit.log| NodeDisk[(control-plane 节点磁盘)]  end  NodeDisk --&gt; Exporter["Audit-Exporter(Deployment)"]  Exporter --&gt;|/metrics| Prometheus((Prometheus))  Prometheus --&gt; Grafana[(Grafana Dashboard)]  Grafana --&gt; Script[save-result-images.sh]</pre><hr><h1 id="1️⃣-审计日志：audit-policy-yaml-决定记录什么"><a href="#1️⃣-审计日志：audit-policy-yaml-决定记录什么" class="headerlink" title="1️⃣ 审计日志：audit-policy.yaml 决定记录什么"></a>1️⃣ 审计日志：audit-policy.yaml 决定<strong>记录什么</strong></h1><p>对应前文流程图中的 <code>Control-Plane 审计日志</code> 部分，在 Kubernetes 中，每个请求在不同执行阶段都会生成审计事件；这些审计事件会根据特定策略被预处理并写入后端。<a href="#refer-anchor-1"><sup>[3]</sup></a></p><p>在此过程中，Kubernetes 审计子系统需要一份 <em>Policy</em> 文件来声明规则（指明需记录的事件范围）。而本项目根目录的 <code>audit-policy.yaml</code> 中就声明了一套规则，重点拦截了衡量调度器吞吐量的关键对象 <strong>Pod / Job 的 CRUD</strong>：</p><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><figcaption><span>audit-policy.yaml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">audit.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Policy</span></span><br><span class="line"><span class="attr">omitManagedFields:</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">omitStages:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">RequestReceived</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">ResponseStarted</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">level:</span> <span class="string">RequestResponse</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">group:</span> <span class="string">""</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">pods/binding</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">pods/status</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">group:</span> <span class="string">batch</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">jobs</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">jobs/status</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">group:</span> <span class="string">batch.volcano.sh</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">jobs</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">jobs/status</span></span><br><span class="line">  <span class="attr">verbs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">create</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">patch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">update</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">delete</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">level:</span> <span class="string">Metadata</span></span><br></pre></td></tr></table></figure></div><ul><li><code>omitStages</code> 指明每个请求可无须记录相关的阶段（stage）。Kubernetes 中已定义的阶段有：<ul><li><code>RequestReceived</code> - 此阶段对应审计处理器接收到请求后， 并且在委托给其余处理器之前生成的事件。</li><li><code>ResponseStarted</code> - 在响应消息的头部发送后，响应消息体发送前生成的事件。 只有长时间运行的请求（例如 watch）才会生成这个阶段。</li><li><code>ResponseComplete</code> - 当响应消息体完成并且没有更多数据需要传输的时候。</li><li><code>Panic</code> - 当 panic 发生时生成。</li></ul></li><li><code>level: RequestResponse</code> 既保留请求头，也包含响应体，方便后续解析出 <strong>Result 及耗时</strong>。<ul><li><code>verbs</code> 指明此规则所适用的操作（verb）列表。将 CREATE / PATCH / UPDATE / DELETE 四类操作一次性覆盖；</li><li><code>resources</code> 指明此规则所适用的资源类别列表，包含 <code>batch</code>、<code>batch.volcano.sh/jobs</code> 等 CRD，兼顾不同调度器对象。字段 <code>group</code> 给出包含资源的 API 组的名称，空字符串代表 core API 组。</li></ul></li><li><code>level: Metadata</code> 则仅记录请求的元数据（请求的用户、时间戳、资源、动词等等）， 但是不记录请求或者响应的消息体。<ul><li><code>resources</code> 为空列表意味着适用于 API 组中的所有资源类别。</li></ul></li></ul><p>通常情况下，可以使用 <code>--audit-policy-file</code> 标志将包含策略的文件传递给 <code>kube-apiserver</code>。</p><!-- 在本项目中，...。 --><h2 id="▶️-FAQ：策略细节常见疑问"><a href="#▶️-FAQ：策略细节常见疑问" class="headerlink" title="▶️ FAQ：策略细节常见疑问"></a>▶️ FAQ：策略细节常见疑问</h2><blockquote><p>💡 以下内容专门回应在阅读源码时最常见的 3 个疑惑。</p></blockquote><p><strong>① <code>level: Metadata</code> 与 <code>level: RequestResponse</code> 有何区别？为何都要保留？</strong></p><ul><li>作用域不同：<ul><li><code>RequestResponse</code> 规则<strong>只</strong>匹配我们关心的调度相关资源（<code>pods</code> / <code>jobs</code> / <code>jobs.batch.volcano.sh</code> 等），并且显式列举了 <code>verbs</code>=<code>create|patch|update|delete</code>。它会把 <strong>请求头 + 响应体</strong> 全量落盘，方便后续 Exporter 解析出 <strong>Result (Success/Failure) 与延迟直方图</strong>。</li><li><code>Metadata</code> 规则的 <code>resources: []</code> 表示「兜底规则」——凡是不在前一条命中列表内的 <strong>任何</strong> 资源，统一只记录元数据（谁、何时、做了什么），<strong>不包含请求/响应体</strong>。这样既能保留审计合规性，又避免为海量无关对象写大文件。</li></ul></li><li>优先级：Kubernetes 会按照 YAML 中的 <strong>先后顺序</strong> 匹配规则，一旦命中即停止继续匹配。因此本项目先写精确匹配、再写兜底规则，二者不会冲突。</li><li>同时编写两条规则的目的：<strong>平衡指标精度与日志体积</strong>。<code>RequestResponse</code> 为核心对象提供高粒度延迟直方图与成功率计算；<code>Metadata</code> 兜底满足审计留痕合规，又避免为成百上千个与调度无关的对象写入冗余响应体，从而显著降低磁盘占用与解析成本。</li></ul><p><strong>② 为什么 <code>omitStages</code> 要排除 <code>RequestReceived</code> 和 <code>ResponseStarted</code>？最终会记录哪些 Stage？</strong></p><ul><li>背景：一次 API 请求最多可生成四个 Stage 事件（<code>RequestReceived</code> ➡ <code>ResponseStarted</code> ➡ <code>ResponseComplete</code> ➡ <code>Panic</code>）。其中 <code>RequestReceived</code> 与 <code>ResponseStarted</code> <em>体量大且价值有限</em>：<ul><li><code>RequestReceived</code> 只表明「请求到达了 APIServer」，但拿不到任何时长信息；</li><li><code>ResponseStarted</code> 仅对 <strong>长连接 watch</strong> 场景才会生成，对我们的批量 CRUD 测试用例几乎恒为空；</li></ul></li><li>因此在策略里把这两阶段排除，既减少日志体积，也避免 Exporter 做无意义解析。</li><li>与规则无冲突：<code>omitStages</code> 作用于 <strong>全局</strong>，告诉 APIServer 在生成审计事件时忽略指定阶段；后面的 <code>rules</code> 只决定「对哪些请求生成事件以及生成到什么 level」。二者工作维度不同，不会互相覆盖。</li><li>在本项目的批量 Job / Pod 测试中，最终实际落盘的 Stage 主要是：<ul><li><code>ResponseComplete</code> — 绝大多数正常请求；</li><li><code>Panic</code> — 只有当 APIServer panic 才会出现（理论上极少）。</li></ul></li><li>如何拿到「创建 / 调度 / 运行」等关键时间点？Exporter 仅需关注 <code>stage="ResponseComplete"</code> 的事件：<ul><li><strong>创建时间</strong>：匹配 <code>verb=create</code> 且 <code>resource=pods|jobs</code> 的完成时间戳；</li><li><strong>调度时间</strong>：匹配 <code>resource=pods/binding</code> 的完成时间戳（kube-scheduler 向 APIServer 发起 bind 请求）；</li><li><strong>运行时间</strong>：匹配 <code>resource=pods/status</code>、<code>verb=update</code> 且 <code>status.phase=Running</code> 的完成时间戳；<br>Exporter 在内存中以同名 Pod UID 关联多条事件，计算时间差即可，无需 <code>RequestReceived/Started</code> 阶段即可还原完整链路。</li><li><strong><code>Panic</code> 含义</strong>：当 APIServer 在处理请求过程中发生运行时崩溃并捕获到 panic 时才会生成，用于事后问题排查，正常测试流程极罕见。</li></ul></li></ul><p><strong>③ <code>audit-policy.yaml</code> 是如何交给 Kind 中的 kube-apiserver 的？</strong></p><ul><li>每个调度器对应的 Kind 集群（位于 <code>clusters/&lt;scheduler&gt;/kind.yaml</code>）都做了如下三种操作：<ol><li><code>extraMounts</code> 把根目录下的 <code>audit-policy.yaml</code> <strong>挂载</strong>到控制平面节点的 <code>/etc/kubernetes/policies/audit-policy.yaml</code>；</li><li><code>apiServer.extraVolumes</code> 定义名为 <code>audit-policies</code> 的 HostPath 卷，并将其挂载到同一路径，确保文件在 Pod 内可读；</li><li><code>apiServer.extraArgs</code> 增加<div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">audit-policy-file:</span> <span class="string">/etc/kubernetes/policies/audit-policy.yaml</span></span><br><span class="line"><span class="attr">audit-log-path:</span> <span class="string">/var/log/kubernetes/kube-apiserver-audit.&lt;scheduler&gt;.log</span></span><br><span class="line"><span class="attr">audit-log-maxsize:</span> <span class="string">"10240"</span></span><br></pre></td></tr></table></figure></div>这样 APIServer 一启动就按照我们自定义的策略把审计事件写入宿主机 <code>/var/log/kubernetes/</code>，后续再被 Exporter Tail。</li></ol></li></ul><hr><h1 id="2️⃣-Exporter：kube-apiserver-audit-exporter-把日志变成指标"><a href="#2️⃣-Exporter：kube-apiserver-audit-exporter-把日志变成指标" class="headerlink" title="2️⃣ Exporter：kube-apiserver-audit-exporter 把日志变成指标"></a>2️⃣ Exporter：kube-apiserver-audit-exporter 把日志变成指标</h1><p>前文 Policy 决定了「记录什么」，Exporter 则决定了「怎么提炼指标」。<br>部署清单位于：<code>base/kube-apiserver-audit-exporter/kube-apiserver-audit-exporter/deployment.yaml</code></p><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><figcaption><span>base/kube-apiserver-audit-exporter/kube-apiserver-audit-exporter/deployment.yaml:24:41</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--audit-log-path</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/var/log/kubernetes/kube-apiserver-audit.log</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">kind-registry:5000/ghcr.io/wzshiming/kube-apiserver-audit-exporter/kube-apiserver-audit-exporter:v0.0.25</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">exporter</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8080</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">          <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">100Mi</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/var/log/kubernetes</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">audit-logs</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure></div><p>关键参数说明：</p><table><thead><tr><th>字段</th><th>含义</th><th>示例值</th></tr></thead><tbody><tr><td><code>--audit-log-path</code></td><td>审计日志所在宿主机路径</td><td><code>/var/log/kubernetes/kube-apiserver-audit.log</code></td></tr><tr><td><code>image</code></td><td>可独立升级的 Exporter 镜像</td><td><code>…/kube-apiserver-audit-exporter:v0.0.25</code></td></tr><tr><td>VolumeMount</td><td>将宿主机日志目录挂载进 Pod</td><td><code>mountPath: /var/log/kubernetes</code></td></tr></tbody></table><h2 id="📌-组件何时被部署？——-Makefile-触发点"><a href="#📌-组件何时被部署？——-Makefile-触发点" class="headerlink" title="📌 组件何时被部署？—— Makefile 触发点"></a>📌 组件何时被部署？—— Makefile 触发点</h2><p>在本仓库最常用的入口 <code>make default</code> 会连续执行多轮 <strong>serial-test</strong>。理解一次 <em>serial-test</em> 的执行序列即可明白监控组件的真实部署时机：</p><table><thead><tr><th>步骤</th><th>触发目标</th><th>关键动作</th></tr></thead><tbody><tr><td>1</td><td><code>prepare-&lt;scheduler&gt;</code></td><td><code>make up-&lt;scheduler&gt;</code> 创建 <strong>单个调度器集群</strong>，但此时 <em>没有</em> 监控栈</td></tr><tr><td>2</td><td><code>start-&lt;scheduler&gt;</code></td><td>运行性能测试 (<code>TestBatchJob</code> 等) 并 <strong>写入 audit-log</strong></td></tr><tr><td>3</td><td><code>end-&lt;scheduler&gt;</code></td><td><code>make down-&lt;scheduler&gt;</code> 销毁该集群，<strong>日志仍留在宿主机</strong> <code>/var/log/kubernetes/</code></td></tr><tr><td>⬇(循环)</td><td>(依次换下一个调度器)</td><td>…</td></tr><tr><td>4</td><td><code>prepare-overview</code></td><td><code>make up-overview</code> 创建 <strong>独立的 overview 集群</strong></td></tr><tr><td>5</td><td><code>start-overview</code></td><td><code>clusters/overview/Makefile:start-export</code> 部署 Exporter + PromStack，并把 <em>所有</em> <code>kube-apiserver-audit.*.log</code> HostPath 挂载到 Pod</td></tr><tr><td>6</td><td><code>save-result</code></td><td>睡 <code>$(RESULT_RECENT_DURATION_SECONDS)</code> 秒等待指标就绪→执行 <code>hack/save-result-images.sh</code> 截图</td></tr><tr><td>7</td><td><code>end-overview</code></td><td>销毁 overview 集群，聚合循环结束</td></tr></tbody></table><blockquote><p>也就是说：<strong>Export­er 和 Prometheus 直到 <em>所有</em> 调度器测试跑完后才被一次性拉起</strong>，随后一次性重放/解析先前留下的多份 audit-log。</p></blockquote><p>Exporter 会 tail 文件并实时解析，输出如下两类 Prometheus 指标（简化）：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># HELP kube_audit_event_total Total number of audit events</span><br><span class="line"># TYPE kube_audit_event_total counter</span><br><span class="line">kube_audit_event_total{verb="create",resource="pods",status="Success"}  1280</span><br><span class="line"></span><br><span class="line"># HELP kube_audit_event_latency_seconds Histogram of audit event latency</span><br><span class="line"># TYPE kube_audit_event_latency_seconds histogram</span><br><span class="line">kube_audit_event_latency_seconds_bucket{resource="pods",le="0.1"} 240</span><br></pre></td></tr></table></figure></div><p>其中 <code>status="Success"</code> 字段让我们能够在 Grafana 中分别绘制 <strong>CREATED / SCHEDULED / RUNNING</strong> 三条曲线。</p><h2 id="🔍-内部实现：Exporter-如何-tail-解析？"><a href="#🔍-内部实现：Exporter-如何-tail-解析？" class="headerlink" title="🔍 内部实现：Exporter 如何 tail + 解析？"></a>🔍 内部实现：Exporter 如何 tail + 解析？</h2><p>该部分比较复杂，涉及另一个项目。简单理解后，将该部分分为以下三步：</p><ul><li><strong>跟踪文件</strong>：Exporter 使用 Go 语言实现，入口位于 &lt;base/kube-apiserver-audit-exporter&gt;，源仓库位于<a class="link" href="https://github.com/wzshiming/kube-apiserver-audit-exporter">https://github.com/wzshiming/kube-apiserver-audit-exporter<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>。其核心依赖 <code>tail</code>（或 OS <code>inotify</code>）持续读取宿主机 <code>/var/log/kubernetes/…audit.log</code>；</li><li><strong>JSON 解析</strong>：每行审计日志都是合法 JSON，Exporter 利用 <code>encoding/json</code> 反序列化为 <code>auditinternal.Event</code> 结构体，随后按 <code>verb / resource / stage / status</code> 维度进行 <code>map</code> 聚合；</li><li><strong>指标暴露</strong>：聚合结果通过 <code>prometheus/client_golang</code> 转为 <code>counter</code> 与 <code>histogram</code> 两类 <code>kube_audit_*</code> 指标；</li></ul><p>若要<strong>增加更多指标</strong>（如自定义 label、增加 <code>summary</code> 等）：</p><ul><li><strong>定位代码</strong>：仓库中路径 <code>exporter/metrics.go</code> 下可见：以apiRequests(api_requests_total)、podSchedulingLatency(pod_scheduling_latency_seconds)为例<div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Metric definitions</span></span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">  registry = prometheus.NewRegistry()</span><br><span class="line"></span><br><span class="line">  apiRequests = prometheus.NewCounterVec(prometheus.CounterOpts{</span><br><span class="line">    Name: <span class="string">"api_requests_total"</span>,</span><br><span class="line">    Help: <span class="string">"Total number of API requests to the scheduler"</span>,</span><br><span class="line">  }, []<span class="type">string</span>{<span class="string">"cluster"</span>, <span class="string">"namespace"</span>, <span class="string">"user"</span>, <span class="string">"verb"</span>, <span class="string">"resource"</span>, <span class="string">"code"</span>})</span><br><span class="line">  <span class="comment">// 核心为：apiRequests = prometheus.NewCounterVec(...)</span></span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  podSchedulingLatency = prometheus.NewHistogramVec(prometheus.HistogramOpts{</span><br><span class="line">    Name:    <span class="string">"pod_scheduling_latency_seconds"</span>,</span><br><span class="line">    Help:    <span class="string">"Duration from pod creation to scheduled on node in seconds"</span>,</span><br><span class="line">    Buckets: prometheus.ExponentialBuckets(<span class="number">0.001</span>, <span class="number">2</span>, <span class="number">20</span>),</span><br><span class="line">  }, []<span class="type">string</span>{<span class="string">"cluster"</span>, <span class="string">"namespace"</span>, <span class="string">"user"</span>})</span><br><span class="line">  <span class="comment">// 核心为：batchJobCompleteLatency = prometheus.NewCounterVec(...)</span></span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">init</span><span class="params">()</span></span> {</span><br><span class="line">  registry.MustRegister(</span><br><span class="line">    apiRequests,</span><br><span class="line">    podSchedulingLatency,</span><br><span class="line">    ...</span><br><span class="line">  )</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// updateMetrics processes audit event and updates metrics</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *Exporter)</span></span> updateMetrics(clusterLabel <span class="type">string</span>, event auditv1.Event) {</span><br><span class="line">  <span class="comment">// ... 根据需求，自定义规则将  verb/resource 填充指标 ...</span></span><br><span class="line">  <span class="keyword">if</span> event.Stage == auditv1.StageResponseComplete {</span><br><span class="line">    labels := []<span class="type">string</span>{</span><br><span class="line">      clusterLabel,</span><br><span class="line">      ns,</span><br><span class="line">      extractUserAgent(event.UserAgent),</span><br><span class="line">      event.Verb,</span><br><span class="line">      extractResourceName(event),</span><br><span class="line">      strconv.Itoa(<span class="type">int</span>(event.ResponseStatus.Code)),</span><br><span class="line">  }</span><br><span class="line">  apiRequests.WithLabelValues(labels...).Inc()</span><br><span class="line">  <span class="comment">// 核心为：apiRequests.WithLabelValues(labels...).Inc()</span></span><br><span class="line"> }</span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">if</span> event.ObjectRef != <span class="literal">nil</span> {</span><br><span class="line">    <span class="keyword">switch</span> event.ObjectRef.Resource {</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"pods"</span>:</span><br><span class="line">      <span class="keyword">if</span> event.ObjectRef.Subresource == <span class="string">"binding"</span> &amp;&amp; event.Verb == <span class="string">"create"</span> {</span><br><span class="line">        target := buildTarget(event.ObjectRef)</span><br><span class="line">        createTime, exists := p.podCreationTimes[target]</span><br><span class="line">        <span class="keyword">if</span> !exists {</span><br><span class="line">          <span class="comment">// Kueue's audit events may create pod/binding events before pod creation events</span></span><br><span class="line">          user := extractUserAgent(event.UserAgent)</span><br><span class="line">          podSchedulingLatency.WithLabelValues(</span><br><span class="line">            clusterLabel,</span><br><span class="line">            ns,</span><br><span class="line">            user,</span><br><span class="line">          ).Observe(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">// 核心为：podSchedulingLatency.WithLabelValues(...).Observe()</span></span><br><span class="line">          p.podCreationTimes[target] = <span class="literal">nil</span></span><br><span class="line">          <span class="keyword">return</span></span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> createTime == <span class="literal">nil</span> {</span><br><span class="line">          <span class="keyword">return</span></span><br><span class="line">        }</span><br><span class="line">        latency := event.StageTimestamp.Sub(*createTime).Seconds()</span><br><span class="line"></span><br><span class="line">        user := extractUserAgent(event.UserAgent)</span><br><span class="line">        podSchedulingLatency.WithLabelValues(</span><br><span class="line">          clusterLabel,</span><br><span class="line">          ns,</span><br><span class="line">          user,</span><br><span class="line">        ).Observe(latency)</span><br><span class="line">      <span class="comment">// 核心为：podSchedulingLatency.WithLabelValues(...).Observe()</span></span><br><span class="line">        p.podCreationTimes[target] = <span class="literal">nil</span></span><br><span class="line"></span><br><span class="line">      }</span><br><span class="line">      ...</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div></li><li><strong>扩展步骤</strong>：<ol><li>复制：复制上述变量块，替换 <code>Name</code> 为 <code>kube_audit_pod_latency_seconds</code>（示例），同时调整 <code>Buckets</code>、<code>Help</code> 等参数；</li><li>修改：在 <code>updateMetrics</code> 中增加条件：<div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> evt.ObjectRef.Resource == <span class="string">"pods"</span> {</span><br><span class="line">    podLatency.WithLabelValues(evt.Verb, evt.Stage).Observe(cost)</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div></li><li>注册：确保在 <code>init()</code> 或 <code>NewCollector()</code> 中 <code>registry.MustRegister(podLatency)</code>；</li><li>换镜像：<code>docker build -t &lt;registry&gt;/audit-exporter:dev . &amp;&amp; docker push …</code>，然后在 <code>base/kube-apiserver-audit-exporter/.../deployment.yaml</code> 更新 <code>image</code> 并 <code>kubectl apply -k</code>。</li></ol></li></ul><blockquote><p>完整示例可参考项目 <code>exporter/metrics.go</code> <a class="link" href="https://github.com/wzshiming/kube-apiserver-audit-exporter/blob/master/exporter/metrics.go">源码<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>。</p></blockquote><hr><h1 id="3️⃣-Prometheus-抓取：Kustomize-一条龙"><a href="#3️⃣-Prometheus-抓取：Kustomize-一条龙" class="headerlink" title="3️⃣ Prometheus 抓取：Kustomize 一条龙"></a>3️⃣ Prometheus 抓取：Kustomize 一条龙</h1><p><code>base/kube-prometheus-stack</code> 目录通过 Kustomize 把 Exporter、Prometheus Operator 与多个 ServiceMonitor 组合在一起，无需额外手动配置抓取目标。</p><ul><li>Prometheus 会自动发现 Exporter 的 <code>metrics</code> 端口；</li><li>Grafana 面板 JSON <code>audit-exporter.json</code> 已预置在同目录，标签切片（Scheduler 类型、Namespace、Verb）均可动态选择。</li></ul><blockquote><p>若要自定义阈值或颜色，只需 <code>kubectl edit cm grafana-dashboards</code> 后刷新浏览器即可即时生效。</p></blockquote><p>其中用到了 Kustomize 工具，较为复杂，在此仅简单介绍。</p><h2 id="✨-Kustomize-简介"><a href="#✨-Kustomize-简介" class="headerlink" title="✨ Kustomize 简介"></a>✨ Kustomize 简介</h2><p>Kustomize 是 Kubernetes 官方提供的 <strong>原生资源定制工具</strong>，核心理念是“声明式 Patch 与组合”。相比 <code>helm</code>，它无需模板语言，也不引入额外 CRD：</p><ul><li><strong>基础资源</strong>（Base）：每个目录下的 <code>kustomization.yaml</code> 列出若干 <code>resources</code>，可按文件或目录引用；</li><li><strong>叠加层</strong>（Overlay）：上层可以通过 <code>patches</code>, <code>images</code>, <code>replicas</code> 等声明式字段覆写或追加配置；</li><li><strong>生成器</strong>：<code>configMapGenerator</code>, <code>secretGenerator</code> 快速为应用生成引用。</li></ul><p>在本项目中：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">base/kube-prometheus-stack/           # 监控基础组件 Base</span><br><span class="line">  ├── crd/                            # CRD 资源</span><br><span class="line">  ├── grafana/                        # Dashboard JSON 及账号</span><br><span class="line">  ├── ...</span><br><span class="line">  └── kustomization.yaml              # 声明所有组件</span><br></pre></td></tr></table></figure></div><p><code>clusters/overview/Makefile</code> 里的 <code>kubectl kustomize &lt;dir&gt; | hack/local-registry-with-load-images.sh</code> 两步做了：</p><ol><li><code>kubectl kustomize</code> → <strong>渲染</strong>：把以上 Base + Patch 解析成纯 YAML 清单；</li><li><code>local-registry-with-load-images.sh</code> → <strong>镜像处理</strong>：重写鏡像地址到本地 Kind Registry 并预先 <code>docker pull</code>；</li><li><code>kubectl create -k</code> → <strong>应用</strong>：批量创建 Exporter、Prometheus Operator、Alertmanager、ServiceMonitor 等所有资源，一次到位。</li></ol><p>因此我们才能“一键 make”拿到完整的监控栈。</p><hr><h1 id="4️⃣-截图归档：save-result-images-sh-归档面板截图"><a href="#4️⃣-截图归档：save-result-images-sh-归档面板截图" class="headerlink" title="4️⃣ 截图归档：save-result-images.sh 归档面板截图"></a>4️⃣ 截图归档：save-result-images.sh 归档面板截图</h1><p>运行 <code>make save-result</code> 后，<code>hack/save-result-images.sh</code> 会在本地循环调用 Grafana <code>render</code> API，按面板 ID 生成 <code>output/panel-*.png</code>：</p><div class="code-container" data-rel="Sh"><figure class="iseeu highlight sh"><figcaption><span>hack/save-result-images.sh</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">RECENT_DURATION=<span class="variable">${RECENT_DURATION:-5min}</span></span><br><span class="line"></span><br><span class="line">FROM=$(<span class="built_in">date</span> -u -Iseconds -d <span class="string">"- <span class="variable">${RECENT_DURATION}</span>"</span> | sed <span class="string">'s/+00:00/.000Z/'</span>)</span><br><span class="line">TO=$(<span class="built_in">date</span> -u -Iseconds | sed <span class="string">'s/+00:00/.000Z/'</span>)</span><br><span class="line"></span><br><span class="line">OUTPUT=<span class="string">"<span class="variable">${ROOT_DIR}</span>/output"</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="string">"<span class="variable">${OUTPUT}</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> {1..8}; <span class="keyword">do</span></span><br><span class="line">  wget -O <span class="string">"<span class="variable">${OUTPUT}</span>/panel-<span class="variable">${i}</span>.png"</span> <span class="string">"http://127.0.0.1:8080/grafana/render/d-solo/perf?var-rate_interval=5s&amp;orgId=1&amp;from=<span class="variable">${FROM}</span>&amp;to=<span class="variable">${TO}</span>&amp;timezone=browser&amp;var-datasource=prometheus&amp;var-resource=\$__all&amp;var-user=\$__all&amp;var-verb=create&amp;var-verb=delete&amp;var-verb=patch&amp;var-verb=update&amp;var-namespace=default&amp;var-cluster=\$__all&amp;refresh=5s&amp;theme=dark&amp;panelId=panel-<span class="variable">${i}</span>&amp;__feature.dashboardSceneSolo&amp;width=900&amp;height=500&amp;scale=10"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></div><ul><li><code>RECENT_DURATION</code> 环境变量控制 <em>截图时间窗口</em>（默认 5 分钟）；</li><li><code>FROM/TO</code> 时间戳使用 ISO-8601 UTC 毫秒格式，避免时区混淆；</li><li><code>panelId</code> 与面板 JSON 中的 <code>id</code> 一一对应，可根据需要扩展。</li></ul><p>示例输出目录结构：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">output/</span><br><span class="line">  ├── panel-1.png   # CREATED 速率</span><br><span class="line">  ├── panel-2.png   # SCHEDULED 速率</span><br><span class="line">  ├── panel-3.png   # RUNNING 速率</span><br><span class="line">  └── …</span><br></pre></td></tr></table></figure></div><h2 id="⏱️-为什么三套调度器曲线对齐到同一起始时间？"><a href="#⏱️-为什么三套调度器曲线对齐到同一起始时间？" class="headerlink" title="⏱️ 为什么三套调度器曲线对齐到同一起始时间？"></a>⏱️ 为什么三套调度器曲线对齐到同一起始时间？</h2><p>在 <em>serial-test</em> 模式下，Exporter 直到第 4 步才启动，<strong>它会从头开始顺序扫描所有 audit-log</strong>。Prometheus 采集时将「第一次 scrape 该指标的时刻」视为样本时间戳，而不是事件发生时间。因此：</p><ul><li>当 Exporter 第一次读取 <em>三份</em> 日志文件时（约 <strong>T0</strong>），所有指标都会带上 <strong>T0</strong> 的统一时间戳；</li><li>读取完第一份文件后继续第二、第三份——对于 Prometheus 来说也仍是 “T0~T0+Δ” 的时间窗口；</li></ul><p>结果就是：Grafana 图上三条曲线似乎“同一时刻起跑”。它们并非并发，而是 <strong>日志回放造成的时间折叠</strong> —— 先跑的调度器其实更早完成，但其事件被延后才被采集。</p><p>如果希望曲线按真实事件时间展开，可以：</p><ol><li>修改 Exporter，让它把 <code>evt.StageTimestamp</code> 用作 Prometheus <code>histogram</code> 的 <code>ObserveWithTimestamp</code>；</li><li>或者在测试流程中提前启动 overview 集群，使 Exporter 按实时模式持续采集。</li></ol><p>如果想<strong>亲眼查看 audit-log</strong>，有两种方法：</p><ol><li><strong>直接读宿主机文件</strong>（Kind 节点实际上是 Docker 容器）：<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> kueue-control-plane \</span><br><span class="line">  <span class="built_in">cat</span> /var/log/kubernetes/kube-apiserver-audit.kueue.log | <span class="built_in">head</span></span><br></pre></td></tr></table></figure></div>三个文件名称分别为 <code>kube-apiserver-audit.{kueue|volcano|yunikorn}.log</code>。</li><li><strong>查看 Exporter 容器日志</strong>（overview 集群）：<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n monitoring logs deploy/kube-apiserver-audit-exporter | <span class="built_in">head</span></span><br></pre></td></tr></table></figure></div>启动时它会打印 <code>starting from offset=0 file=...</code>，表明正在从头重放。</li></ol><blockquote><p>日志文件很大，可用 <code>grep '"verb":"create"'</code> 等命令过滤感兴趣事件，字段含义详见 Kubernetes 官方审计文档。</p></blockquote><h2 id="📊-Panel-一览速查表"><a href="#📊-Panel-一览速查表" class="headerlink" title="📊 Panel 一览速查表"></a>📊 Panel 一览速查表</h2><table><thead><tr><th>面板 ID</th><th>Grafana 标题</th><th>输出文件</th><th>主要查询</th><th>典型用途</th></tr></thead><tbody><tr><td>panel-1</td><td>Pod Scheduling Latency Group By UserAgent</td><td>panel-1.png</td><td><code>histogram_quantile(0.99, pod_scheduling_latency_seconds_bucket)</code> 等分位线</td><td>调度延迟长尾监控</td></tr><tr><td>panel-2</td><td>Total API Calls Group By (UserAgent, Verb, Resource)</td><td>panel-2.png</td><td><code>api_requests_total</code> 累积</td><td>全维度 API 调用计数</td></tr><tr><td>panel-3</td><td>API Calls Rate Group By (UserAgent, Verb, Resource)</td><td>panel-3.png</td><td><code>rate(api_requests_total[$rate_interval])</code></td><td>API 吞吐趋势（含资源/动词维度）</td></tr><tr><td>panel-4</td><td>BatchJob Completion Latency Group By UserAgent</td><td>panel-4.png</td><td><code>histogram_quantile(... batchjob_completion_latency_seconds_bucket)</code></td><td>关注 Job 完成延迟</td></tr><tr><td>panel-5</td><td>Total Pod Scheduled Group By UserAgent</td><td>panel-5.png</td><td><code>sum(pod_scheduling_latency_seconds_count)</code> vs <code>sum(api_requests_total{verb="create",resource="pods"})</code></td><td>对比已调度与已创建 Pod 总量</td></tr><tr><td>panel-6</td><td>Total BatchJob Completed Group By UserAgent</td><td>panel-6.png</td><td><code>sum(batchjob_completion_latency_seconds_count)</code></td><td>Job 完成总量统计</td></tr><tr><td>panel-7</td><td>Total API Calls Group By UserAgent</td><td>panel-7.png</td><td><code>sum(api_requests_total) by (cluster,user)</code></td><td>按 UserAgent 维度累计 API 调用</td></tr><tr><td>panel-8</td><td>API Calls Rate Group By UserAgent</td><td>panel-8.png</td><td><code>rate(api_requests_total[$rate_interval])</code></td><td>按 UserAgent 维度 API 吞吐</td></tr></tbody></table><blockquote><p>以上所有查询皆可在 <code>base/kube-prometheus-stack/audit-exporter.json</code> 中找到对应 <code>panel.id</code> 的 <code>expr</code> 字段。</p></blockquote><h3 id="🏷️-CREATED-SCHEDULED-指标与-Panel-5-解读"><a href="#🏷️-CREATED-SCHEDULED-指标与-Panel-5-解读" class="headerlink" title="🏷️ CREATED / SCHEDULED 指标与 Panel-5 解读"></a>🏷️ CREATED / SCHEDULED 指标与 Panel-5 解读</h3><p>Grafana 的 <strong>panel-5.png</strong>（<code>Total Pod Scheduled Group By UserAgent</code>）同时叠加了 <em>累计已调度</em> 与 <em>累计已创建</em> 两条时间序列，用来快速判断“调度器吞吐量是否跟得上工作负载产生速度”。</p><p>两条序列的 PromQL 如下：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 已调度总量（Series-A）</span><br><span class="line">sum(pod_scheduling_latency_seconds_count{cluster=~"$cluster",namespace=~"$namespace",user=~"$user"}) by (cluster,user)</span><br><span class="line"></span><br><span class="line"># 已创建总量（Series-B）</span><br><span class="line">sum(api_requests_total{cluster=~"$cluster",namespace=~"$namespace",user=~"$user",resource="pods",verb="create"}) by (cluster,user)</span><br></pre></td></tr></table></figure></div><blockquote><p>💡 <strong>资源类型</strong></p><ul><li>两条序列均针对 <strong>Pod</strong> 资源：<ul><li><strong>Series-B（CREATED）</strong> 捕获 <code>verb=create, resource=pods</code> 的审计事件；</li><li><strong>Series-A（SCHEDULED）</strong> 统计 <code>pod_scheduling_latency_seconds_count</code> 直方图计数，同样基于 Pod UID 聚合。</li></ul></li><li>Volcano 中的 <strong>PodGroup</strong> 仅在 <em>Gang</em> 场景下辅助调度，不会出现在上述指标中，因其创建频次远低于 Pod，且非调度器吞吐主瓶颈。</li></ul></blockquote><h3 id="ℹ️-为什么-Panel-5-采用-pod-scheduling-latency-seconds-count？可以改用-api-requests-total-吗？"><a href="#ℹ️-为什么-Panel-5-采用-pod-scheduling-latency-seconds-count？可以改用-api-requests-total-吗？" class="headerlink" title="ℹ️ 为什么 Panel-5 采用 pod_scheduling_latency_seconds_count？可以改用 api_requests_total 吗？"></a>ℹ️ 为什么 Panel-5 采用 <code>pod_scheduling_latency_seconds_count</code>？可以改用 <code>api_requests_total</code> 吗？</h3><ol><li><strong>数据源差异</strong><ul><li><code>api_requests_total{verb="create",resource="pods/binding"}</code> 也能反映调度动作，但 <strong>Exporter 默认并未将 <code>pods/binding</code> 事件打入此 Counter</strong>，而是交由 <code>pod_scheduling_latency_seconds</code> Histogram 统一处理（具体实现可见本文“<a href="#-%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0exporter-%E5%A6%82%E4%BD%95-tail--%E8%A7%A3%E6%9E%90">🔍 内部实现：Exporter 如何 tail + 解析？</a>”章节代码示例），以便同时统计累积延迟。</li><li>Histogram 自带 <code>_count</code> 系列，天然表示 <strong>成功调度次数</strong>；其 Bucket 仍能计算 P99 等延迟 —— 一举两得。</li></ul></li><li><strong>统计精度</strong><ul><li>Exporter 对同一 Pod 仅在 <strong>首次绑定成功</strong> 时 <code>Observe</code> 一次，所以 <code>_count</code> 与实际调度 Pod 数量一一对应，不会多计。</li><li>若直接使用 <code>api_requests_total</code> 方案，需要确保：<ol><li>Exporter 也把 <code>pods/binding</code> 计入 Counter；</li><li>Retry 或失败重试场景会导致多计，需要额外 <code>status="Success"</code> 过滤。</li></ol></li></ul></li><li><strong>替代方案</strong><ul><li>若你更习惯 Counter，可在 <code>metrics.go</code> 中追加：<div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">podBindRequests := prometheus.NewCounterVec(... name=<span class="string">"pod_bind_requests_total"</span> ...)</span><br><span class="line"><span class="comment">// 在 event.ObjectRef.Subresource=="binding" 时 Inc()</span></span><br></pre></td></tr></table></figure></div></li><li>然后在 Dashboard 将 Panel-5 的 Series-A 改为 <code>sum(pod_bind_requests_total)</code>。</li></ul></li></ol><p>⚖️ <strong>结论</strong>：默认 <code>_count</code> 与 Counter 效果一致且无需新指标，<strong>不会导致调度数量误差</strong>；若需要可根据上述方法自定义。</p><hr><h1 id="5️⃣-本地验证：三步走"><a href="#5️⃣-本地验证：三步走" class="headerlink" title="5️⃣ 本地验证：三步走"></a>5️⃣ 本地验证：三步走</h1><ol><li><strong>最小规模跑一次</strong></li></ol><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make prepare-volcano start-volcano end-volcano \</span><br><span class="line">    NODES_SIZE=1 JOBS_SIZE_PER_QUEUE=1 PODS_SIZE_PER_JOB=1</span><br></pre></td></tr></table></figure></div><ol start="2"><li><strong>打开 Grafana</strong></li></ol><p>浏览器访问 <a class="link" href="http://127.0.0.1:8080/grafana">http://127.0.0.1:8080/grafana<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>，Dashboards → <code>perf</code>，即可看到实时曲线。</p><ol start="3"><li><strong>查看截图</strong></li></ol><p>测试结束后，<code>output/</code> 将出现自动截好的图片，确认时间轴与曲线一致。</p><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[1] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.youtube.com/watch?v=njT5r3JjIaA&list=PLj6h78yzYM2MP0QhYFK8HOb8UqgbIkLMc&index=226">[2] A Comparative Analysis of Kueue, Volcano, and YuniKorn - Wei Huang, Apple &amp; Shiming Zhang, DaoCloud<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/tasks/debug/debug-cluster/audit/">[3] Kubernetes官方文档 - 审计<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/config-api/apiserver-audit.v1/#audit-k8s-io-v1-Policy">[4] Kubernetes官方文档 - 审计Policy配置参考<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">延续上一篇测试流程拆解，本文聚焦 kube-apiserver 审计日志如何被导出、转化为 Prometheus 指标并在 Grafana 面板上呈现。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
    <category term="监控" scheme="https://freshwlnd.github.io/tags/%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：Volcano 测试流程拆解</title>
    <link href="https://freshwlnd.github.io/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/"/>
    <id>https://freshwlnd.github.io/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/</id>
    <published>2025-07-27T08:21:35.000Z</published>
    <updated>2025-08-24T07:22:19.725Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li><li><a href="/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/" title="监控与测试环境解析：测试流程拆解篇">监控与测试环境解析：测试流程拆解篇</a></li><li><a href="/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/" title="监控与测试环境解析：指标采集与可视化篇">监控与测试环境解析：指标采集与可视化篇</a></li><li><a href="/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/" title="监控与测试环境解析：自定义镜像性能回归测试">监控与测试环境解析：自定义镜像性能回归测试</a></li><li><a href="/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/" title="监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题">监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题</a></li><li><a href="/2025/08/17/k8s/k8s-scheduler-performance-volcano-enqueue/" title="云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试">云原生批调度实战：Volcano调度器enqueue功能禁用与性能测试</a></li><li><a href="/2025/08/18/k8s/k8s-scheduler-performance-volcano-webhook-debug/" title="云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复">云原生批调度实战：Volcano Pod创建数量不足问题排查与Webhook超时修复</a></li><li><a href="/2025/08/20/k8s/k8s-scheduler-performance-volcano-version/" title="云原生批调度实战：Volcano版本修改与性能测试优化">云原生批调度实战：Volcano版本修改与性能测试优化</a></li><li><a href="/2025/08/22/k8s/k8s-scheduler-performance-volcano-webhook-disable/" title="云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析">云原生批调度实战：Volcano Webhook禁用与性能瓶颈分析</a></li><li><a href="/2025/08/24/k8s/k8s-scheduler-performance-volcano-hypothesis-verification/" title="云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结">云原生批调度实战：Volcano性能瓶颈猜想验证与实验总结</a></li></ol></blockquote><p>本文将以 Volcano 为代表，解析<a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">kube-scheduler-performance<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>工具一次 <strong>调度器性能测试</strong> 从集群启动到结果归档的完整链路，搞清楚 <strong>每个步骤调用了哪些文件、各自作用是什么</strong>，为后续指标剖析与实验扩展打下基础。</p><p>阅读完本文，希望能够帮你快速实现：</p><ol><li>复现最小规模的 Volcano 性能测试；</li><li>在源码中快速定位某一步骤的入口脚本 / YAML。</li></ol><hr><h1 id="1️⃣-执行链路总览"><a href="#1️⃣-执行链路总览" class="headerlink" title="1️⃣ 执行链路总览"></a>1️⃣ 执行链路总览</h1><p>执行一次最小测试的命令非常简单：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make prepare-volcano start-volcano end-volcano \</span><br><span class="line">    NODES_SIZE=1 JOBS_SIZE_PER_QUEUE=1 PODS_SIZE_PER_JOB=1</span><br></pre></td></tr></table></figure></div><p>这背后却触发了 <strong>≥ 10</strong> 个 Makefile 目标与 <strong>30+</strong> 个 YAML / Shell / Go 文件。整体时序如图所示（流程简化，仅保留关键节点）：</p><pre class="mermaid">graph TD;    A[prepare-volcano] --&gt; B[up-volcano];    B --&gt; C[kind 创建测试集群];    C --&gt; D[wait-volcano];    D --&gt; E[test-init-volcano];    A -.-&gt;|完成后调用| F[start-volcano];    F --&gt; G[reset-auditlog-volcano];    G --&gt; H[test-batch-job-volcano];    F -.-&gt; |完成后调用| I[end-volcano];    I --&gt; J[down-volcano];</pre><blockquote><p>Tips: <code>prepare-volcano → start-volcano → end-volcano</code> 由根 Makefile 的 <code>define test-scheduler</code> 宏在编译期自动展开。</p></blockquote><h2 id="宏展开示例：Volcano"><a href="#宏展开示例：Volcano" class="headerlink" title="宏展开示例：Volcano"></a>宏展开示例：Volcano</h2><p><code>define test-scheduler</code> 是一个带占位符 <code>$(1)</code> 的宏，最后通过 <code>$(foreach sched,$(SCHEDULERS),$(eval $(call test-scheduler,$(sched))))</code> 对 <em>kueue / volcano / yunikorn</em> 进行循环替换。下面以 <strong>Volcano</strong> 为例简要对比“模板”与“实例”——</p><p><strong>模板片段（截自 Makefile:110:140）</strong></p><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">define</span> test-scheduler</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: prepare-$(1)</span></span><br><span class="line"><span class="section">prepare-$(1):</span></span><br><span class="line">make up-$(1)</span><br><span class="line">make wait-$(1)</span><br><span class="line">make test-init-$(1)</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: start-$(1)</span></span><br><span class="line"><span class="section">start-$(1):</span></span><br><span class="line">make reset-auditlog-$(1)</span><br><span class="line">make test-batch-job-$(1)</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: end-$(1)</span></span><br><span class="line"><span class="section">end-$(1):</span></span><br><span class="line">make down-$(1)</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: up-$(1)</span></span><br><span class="line"><span class="section">up-$(1):</span></span><br><span class="line">make -C ./clusters/$(1) up</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: down-$(1)</span></span><br><span class="line"><span class="section">down-$(1):</span></span><br><span class="line">-make -C ./clusters/$(1) down</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: wait-$(1)</span></span><br><span class="line"><span class="section">wait-$(1):</span></span><br><span class="line">make -C ./clusters/$(1) wait</span><br><span class="line"></span><br><span class="line"><span class="section">bin/test-$(1): $(shell find ./test/utils ./test/$(1) -type f)</span></span><br><span class="line"><span class="variable">$(GO_IN_DOCKER)</span> go test -c -o ./bin/test-$(1) ./test/$(1)</span><br></pre></td></tr></table></figure></div><p><strong>展开后（自动生成）的部分规则</strong></p><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: prepare-volcano</span></span><br><span class="line"><span class="section">prepare-volcano:</span></span><br><span class="line">make up-volcano</span><br><span class="line">make wait-volcano</span><br><span class="line">make test-init-volcano</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: start-volcano</span></span><br><span class="line"><span class="section">start-volcano:</span></span><br><span class="line">make reset-auditlog-volcano</span><br><span class="line">make test-batch-job-volcano</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: end-volcano</span></span><br><span class="line"><span class="section">end-volcano:</span></span><br><span class="line">make down-volcano</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure></div><p>借助这种“写一次、生成三份”的做法，大幅减少了针对不同调度器写重复 Make 目标的工作量。</p><hr><h1 id="2️⃣-关键-Makefile-目标拆解"><a href="#2️⃣-关键-Makefile-目标拆解" class="headerlink" title="2️⃣ 关键 Makefile 目标拆解"></a>2️⃣ 关键 Makefile 目标拆解</h1><table><thead><tr><th>目标</th><th>所在文件</th><th>主要命令</th><th>职责说明</th></tr></thead><tbody><tr><td><code>prepare-volcano</code></td><td>根 <code>Makefile</code></td><td><code>make up-volcano</code> <code>make wait-volcano</code> <code>make test-init-volcano</code></td><td>集群启动 + 基础就绪检查 + 预热测试二进制</td></tr><tr><td><code>up-volcano</code></td><td><code>clusters/volcano/Makefile</code></td><td><code>kind create cluster</code> &amp; 部署 Volcano</td><td>创建 Kind 集群并应用 Volcano 相关 Kustomize 资源</td></tr><tr><td><code>wait-volcano</code></td><td>同上</td><td><code>kubectl wait --for=condition=Ready</code></td><td>等待所有 Pod Ready，含 controller / scheduler</td></tr><tr><td><code>test-init-volcano</code></td><td>根 <code>Makefile</code></td><td>运行 <code>bin/test-volcano -run ^TestInit</code></td><td>生成初始队列、扩容节点</td></tr><tr><td><code>start-volcano</code></td><td>根 <code>Makefile</code></td><td><code>make reset-auditlog-volcano</code> <code>make test-batch-job-volcano</code></td><td>清空上一轮 audit 日志 &amp; 正式发压</td></tr><tr><td><code>reset-auditlog-volcano</code></td><td><code>clusters/volcano/Makefile</code></td><td><code>kubectl delete</code> audit-log ConfigMap</td><td>置空历史日志，保证数据窗口准确</td></tr><tr><td><code>test-batch-job-volcano</code></td><td>根 <code>Makefile</code></td><td><code>bin/test-volcano -run ^TestBatchJob</code></td><td>按参数批量提交 Job / Pod 并记录时间线</td></tr><tr><td><code>end-volcano</code></td><td>根 <code>Makefile</code></td><td><code>make down-volcano</code></td><td>销毁 Kind 集群，释放资源</td></tr></tbody></table><hr><h1 id="3️⃣-clusters-volcano-目录速览"><a href="#3️⃣-clusters-volcano-目录速览" class="headerlink" title="3️⃣ clusters/volcano 目录速览"></a>3️⃣ clusters/volcano 目录速览</h1><ul><li><code>kind.yaml</code>：集群版本、节点数量、containerd 本地镜像仓库挂载；</li><li><code>deployment.yaml</code>：Volcano Controller 与 Scheduler 部署模板；</li><li><code>kustomization.yaml</code>：声明所有资源并支持 <code>image</code> 覆盖；</li><li><code>service.yaml</code>：暴露 Volcano webhook / metrics（如需）。</li></ul><p>这些文件通过 <code>kustomize build</code> 管道被 <code>up-volcano</code> 目标应用到 Kind 集群中。</p><hr><h1 id="4️⃣-test-volcano-测试代码剖析"><a href="#4️⃣-test-volcano-测试代码剖析" class="headerlink" title="4️⃣ test/volcano 测试代码剖析"></a>4️⃣ test/volcano 测试代码剖析</h1><p>核心 Go 测试位于 <code>test/volcano/</code> 目录，包含两个主要测试函数和一套完整的 Provider 实现。<strong>整体思路</strong>：通过 Go 语言编写测试用例，调用 Kubernetes API 在真实集群中创建资源，模拟大规模批量调度场景。</p><h2 id="TestInit-和-TestBatchJob-流程"><a href="#TestInit-和-TestBatchJob-流程" class="headerlink" title="TestInit 和 TestBatchJob 流程"></a>TestInit 和 TestBatchJob 流程</h2><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><figcaption><span>test/volcano/batch_job_test.go</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestInit</span><span class="params">(t *testing.T)</span></span> {</span><br><span class="line">err := provider.AddNodes(t.Context())</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">t.Fatal(err)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">err = provider.InitCase(t.Context())</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">t.Fatal(err)</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestBatchJob</span><span class="params">(t *testing.T)</span></span> {</span><br><span class="line">err := provider.AddJobs(t.Context())</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">t.Fatal(err)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">err = utils.WaitDeployment(t.Context(), utils.Resources)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">t.Fatal(err)</span><br><span class="line">}</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p><strong>核心逻辑</strong>：</p><ul><li><code>TestInit</code>：<strong>集群预热阶段</strong>，创建假节点（通过 KWOK）和配置 Volcano 队列层级</li><li><code>TestBatchJob</code>：<strong>正式压测阶段</strong>，批量提交 Job 并等待调度完成</li></ul><p>两个测试函数通过 **全局变量 <code>provider</code>**（VolcanoProvider 实例）共享状态，测试参数从环境变量或 Makefile 传入。</p><h2 id="参数传递机制解析"><a href="#参数传递机制解析" class="headerlink" title="参数传递机制解析"></a>参数传递机制解析</h2><p>测试参数的传递遵循 <strong>Makefile → 环境变量 → Go Flag → Provider 实例</strong> 的四级链路：</p><h3 id="1-Makefile-参数定义"><a href="#1-Makefile-参数定义" class="headerlink" title="1. Makefile 参数定义"></a>1. Makefile 参数定义</h3><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><figcaption><span>Makefile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CPU_PER_NODE ?= 128</span><br><span class="line">MEMORY_PER_NODE ?= 1024Gi</span><br><span class="line">NODES_SIZE ?= 1</span><br><span class="line"></span><br><span class="line">QUEUES_SIZE ?= 1</span><br><span class="line">JOBS_SIZE_PER_QUEUE ?= 1</span><br><span class="line">PODS_SIZE_PER_JOB ?= 1</span><br><span class="line"></span><br><span class="line">CPU_REQUEST_PER_POD ?= 1</span><br><span class="line">MEMORY_REQUEST_PER_POD ?= 1Gi</span><br><span class="line"></span><br><span class="line">GANG ?= false</span><br><span class="line">PREEMPTION ?= false</span><br></pre></td></tr></table></figure></div><h3 id="2-环境变量注入"><a href="#2-环境变量注入" class="headerlink" title="2. 环境变量注入"></a>2. 环境变量注入</h3><p>当执行 <code>make test-batch-job-volcano</code> 时，Makefile 会将上述变量作为环境变量传递给测试进程：</p><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><figcaption><span>Makefile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TEST_ENVS = \</span><br><span class="line">    NODES_SIZE=<span class="variable">$(NODES_SIZE)</span> \</span><br><span class="line">    CPU_PER_NODE=<span class="variable">$(CPU_PER_NODE)</span> \</span><br><span class="line">    MEMORY_PER_NODE=<span class="variable">$(MEMORY_PER_NODE)</span> \</span><br><span class="line">    QUEUES_SIZE=<span class="variable">$(QUEUES_SIZE)</span> \</span><br><span class="line">    JOBS_SIZE_PER_QUEUE=<span class="variable">$(JOBS_SIZE_PER_QUEUE)</span> \</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></div><h3 id="3-Go-Flag-解析"><a href="#3-Go-Flag-解析" class="headerlink" title="3. Go Flag 解析"></a>3. Go Flag 解析</h3><p><code>test/volcano/main_test.go</code> 中，<code>provider.AddFlags()</code> 调用 <code>test/utils/option.go</code> 的标准 flag 库：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><figcaption><span>test/utils/option.go</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(o *Options)</span></span> AddFlags() {</span><br><span class="line">    flag.StringVar(&amp;o.CpuPerNode, <span class="string">"cpu-per-node"</span>, getEnv(<span class="string">"CPU_PER_NODE"</span>, <span class="string">"32"</span>), <span class="string">"CPU resources per node"</span>)</span><br><span class="line">    flag.StringVar(&amp;o.MemoryPerNode, <span class="string">"memory-per-node"</span>, getEnv(<span class="string">"MEMORY_PER_NODE"</span>, <span class="string">"256Gi"</span>), <span class="string">"Memory resources per node"</span>)</span><br><span class="line">    flag.IntVar(&amp;o.NodeSize, <span class="string">"nodes-size"</span>, getEnvInt(<span class="string">"NODES_SIZE"</span>, <span class="number">1</span>), <span class="string">"Number of nodes to create"</span>)</span><br><span class="line">    </span><br><span class="line">    flag.IntVar(&amp;o.QueueSize, <span class="string">"queues-size"</span>, getEnvInt(<span class="string">"QUEUES_SIZE"</span>, <span class="number">1</span>), <span class="string">"Number of queues to create"</span>)</span><br><span class="line">    flag.IntVar(&amp;o.JobsSizePerQueue, <span class="string">"jobs-size-per-queue"</span>, getEnvInt(<span class="string">"JOBS_SIZE_PER_QUEUE"</span>, <span class="number">1</span>), <span class="string">"Number of jobs per queue"</span>)</span><br><span class="line">    flag.IntVar(&amp;o.PodsSizePerJob, <span class="string">"pods-size-per-job"</span>, getEnvInt(<span class="string">"PODS_SIZE_PER_JOB"</span>, <span class="number">1</span>), <span class="string">"Number of pods per job"</span>)</span><br><span class="line">    </span><br><span class="line">    flag.BoolVar(&amp;o.Gang, <span class="string">"gang"</span>, getEnvBool(<span class="string">"GANG"</span>, <span class="literal">false</span>), <span class="string">"Enable gang scheduling"</span>)</span><br><span class="line">    flag.BoolVar(&amp;o.Preemption, <span class="string">"preemption"</span>, getEnvBool(<span class="string">"PREEMPTION"</span>, <span class="literal">false</span>), <span class="string">"Enable preemption"</span>)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getEnv</span><span class="params">(key, fallback <span class="type">string</span>)</span></span> <span class="type">string</span> {</span><br><span class="line">    <span class="keyword">if</span> value, exists := os.LookupEnv(key); exists {</span><br><span class="line">        <span class="keyword">return</span> value</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> fallback</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><h3 id="4-优先级覆盖机制"><a href="#4-优先级覆盖机制" class="headerlink" title="4. 优先级覆盖机制"></a>4. 优先级覆盖机制</h3><p>参数读取有明确的优先级：<strong>命令行参数 &gt; 环境变量 &gt; 默认值</strong></p><p>例如：</p><ul><li><code>make test-batch-job-volcano NODES_SIZE=100</code>：通过 Makefile 变量传递</li><li><code>NODES_SIZE=200 ./bin/test-volcano</code>：直接设置环境变量</li><li><code>./bin/test-volcano -nodes-size=300</code>：命令行参数（最高优先级）</li></ul><p>因此，当我们在前面代码中看到 <code>p.CpuPerNode</code>、<code>p.NodeSize</code> 等字段时，它们的值最终来源于这套参数传递链路，确保了测试规模可以通过 Makefile 灵活调控。</p><h2 id="VolcanoProvider-核心实现"><a href="#VolcanoProvider-核心实现" class="headerlink" title="VolcanoProvider 核心实现"></a>VolcanoProvider 核心实现</h2><h3 id="1-AddNodes：批量创建假节点"><a href="#1-AddNodes：批量创建假节点" class="headerlink" title="1. AddNodes：批量创建假节点"></a>1. AddNodes：批量创建假节点</h3><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><figcaption><span>test/volcano/provider_test.go</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *VolcanoProvider)</span></span> AddNodes(ctx context.Context) <span class="type">error</span> {</span><br><span class="line">builder := utils.NewNodeBuilder().</span><br><span class="line">WithFastReady().</span><br><span class="line">WithCPU(p.CpuPerNode).</span><br><span class="line">WithMemory(p.MemoryPerNode)</span><br><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> p.NodeSize {</span><br><span class="line">err := utils.Resources.Create(ctx,</span><br><span class="line">builder.</span><br><span class="line">WithName(fmt.Sprintf(<span class="string">"node-%d"</span>, i)).</span><br><span class="line">Build(),</span><br><span class="line">)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p><strong>解析</strong>：通过 <code>utils.NewNodeBuilder()</code> 构造器模式批量生成 Node 对象。关键点：</p><ul><li><code>WithFastReady()</code>：设置节点状态为 Ready，跳过真实硬件检测</li><li><code>WithCPU(p.CpuPerNode)</code>：每个节点的 CPU 容量（如 “128”）</li><li><code>WithMemory(p.MemoryPerNode)</code>：每个节点的内存容量（如 “1024Gi”）</li></ul><h3 id="2-InitCase：配置-Volcano-调度器"><a href="#2-InitCase：配置-Volcano-调度器" class="headerlink" title="2. InitCase：配置 Volcano 调度器"></a>2. InitCase：配置 Volcano 调度器</h3><p><strong>关键代码片段</strong>：</p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><figcaption><span>test/volcano/provider_test.go</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *VolcanoProvider)</span></span> InitCase(ctx context.Context) <span class="type">error</span> {</span><br><span class="line"><span class="comment">// 1. 解析资源配额</span></span><br><span class="line">cpuPerQueue, err := resource.ParseQuantity(p.CpuPerQueue)</span><br><span class="line">memoryPerQueue, err := resource.ParseQuantity(p.MemoryPerQueue)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 计算层级资源限制</span></span><br><span class="line"><span class="keyword">if</span> p.CpuLendingLimit != <span class="string">""</span> {</span><br><span class="line">cpuLendingLimit, err := resource.ParseQuantity(p.CpuLendingLimit)</span><br><span class="line">hierarchy = <span class="literal">true</span></span><br><span class="line">cpuCapabilityTotal = utils.TimesQuantity(cpuPerQueue, p.QueueSize+p.ImpactingQueuesSize+p.CriticalQueuesSize).String()</span><br><span class="line">cpuCapability = cpuPerQueue.String()</span><br><span class="line">cpuPerQueue.Sub(cpuLendingLimit)  <span class="comment">// deserved = capability - lending</span></span><br><span class="line">cpuDeserved = cpuPerQueue.String()</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 更新 root Queue 配置</span></span><br><span class="line">obj := &amp;unstructured.Unstructured{}</span><br><span class="line">obj.SetName(<span class="string">"root"</span>)</span><br><span class="line">obj.SetAPIVersion(<span class="string">"scheduling.volcano.sh/v1beta1"</span>)</span><br><span class="line">obj.SetKind(<span class="string">"Queue"</span>)</span><br><span class="line">err = utils.Resources.Patch(ctx, obj, k8s.Patch{</span><br><span class="line">PatchType: types.MergePatchType,</span><br><span class="line">Data: []<span class="type">byte</span>(fmt.Sprintf(<span class="string">`{"spec":{"capability":{"cpu": %q, "memory": %q}}}`</span>, cpuCapabilityTotal, memoryCapabilityTotal)),</span><br><span class="line">})</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. 重启 Volcano Scheduler 使配置生效</span></span><br><span class="line">err = utils.RestartDeployment(ctx, utils.Resources, <span class="string">"volcano-scheduler"</span>, <span class="string">"volcano-system"</span>)</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p><strong>核心逻辑解析</strong>：</p><ol><li><strong>资源计算</strong>：根据队列数量计算总容量（<code>cpuCapabilityTotal</code>）和单队列配额（<code>cpuCapability</code>）</li><li><strong>层级调度</strong>：如果设置了 <code>CpuLendingLimit</code>，启用 <code>hierarchy=true</code>，计算 <code>deserved</code>（保证资源）和 <code>capability</code>（借用上限）</li><li><strong>动态配置</strong>：通过 <code>unstructured.Unstructured</code> 直接操作 CRD，避免导入 Volcano 依赖</li><li><strong>热重启</strong>：更新 ConfigMap 后重启 Scheduler Pod，确保新配置生效</li></ol><h3 id="3-AddJobs：分批提交作业"><a href="#3-AddJobs：分批提交作业" class="headerlink" title="3. AddJobs：分批提交作业"></a>3. AddJobs：分批提交作业</h3><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><figcaption><span>test/volcano/provider_test.go</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *VolcanoProvider)</span></span> AddJobs(ctx context.Context) <span class="type">error</span> {</span><br><span class="line">steps := []<span class="keyword">struct</span> {</span><br><span class="line">queueSize    <span class="type">int</span></span><br><span class="line">jobsPerQueue <span class="type">int</span></span><br><span class="line">podsPerJob   <span class="type">int</span></span><br><span class="line">priority     <span class="type">string</span></span><br><span class="line">duration     <span class="type">string</span></span><br><span class="line">delay        time.Duration</span><br><span class="line">}{</span><br><span class="line">{p.QueueSize, p.JobsSizePerQueue, p.PodsSizePerJob, <span class="string">"long-term-research"</span>, p.PodDuration, <span class="number">0</span>},</span><br><span class="line">{p.ImpactingQueuesSize, p.ImpactingJobsSizePerQueue, p.ImpactingPodsSizePerJob, <span class="string">"business-impacting"</span>, p.ImpactingPodDuration, <span class="number">5</span> * time.Second},</span><br><span class="line">{p.CriticalQueuesSize, p.CriticalJobsSizePerQueue, p.CriticalPodsSizePerJob, <span class="string">"human-critical"</span>, p.CriticalPodDuration, <span class="number">5</span> * time.Second},</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, step := <span class="keyword">range</span> steps {</span><br><span class="line"><span class="keyword">if</span> step.delay &gt; <span class="number">0</span> {</span><br><span class="line">time.Sleep(step.delay)  <span class="comment">// 模拟业务场景：优先级作业延迟到达</span></span><br><span class="line">}</span><br><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> step.queueSize {</span><br><span class="line"><span class="keyword">for</span> <span class="keyword">range</span> step.jobsPerQueue {</span><br><span class="line">err := p.addSingleJobs(ctx, step.podsPerJob, i, step.priority, step.duration)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p><strong>策略解析</strong>：通过 <strong>三阶段提交</strong> 模拟真实多租户场景：</p><ol><li><strong>第一波</strong>：<code>long-term-research</code> 队列，立即提交（delay=0）</li><li><strong>第二波</strong>：<code>business-impacting</code> 队列，5秒后提交（模拟业务高峰）</li><li><strong>第三波</strong>：<code>human-critical</code> 队列，再5秒后提交（模拟紧急任务）</li></ol><p>每个阶段都有独立的 <strong>queueSize × jobsPerQueue × podsPerJob</strong> 三维参数，可灵活调整压测规模。</p><h2 id="Volcano-Job-模板解析"><a href="#Volcano-Job-模板解析" class="headerlink" title="Volcano Job 模板解析"></a>Volcano Job 模板解析</h2><p>使用的是 Volcano CRD <code>batch.volcano.sh/v1alpha1/Job</code>，关键字段：</p><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><figcaption><span>test/volcano/batch_job.yaml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch.volcano.sh/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volcano-job-#{{</span> <span class="string">.name</span> <span class="string">}}-#{{</span> <span class="string">.index</span> <span class="string">}}</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment">#{{ if .gang }}</span></span><br><span class="line">  <span class="attr">minAvailable:</span> <span class="comment">#{{ .size }}</span></span><br><span class="line">  <span class="comment">#{{ else }}</span></span><br><span class="line">  <span class="attr">minAvailable:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment">#{{ end }}</span></span><br><span class="line">  <span class="attr">schedulerName:</span> <span class="string">volcano</span></span><br><span class="line">  <span class="attr">queue:</span> <span class="comment">#{{ .queue }}</span></span><br><span class="line">  <span class="attr">tasks:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">replicas:</span> <span class="comment">#{{ .size }}</span></span><br><span class="line">    <span class="attr">template:</span></span><br><span class="line">      <span class="attr">spec:</span></span><br><span class="line">        <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sleep</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">hello-world</span></span><br><span class="line">          <span class="attr">resources:</span></span><br><span class="line">            <span class="attr">requests:</span></span><br><span class="line">              <span class="attr">cpu:</span> <span class="comment">#{{ .cpuRequestPerPod }}</span></span><br><span class="line">              <span class="attr">memory:</span> <span class="comment">#{{ .memoryRequestPerPod }}</span></span><br><span class="line">        <span class="attr">nodeSelector:</span></span><br><span class="line">          <span class="attr">"type":</span> <span class="string">kwok</span></span><br></pre></td></tr></table></figure></div><p><strong>模板机制解析</strong>：</p><ul><li><strong>模板语法</strong>：<code>#{{ .variable }}</code> 由 <code>utils.YamlWithArgs()</code> 在运行时替换</li><li><strong>Gang 调度</strong>：<code>minAvailable: #{{ .size }}</code> 要求所有 Pod 同时就绪，模拟 MPI/AI 训练场景</li><li><strong>资源隔离</strong>：<code>queue: #{{ .queue }}</code> 指定队列，<code>schedulerName: volcano</code> 确保由 Volcano 调度</li><li><strong>假负载</strong>：<code>nodeSelector: "type": kwok</code> 强制调度到假节点，<code>image: hello-world</code> 秒级启动</li></ul><p><strong>实际执行过程</strong>：<code>addSingleJobs()</code> 调用 <code>utils.YamlWithArgs()</code> 将参数注入模板，再通过 <code>decoder.DecodeEach()</code> 解析 YAML 并调用 Kubernetes API 创建 Job。</p><hr><p><strong>技术要点总结</strong>：整个测试框架通过 <code>sigs.k8s.io/e2e-framework</code> 与真实 API Server 通信，产生的所有 API 调用都会被 audit-policy.yaml 记录，为后续指标分析提供数据源。</p><hr><h1 id="5️⃣-hack-脚本的幕后协同"><a href="#5️⃣-hack-脚本的幕后协同" class="headerlink" title="5️⃣ hack 脚本的幕后协同"></a>5️⃣ hack 脚本的幕后协同</h1><table><thead><tr><th>脚本</th><th>触发时机</th><th>作用</th></tr></thead><tbody><tr><td><code>hack/kind-with-local-registry.sh</code></td><td><code>up-volcano</code> 前</td><td>启动本地 5001 registry + 注入 containerd 配置（**在每个 Kind 节点的 <code>/etc/containerd/certs.d/</code> 写入 <code>hosts.toml</code>**，指向本地仓库），实现“边拉边推、本地秒级拉取”</td></tr><tr><td><code>hack/local-registry-with-load-images.sh</code></td><td><code>up-volcano</code> 期间</td><td>将远端镜像拉取后重新打 tag 推到本地 registry，Kind 节点下载极快</td></tr><tr><td><code>hack/replace-qps.sh</code></td><td><em>可选</em></td><td>修改 APIServer QPS，<strong>一键替换所有带 <code># &lt;--QPS</code> 注释的 YAML 中的数值</strong>，从而把 <code>--kube-api-qps</code> 或 webhook QPS 提到 1000+，缓解 API Server 限流</td></tr><tr><td><code>hack/save-result-images.sh</code></td><td>测试结束后</td><td>调用 Grafana API 截图面板，写入 <code>output/</code> 归档</td></tr></tbody></table><h2 id="containerd-配置注入示例"><a href="#containerd-配置注入示例" class="headerlink" title="containerd 配置注入示例"></a>containerd 配置注入示例</h2><p><code>hack/kind-with-local-registry.sh</code> 的核心逻辑：</p><div class="code-container" data-rel="Sh"><figure class="iseeu highlight sh"><figcaption><span>hack/kind-with-local-registry.sh</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create kind cluster with containerd registry configuration</span></span><br><span class="line">kind create cluster --config <span class="string">"<span class="variable">${KIND_CONFIG:-}</span>"</span> --name <span class="string">"<span class="variable">${KIND_CLUSTER_NAME:-kind}</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure containerd registry hosts on all nodes</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> $(kind get nodes); <span class="keyword">do</span></span><br><span class="line">  docker <span class="built_in">exec</span> <span class="string">"<span class="variable">${node}</span>"</span> <span class="built_in">mkdir</span> -p <span class="string">"<span class="variable">${registry_dir}</span>"</span></span><br><span class="line">  docker <span class="built_in">exec</span> -i <span class="string">"<span class="variable">${node}</span>"</span> <span class="built_in">tee</span> <span class="string">"<span class="variable">${registry_dir}</span>/hosts.toml"</span> &gt;/dev/null &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">[host."http://${in_cluster_registry}"]</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></div><p><strong>核心原理</strong>：脚本为每个 Kind 节点在 <code>/etc/containerd/certs.d/kind-registry:5000/hosts.toml</code> 写入配置，告诉 containerd 优先从本地 registry 拉取镜像，实现离线加速。</p><h4 id="修改-APIServer-QPS-示例"><a href="#修改-APIServer-QPS-示例" class="headerlink" title="修改 APIServer QPS 示例"></a>修改 APIServer QPS 示例</h4><p>当测试遇到 API Server QPS 瓶颈时，可以手动调用脚本，批量调整：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把所有标记为 "# &lt;--QPS" 的数值统一改为 2000</span></span><br><span class="line">hack/replace-qps.sh 2000</span><br></pre></td></tr></table></figure></div><p>脚本核心逻辑：</p><div class="code-container" data-rel="Sh"><figure class="iseeu highlight sh"><figcaption><span>hack/replace-qps.sh</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">find . \</span><br><span class="line">    -iname <span class="string">"*.yaml"</span> \</span><br><span class="line">    -not \( -path ./vendor/\* -o -path ./tmp/\* \) \</span><br><span class="line">    -<span class="built_in">type</span> f \</span><br><span class="line">    -<span class="built_in">exec</span> sed -i <span class="string">'s|\([0-9]\+\)\(.\+\)# &lt;--QPS|'</span><span class="variable">${QPS}</span><span class="string">'\2# &lt;--QPS|g'</span> {} +</span><br></pre></td></tr></table></figure></div><p><strong>工作原理</strong>：遍历项目内所有 <code>.yaml</code> 文件，将包含 <code># &lt;--QPS</code> 标记行的数字替换为指定值。项目中共有 12 个文件、20+ 处配置受影响，包括调度器组件的 <code>--kube-api-qps</code>/<code>--kube-api-burst</code> 和 Kind 集群的 <code>kube-api-qps</code> 参数。</p><hr><h1 id="6️⃣-结语"><a href="#6️⃣-结语" class="headerlink" title="6️⃣ 结语"></a>6️⃣ 结语</h1><p>至此，我们已经串起了 <strong>Volcano 性能测试的最小可运行链路</strong>，并定位了关键 Makefile 目标与 YAML / Go 源码。下一篇将深入 <strong>指标采集与可视化</strong>，详解 audit-exporter 如何把 <code>CREATED / SCHEDULED / RUNNING</code> 三条曲线绘制到同一张 Grafana 面板。</p><blockquote><p>📌 <strong>实践练习</strong>：如果感兴趣，可以尝试把 <code>JOBS_SIZE_PER_QUEUE</code> 改为 2，再次运行测试，观察 <code>logs/</code> 与 <code>results/</code> 目录下是否出现新的时间戳文件夹，并查看面板截图差异。 </p></blockquote><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[1] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.youtube.com/watch?v=njT5r3JjIaA&list=PLj6h78yzYM2MP0QhYFK8HOb8UqgbIkLMc&index=226">[2] A Comparative Analysis of Kueue, Volcano, and YuniKorn - Wei Huang, Apple &amp; Shiming Zhang, DaoCloud<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">本文是针对 kube-scheduling-perf 项目中 Volcano 调度器测试流程的第一篇解析，手把手带你读懂一次 make prepare-volcano → start-volcano → end-volcano 的全过程。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：本地环境测试结果与视频对比分析</title>
    <link href="https://freshwlnd.github.io/2025/07/23/k8s/k8s-scheduler-performance-test-local/"/>
    <id>https://freshwlnd.github.io/2025/07/23/k8s/k8s-scheduler-performance-test-local/</id>
    <published>2025-07-23T06:28:27.000Z</published>
    <updated>2025-08-24T05:33:47.437Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li></ol></blockquote><h1 id="💡简介"><a href="#💡简介" class="headerlink" title="💡简介"></a>💡简介</h1><p>在<a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="上一篇博客">上一篇博客</a>中，我们详细介绍了 <code>kube-scheduling-perf</code> 项目的自动化测试框架。本文记录了笔者在本地环境中实际运行该测试工具的过程，并将测试结果与 <a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="KubeCon 技术分享">KubeCon 技术分享</a> 中的视频结果进行对比分析。</p><p>通过对比发现，虽然整体趋势基本一致，但在某些测试场景下存在显著差异，这些差异主要源于硬件配置、软件版本等因素的影响。本文分析了这些差异的原因，为读者在实际部署和测试时提供参考。</p><h1 id="🖼️测试环境配置"><a href="#🖼️测试环境配置" class="headerlink" title="🖼️测试环境配置"></a>🖼️测试环境配置</h1><h2 id="硬件配置"><a href="#硬件配置" class="headerlink" title="硬件配置"></a>硬件配置</h2><p>应 <a class="link" href="https://github.com/hwdef">@hwdef<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> 建议，想办法更换了系统和实验环境，避免了前期<a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="实践篇博客">实践篇博客</a>遇到的大量因为内核版本导致的问题，也避免用低版本系统测出不准确数据导致对性能测试和优化的影响。<br>顺便一提，在实践中遇到的与内核版本无关的通用性问题也已提交<a class="link" href="https://github.com/wzshiming/kube-scheduling-perf/pull/17">PR<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>到kube-scheduling-perf仓库～</p><table><thead><tr><th>配置项</th><th>规格</th></tr></thead><tbody><tr><td><strong>操作系统</strong></td><td>Ubuntu Linux 5.15.0-143-generic</td></tr><tr><td><strong>CPU</strong></td><td>Intel Xeon Gold 6230 @ 2.10GHz, 8核</td></tr><tr><td><strong>内存</strong></td><td>15GB (可用13GB)</td></tr><tr><td><strong>存储</strong></td><td>79GB (可用21GB)</td></tr></tbody></table><h2 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h2><table><thead><tr><th>软件</th><th>版本</th></tr></thead><tbody><tr><td><strong>Docker</strong></td><td>27.5.1</td></tr><tr><td><strong>Kubernetes</strong></td><td>1.32.2 (Kind集群)</td></tr><tr><td><strong>kubectl</strong></td><td>v1.33.2</td></tr><tr><td><strong>Go</strong></td><td>1.24 (Docker容器)</td></tr></tbody></table><h2 id="存储需求"><a href="#存储需求" class="headerlink" title="存储需求"></a>存储需求</h2><table><thead><tr><th>项目</th><th>大小</th></tr></thead><tbody><tr><td><strong>当前结果目录</strong></td><td>15GB</td></tr><tr><td><strong>单个测试结果</strong></td><td>1.3GB - 2.7GB（主要占空间的是日志文件）</td></tr><tr><td><strong>建议预留空间</strong></td><td>50GB+</td></tr></tbody></table><h1 id="🧠测试结果对比分析"><a href="#🧠测试结果对比分析" class="headerlink" title="🧠测试结果对比分析"></a>🧠测试结果对比分析</h1><h2 id="第一种-Benchmark：10K-Jobs-×-1-Pod"><a href="#第一种-Benchmark：10K-Jobs-×-1-Pod" class="headerlink" title="第一种 Benchmark：10K Jobs × 1 Pod"></a>第一种 Benchmark：10K Jobs × 1 Pod</h2><p><strong>测试参数</strong>：每个Job只有1个Pod，共10K个Job，共10kPod</p><h3 id="预期结果（基于视频）"><a href="#预期结果（基于视频）" class="headerlink" title="预期结果（基于视频）"></a>预期结果（基于视频）</h3><ul><li>YuniKorn吞吐量比另外两种调度器更高，主要是因为 Kueue 和 Volcano 的 Job 受 K8s Webhook QPS限制</li><li>CREATED和SCHEDULED事件之间的差距很小，说明没有调度阶段不为瓶颈、没有排队，此时性能瓶颈为创建阶段</li></ul><h3 id="实际结果"><a href="#实际结果" class="headerlink" title="实际结果"></a>实际结果</h3><ul><li><strong>符合预期</strong>：如下图所示，YuniKorn的吞吐量确实高于其他两种调度器</li><li><strong>瓶颈分析</strong>：CREATED和SCHEDULED事件紧密跟随，说明调度阶段不是瓶颈，性能瓶颈确实在创建阶段</li></ul><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/0-raw/a.NoGang-10KJob/output/panel-5.png?raw=true" alt="图1：本地测试，无GangScheduling要求下，第一种benchmark测试结果"><figcaption>图1：本地测试，无GangScheduling要求下，第一种benchmark测试结果</figcaption></figure></p><h2 id="第二种-Benchmark：500-Jobs-×-20-Pods"><a href="#第二种-Benchmark：500-Jobs-×-20-Pods" class="headerlink" title="第二种 Benchmark：500 Jobs × 20 Pods"></a>第二种 Benchmark：500 Jobs × 20 Pods</h2><p><strong>测试参数</strong>：每个Job有20个Pod，共500个Job，共10kPod</p><h3 id="预期结果（基于视频）-1"><a href="#预期结果（基于视频）-1" class="headerlink" title="预期结果（基于视频）"></a>预期结果（基于视频）</h3><ul><li>Volcano的调度速度慢于另外两种调度器</li><li>SCHEDULED明显滞后于CREATED，说明调度速度较慢，此时性能瓶颈为调度（且根据斜率，前期调度速度快、后期逐渐变慢）</li><li>CREATED阶段性突变现象（正常情况下CREATED应该匀速增加，这里的现象说明controller会间歇性卡住一会儿）</li></ul><h3 id="实际结果-1"><a href="#实际结果-1" class="headerlink" title="实际结果"></a>实际结果</h3><ul><li><strong>部分符合预期</strong>：Volcano的调度速度确实慢于其他调度器</li><li><strong>差异点</strong>：CREATED没有成为瓶颈，始终比SCHEDULED的速度更快，与视频中的预期不符</li><li><strong>可能原因</strong>：本地环境的硬件资源限制或软件版本差异影响了测试结果</li></ul><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/0-raw/b.NoGang-500Job/output/panel-5.png?raw=true" alt="图2：本地测试，无GangScheduling要求下，第二种benchmark测试结果"><figcaption>图2：本地测试，无GangScheduling要求下，第二种benchmark测试结果</figcaption></figure></p><h3 id="进一步测试结果"><a href="#进一步测试结果" class="headerlink" title="进一步测试结果"></a>进一步测试结果</h3><p>在 <a class="link" href="https://github.com/hwdef">@hwdef<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> 支持下，在 <code>24核 CPU，96 GB 内存</code> 环境下开展测试，发现：是硬件资源限制影响了测试结果。</p><ul><li><strong>部分符合预期</strong>：Volcano的调度速度确实慢于其他调度器，且 CREATED 成为了瓶颈，确定是硬件资源限制影响了测试结果。</li><li><strong>差异点</strong>：资源丰富后，CREATED瓶颈效应存在，但反而由于资源过多，导致瓶颈效应不明显，与视频中的预期有所差异。</li><li><strong>其他问题</strong>：缺少 kueue 数据，需要重复测试（确保镜像被正确拉取）或延长 timeout 时间。</li></ul><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/0-raw/b.NoGang-500Job/output/panel-5-hwdef.png?raw=true" alt="图3：高资源量环境下测试，无GangScheduling要求下，第二种benchmark测试结果"><figcaption>图3：高资源量环境下测试，无GangScheduling要求下，第二种benchmark测试结果</figcaption></figure></p><h2 id="第三种-Benchmark：20-Jobs-×-500-Pods"><a href="#第三种-Benchmark：20-Jobs-×-500-Pods" class="headerlink" title="第三种 Benchmark：20 Jobs × 500 Pods"></a>第三种 Benchmark：20 Jobs × 500 Pods</h2><p><strong>测试参数</strong>：每个Job有500Pod，共20个Job，共10kPod</p><h3 id="预期结果（基于视频）-2"><a href="#预期结果（基于视频）-2" class="headerlink" title="预期结果（基于视频）"></a>预期结果（基于视频）</h3><ul><li>Volcano的调度速度仍然慢于另外两种调度器</li><li>SCHEDULED仍然明显滞后于CREATED，说明调度速度较慢，此时性能瓶颈为调度（且根据斜率，前期调度速度比第二种benchmark下更慢、后期逐渐加速）</li><li>不存在CREATED阶段性突变现象</li></ul><h3 id="实际结果-2"><a href="#实际结果-2" class="headerlink" title="实际结果"></a>实际结果</h3><ul><li><strong>与预期不符</strong>：反而更接近第二种benchmark的预期，前期CREATED和SCHEDULED线紧贴、后期CREATED出现阶段性突变。但至少验证了 CREATED 确实会成为瓶颈。</li><li><strong>可能原因</strong>：<ul><li>可能是CREATED成为瓶颈，导致SCHEDULED速度被严重限制</li><li>可能是SCHEDULED本来就很慢，反过来导致CREATED没必要提前创建（可能新版本下有其他机制做出该决策）</li></ul></li></ul><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/0-raw/c.NoGang-20Job/output/panel-5.png?raw=true" alt="图4：本地测试，无GangScheduling要求下，第三种benchmark测试结果"><figcaption>图4：本地测试，无GangScheduling要求下，第三种benchmark测试结果</figcaption></figure></p><h3 id="进一步测试结果-1"><a href="#进一步测试结果-1" class="headerlink" title="进一步测试结果"></a>进一步测试结果</h3><p>在 <a class="link" href="https://github.com/hwdef">@hwdef<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> 支持下，在 <code>24核 CPU，96 GB 内存</code> 环境下开展测试，发现：不是硬件资源限制问题。</p><ul><li>与原环境下结果类似，证明即使提供更多硬件资源也仍然存在该问题。</li><li>验证了 CREATED 确实会成为瓶颈，同时可能有 SCHEDULED 对 CREATED 的反向限制。</li></ul><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/0-raw/c.NoGang-20Job/output/panel-5-hwdef.png?raw=true" alt="图5：高资源量环境下测试，无GangScheduling要求下，第三种benchmark测试结果"><figcaption>图5：高资源量环境下测试，无GangScheduling要求下，第三种benchmark测试结果</figcaption></figure></p><h2 id="第四种-Benchmark：1-Job-×-10K-Pods"><a href="#第四种-Benchmark：1-Job-×-10K-Pods" class="headerlink" title="第四种 Benchmark：1 Job × 10K Pods"></a>第四种 Benchmark：1 Job × 10K Pods</h2><p><strong>测试参数</strong>：每个Job有10kPod，共1个Job，共10kPod</p><h3 id="预期结果（基于视频）-3"><a href="#预期结果（基于视频）-3" class="headerlink" title="预期结果（基于视频）"></a>预期结果（基于视频）</h3><ul><li>现象与第三种benchmark类似</li><li>根据斜率，调度速度整体比较平稳</li></ul><h3 id="实际结果-3"><a href="#实际结果-3" class="headerlink" title="实际结果"></a>实际结果</h3><ul><li><strong>与预期不符</strong>：和第三种benchmark类似，CREATED出现阶段性突变。</li><li><strong>可能原因</strong>：本地环境的资源限制影响了大规模Pod的创建和调度。</li></ul><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/0-raw/d.NoGang-1Job/output/panel-5.png?raw=true" alt="图6：本地测试，无GangScheduling要求下，第四种benchmark测试结果"><figcaption>图6：本地测试，无GangScheduling要求下，第四种benchmark测试结果</figcaption></figure></p><h3 id="进一步测试结果-2"><a href="#进一步测试结果-2" class="headerlink" title="进一步测试结果"></a>进一步测试结果</h3><p>在 <a class="link" href="https://github.com/hwdef">@hwdef<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> 支持下，在 <code>24核 CPU，96 GB 内存</code> 环境下开展测试，发现：不是硬件资源限制问题。</p><ul><li>与原环境下结果类似，证明即使提供更多硬件资源也仍然存在该问题。</li><li>验证了 CREATED 确实会成为瓶颈，同时可能有 SCHEDULED 对 CREATED 的反向限制。（与第三种测试几乎一致）</li></ul><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://github.com/Freshwlnd/image/blob/blog/kube-scheduling-perf-image/0-raw/d.NoGang-1Job/output/panel-5-hwdef.png?raw=true" alt="图7：高资源量环境下测试，无GangScheduling要求下，第四种benchmark测试结果"><figcaption>图7：高资源量环境下测试，无GangScheduling要求下，第四种benchmark测试结果</figcaption></figure></p><h2 id="Gang调度测试"><a href="#Gang调度测试" class="headerlink" title="Gang调度测试"></a>Gang调度测试</h2><h3 id="预期结果（基于视频）-4"><a href="#预期结果（基于视频）-4" class="headerlink" title="预期结果（基于视频）"></a>预期结果（基于视频）</h3><ul><li>Volcano的性能都是最佳的</li></ul><h3 id="实际结果-4"><a href="#实际结果-4" class="headerlink" title="实际结果"></a>实际结果</h3><ul><li><strong>与预期不符</strong>：可能由于机器资源有限，大部分情况下集中调度甚至无法正常创建Pod</li><li><strong>额外发现</strong>：YuniKorn所创建的Pod数量大于10k，猜测可能是出现bug导致Pod重启（但按理说使用模拟环境不应该有该问题）</li></ul><h1 id="🔨差异原因分析"><a href="#🔨差异原因分析" class="headerlink" title="🔨差异原因分析"></a>🔨差异原因分析</h1><h2 id="1-硬件资源限制"><a href="#1-硬件资源限制" class="headerlink" title="1. 硬件资源限制"></a>1. 硬件资源限制</h2><p><strong>本地环境限制</strong>：</p><ul><li>CPU：8核 vs 视频中可能使用更高配置（kube-scheduling-perf仓库推荐 16核）</li><li>内存：15GB vs 视频中可能使用更大内存（kube-scheduling-perf仓库推荐 16GB）</li></ul><p><strong>影响</strong>：硬件资源不足可能导致：</p><ul><li>Pod创建速度受限</li><li>调度器处理能力下降</li><li>系统整体性能瓶颈</li></ul><h2 id="2-软件版本差异"><a href="#2-软件版本差异" class="headerlink" title="2. 软件版本差异"></a>2. 软件版本差异</h2><p><strong>版本对比</strong>：</p><ul><li>Kubernetes：1.32.2 vs 视频中可能使用不同版本</li><li>Docker：27.5.1 vs 视频中可能使用不同版本</li><li>调度器版本：可能存在差异</li></ul><p><strong>影响</strong>：不同版本可能存在：</p><ul><li>性能优化差异</li><li>Bug修复差异</li><li>默认配置差异</li></ul><h1 id="🏥总结与反思"><a href="#🏥总结与反思" class="headerlink" title="🏥总结与反思"></a>🏥总结与反思</h1><h2 id="主要发现"><a href="#主要发现" class="headerlink" title="主要发现"></a>主要发现</h2><ol><li><strong>整体趋势一致</strong>：虽然存在差异，但三种调度器的相对性能排名基本符合预期</li><li><strong>环境影响显著</strong>：硬件配置、软件版本、网络环境等因素对测试结果有重要影响</li><li><strong>资源瓶颈明显</strong>：在资源受限的本地环境中，CREATED事件更容易成为瓶颈</li></ol><h2 id="后续工作"><a href="#后续工作" class="headerlink" title="后续工作"></a>后续工作</h2><ol><li><strong>脚本分析</strong>：进一步浏览 kube-scheduling-perf 脚本所使用的环境、所监控的指标、所使用的监控方式，为后续增加其它指标监控做准备</li><li><strong>调度器分析</strong>：进一步分析CREATED阶段性突变的具体原因</li></ol><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[1] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.youtube.com/watch?v=njT5r3JjIaA&list=PLj6h78yzYM2MP0QhYFK8HOb8UqgbIkLMc&index=226">[2] A Comparative Analysis of Kueue, Volcano, and YuniKorn - Wei Huang, Apple &amp; Shiming Zhang, DaoCloud<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kueue.sigs.k8s.io/">[3] Kueue Documentation<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://volcano.sh/">[4] Volcano Documentation<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://yunikorn.apache.org/">[5] YuniKorn Documentation<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">本文记录了在本地环境中使用 kube-scheduling-perf 工具对 Kueue、Volcano、YuniKorn 三大调度器进行性能测试的实际结果，并与 KubeCon 技术分享中的视频结果进行对比分析。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
    <category term="本地测试" scheme="https://freshwlnd.github.io/tags/%E6%9C%AC%E5%9C%B0%E6%B5%8B%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>【集群】Kubernetes Webhook 实战：Kueue 调度器准入控制故障排除与性能优化</title>
    <link href="https://freshwlnd.github.io/2025/07/11/k8s/k8s-scheduler-performance-test-webhook/"/>
    <id>https://freshwlnd.github.io/2025/07/11/k8s/k8s-scheduler-performance-test-webhook/</id>
    <published>2025-07-11T03:49:11.000Z</published>
    <updated>2025-07-11T04:14:26.258Z</updated>
    
    <content type="html"><![CDATA[<h1 id="💡简介"><a href="#💡简介" class="headerlink" title="💡简介"></a>💡简介</h1><blockquote><p><strong>📖 文档定位</strong>：本文为 Kueue 调度器 Webhook 机制的<strong>实战故障排除篇</strong>，与 <a href="/2025/07/08/k8s/k8s-webhook/" title="理论介绍文档">理论介绍文档</a> 形成互补。理论文档重点解析 Webhook 的基本概念和工作原理，而本文则专注于解决实际部署和测试过程中遇到的 Webhook 相关问题。</p></blockquote><p><strong>适用场景</strong>：如果您在部署 Kueue 调度器或进行调度器性能测试时遇到 Webhook 相关的错误，那么本文档将为您提供详细的问题诊断和解决方案。</p><h1 id="🖼️背景"><a href="#🖼️背景" class="headerlink" title="🖼️背景"></a>🖼️背景</h1><h2 id="问题起源"><a href="#问题起源" class="headerlink" title="问题起源"></a>问题起源</h2><p>在 <a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="调度器性能测试调试">调度器性能测试调试</a> 过程中，我们遇到了一个典型的 Webhook 连接问题：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Internal error occurred: failed calling webhook "mresourceflavor.kb.io": failed to call webhook: </span><br><span class="line">Post "https://kueue-webhook-service.kueue-system.svc:443/mutate-kueue-x-k8s-io-v1beta1-resourceflavor?timeout=10s": </span><br><span class="line">dial tcp 10.96.33.70:443: connect: connection refused</span><br></pre></td></tr></table></figure></div><p>这个错误不仅影响了测试的顺利进行，也让我们深入思考了 Webhook 在 Kubernetes 调度器中的重要作用。</p><h2 id="为什么需要-Webhook？"><a href="#为什么需要-Webhook？" class="headerlink" title="为什么需要 Webhook？"></a>为什么需要 Webhook？</h2><h3 id="1-准入控制需求"><a href="#1-准入控制需求" class="headerlink" title="1. 准入控制需求"></a>1. 准入控制需求</h3><ul><li><strong>资源验证</strong>：确保创建的资源符合集群策略</li><li><strong>自动标签</strong>：为工作负载添加必要的元数据</li><li><strong>队列管理</strong>：协调工作负载与调度队列的关系</li></ul><h3 id="2-扩展性要求"><a href="#2-扩展性要求" class="headerlink" title="2. 扩展性要求"></a>2. 扩展性要求</h3><ul><li><strong>动态配置</strong>：无需重启 API 服务器即可添加新的验证规则</li><li><strong>外部集成</strong>：允许外部系统参与资源管理决策</li><li><strong>安全增强</strong>：提供额外的安全验证层</li></ul><h1 id="🧠Webhook-在-Kueue-中的作用机制"><a href="#🧠Webhook-在-Kueue-中的作用机制" class="headerlink" title="🧠Webhook 在 Kueue 中的作用机制"></a>🧠Webhook 在 Kueue 中的作用机制</h1><h2 id="1-准入控制机制"><a href="#1-准入控制机制" class="headerlink" title="1. 准入控制机制"></a>1. 准入控制机制</h2><p>Kueue 使用 Webhook 实现以下核心功能：</p><h3 id="1-1-资源验证"><a href="#1-1-资源验证" class="headerlink" title="1.1 资源验证"></a>1.1 资源验证</h3><ul><li><strong>Job 验证</strong>：检查 Kubernetes Job 是否符合 Kueue 管理要求</li><li><strong>Pod 验证</strong>：验证 Pod 的资源请求和限制</li><li><strong>ResourceFlavor 验证</strong>：确保资源风味配置正确</li></ul><h3 id="1-2-自动标签管理"><a href="#1-2-自动标签管理" class="headerlink" title="1.2 自动标签管理"></a>1.2 自动标签管理</h3><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自动添加的标签示例</span></span><br><span class="line"><span class="attr">kueue.x-k8s.io/managed:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">kueue.x-k8s.io/queue-name:</span> <span class="string">"default"</span></span><br></pre></td></tr></table></figure></div><h3 id="1-3-队列分配"><a href="#1-3-队列分配" class="headerlink" title="1.3 队列分配"></a>1.3 队列分配</h3><ul><li><strong>工作负载分类</strong>：根据标签将工作负载分配到相应队列</li><li><strong>资源配额检查</strong>：验证工作负载是否超出队列资源限制</li></ul><h2 id="2-Webhook-类型"><a href="#2-Webhook-类型" class="headerlink" title="2. Webhook 类型"></a>2. Webhook 类型</h2><h3 id="2-1-Mutating-Webhook（修改性）"><a href="#2-1-Mutating-Webhook（修改性）" class="headerlink" title="2.1 Mutating Webhook（修改性）"></a>2.1 Mutating Webhook（修改性）</h3><ul><li><strong>作用</strong>：修改资源内容</li><li><strong>时机</strong>：在验证性 Webhook 之前执行</li><li><strong>功能</strong>：添加默认标签、注解等</li></ul><h3 id="2-2-Validating-Webhook（验证性）"><a href="#2-2-Validating-Webhook（验证性）" class="headerlink" title="2.2 Validating Webhook（验证性）"></a>2.2 Validating Webhook（验证性）</h3><ul><li><strong>作用</strong>：验证资源是否符合规则</li><li><strong>时机</strong>：在资源持久化到 etcd 之前</li><li><strong>结果</strong>：允许或拒绝请求</li></ul><h2 id="3-工作流程详解"><a href="#3-工作流程详解" class="headerlink" title="3. 工作流程详解"></a>3. 工作流程详解</h2><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户创建 Job → API 服务器接收请求 → Mutating Webhook → Validating Webhook → 持久化到 etcd</span><br></pre></td></tr></table></figure></div><p><strong>具体步骤</strong>：</p><ol><li><strong>请求接收</strong>：用户提交 Job 创建请求</li><li><strong>Webhook 拦截</strong>：API 服务器根据配置拦截请求</li><li><strong>修改处理</strong>：Mutating Webhook 添加必要标签</li><li><strong>验证处理</strong>：Validating Webhook 检查资源合规性</li><li><strong>结果返回</strong>：处理结果返回给 API 服务器</li><li><strong>资源创建</strong>：验证通过后，Job 被创建</li></ol><h1 id="🔨Webhook-配置详解"><a href="#🔨Webhook-配置详解" class="headerlink" title="🔨Webhook 配置详解"></a>🔨Webhook 配置详解</h1><h2 id="1-服务配置"><a href="#1-服务配置" class="headerlink" title="1. 服务配置"></a>1. 服务配置</h2><h3 id="1-1-Webhook-服务定义"><a href="#1-1-Webhook-服务定义" class="headerlink" title="1.1 Webhook 服务定义"></a>1.1 Webhook 服务定义</h3><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kueue-webhook-service</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kueue-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">443</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">9443</span>  <span class="comment"># 指向 webhook 服务器的端口</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">control-plane:</span> <span class="string">controller-manager</span></span><br></pre></td></tr></table></figure></div><h3 id="1-2-Webhook-服务器配置"><a href="#1-2-Webhook-服务器配置" class="headerlink" title="1.2 Webhook 服务器配置"></a>1.2 Webhook 服务器配置</h3><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">webhook:</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">9443</span>  <span class="comment"># webhook 服务器监听端口</span></span><br><span class="line">  <span class="attr">timeoutSeconds:</span> <span class="number">10</span>  <span class="comment"># 超时时间</span></span><br></pre></td></tr></table></figure></div><h2 id="2-Webhook-规则配置"><a href="#2-Webhook-规则配置" class="headerlink" title="2. Webhook 规则配置"></a>2. Webhook 规则配置</h2><h3 id="2-1-Mutating-Webhook-配置"><a href="#2-1-Mutating-Webhook-配置" class="headerlink" title="2.1 Mutating Webhook 配置"></a>2.1 Mutating Webhook 配置</h3><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">admissionregistration.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">MutatingWebhookConfiguration</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kueue-mutating-webhook-configuration</span></span><br><span class="line"><span class="attr">webhooks:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mjob.kb.io</span></span><br><span class="line">  <span class="attr">clientConfig:</span></span><br><span class="line">    <span class="attr">service:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">kueue-webhook-service</span></span><br><span class="line">      <span class="attr">namespace:</span> <span class="string">kueue-system</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/mutate-batch-v1-job</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">"batch"</span>]</span><br><span class="line">    <span class="attr">apiVersions:</span> [<span class="string">"v1"</span>]</span><br><span class="line">    <span class="attr">operations:</span> [<span class="string">"CREATE"</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">"jobs"</span>]</span><br><span class="line">  <span class="attr">failurePolicy:</span> <span class="string">Fail</span></span><br><span class="line">  <span class="attr">timeoutSeconds:</span> <span class="number">10</span></span><br></pre></td></tr></table></figure></div><h3 id="2-2-Validating-Webhook-配置"><a href="#2-2-Validating-Webhook-配置" class="headerlink" title="2.2 Validating Webhook 配置"></a>2.2 Validating Webhook 配置</h3><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">admissionregistration.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ValidatingWebhookConfiguration</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kueue-validating-webhook-configuration</span></span><br><span class="line"><span class="attr">webhooks:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">vresourceflavor.kb.io</span></span><br><span class="line">  <span class="attr">clientConfig:</span></span><br><span class="line">    <span class="attr">service:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">kueue-webhook-service</span></span><br><span class="line">      <span class="attr">namespace:</span> <span class="string">kueue-system</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/validate-kueue-x-k8s-io-v1beta1-resourceflavor</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">"kueue.x-k8s.io"</span>]</span><br><span class="line">    <span class="attr">apiVersions:</span> [<span class="string">"v1beta1"</span>]</span><br><span class="line">    <span class="attr">operations:</span> [<span class="string">"CREATE"</span>, <span class="string">"UPDATE"</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">"resourceflavors"</span>]</span><br></pre></td></tr></table></figure></div><h2 id="3-证书管理"><a href="#3-证书管理" class="headerlink" title="3. 证书管理"></a>3. 证书管理</h2><h3 id="3-1-TLS-证书配置"><a href="#3-1-TLS-证书配置" class="headerlink" title="3.1 TLS 证书配置"></a>3.1 TLS 证书配置</h3><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 证书存储在 Secret 中</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kueue-webhook-server-cert</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kueue-system</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">kubernetes.io/tls</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">tls.crt:</span> <span class="string">&lt;base64-encoded-cert&gt;</span></span><br><span class="line">  <span class="attr">tls.key:</span> <span class="string">&lt;base64-encoded-key&gt;</span></span><br></pre></td></tr></table></figure></div><h3 id="3-2-证书挂载"><a href="#3-2-证书挂载" class="headerlink" title="3.2 证书挂载"></a>3.2 证书挂载</h3><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 Deployment 中挂载证书</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">webhook-server-cert</span></span><br><span class="line">  <span class="attr">secret:</span></span><br><span class="line">    <span class="attr">secretName:</span> <span class="string">kueue-webhook-server-cert</span></span><br><span class="line"><span class="attr">volumeMounts:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">webhook-server-cert</span></span><br><span class="line">  <span class="attr">mountPath:</span> <span class="string">/tmp/certs</span></span><br><span class="line">  <span class="attr">readOnly:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></div><h1 id="🚨常见问题及解决方案"><a href="#🚨常见问题及解决方案" class="headerlink" title="🚨常见问题及解决方案"></a>🚨常见问题及解决方案</h1><h2 id="问题1：Webhook-连接被拒绝"><a href="#问题1：Webhook-连接被拒绝" class="headerlink" title="问题1：Webhook 连接被拒绝"></a>问题1：Webhook 连接被拒绝</h2><h3 id="错误现象"><a href="#错误现象" class="headerlink" title="错误现象"></a>错误现象</h3><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">failed calling webhook "mresourceflavor.kb.io": failed to call webhook: </span><br><span class="line">Post "https://kueue-webhook-service.kueue-system.svc:443/mutate-kueue-x-k8s-io-v1beta1-resourceflavor?timeout=10s": </span><br><span class="line">dial tcp 10.96.33.70:443: connect: connection refused</span><br></pre></td></tr></table></figure></div><h3 id="可能原因"><a href="#可能原因" class="headerlink" title="可能原因"></a>可能原因</h3><ol><li><strong>Webhook 服务未启动</strong></li><li><strong>Webhook Pod 未就绪</strong></li><li><strong>证书未正确生成</strong></li><li><strong>服务端点未配置</strong></li></ol><h3 id="诊断步骤"><a href="#诊断步骤" class="headerlink" title="诊断步骤"></a>诊断步骤</h3><h4 id="步骤1：检查-Webhook-Pod-状态"><a href="#步骤1：检查-Webhook-Pod-状态" class="headerlink" title="步骤1：检查 Webhook Pod 状态"></a>步骤1：检查 Webhook Pod 状态</h4><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查 Pod 是否运行</span></span><br><span class="line">kubectl get pods -n kueue-system -l control-plane=controller-manager</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 Pod 详细信息</span></span><br><span class="line">kubectl describe pod -n kueue-system -l control-plane=controller-manager</span><br></pre></td></tr></table></figure></div><h4 id="步骤2：检查-Webhook-服务"><a href="#步骤2：检查-Webhook-服务" class="headerlink" title="步骤2：检查 Webhook 服务"></a>步骤2：检查 Webhook 服务</h4><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查服务是否存在</span></span><br><span class="line">kubectl get service kueue-webhook-service -n kueue-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看服务详细信息</span></span><br><span class="line">kubectl describe service kueue-webhook-service -n kueue-system</span><br></pre></td></tr></table></figure></div><h4 id="步骤3：检查服务端点"><a href="#步骤3：检查服务端点" class="headerlink" title="步骤3：检查服务端点"></a>步骤3：检查服务端点</h4><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查端点是否配置</span></span><br><span class="line">kubectl get endpoints kueue-webhook-service -n kueue-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看端点详细信息</span></span><br><span class="line">kubectl describe endpoints kueue-webhook-service -n kueue-system</span><br></pre></td></tr></table></figure></div><h4 id="步骤4：检查证书"><a href="#步骤4：检查证书" class="headerlink" title="步骤4：检查证书"></a>步骤4：检查证书</h4><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查证书 Secret 是否存在</span></span><br><span class="line">kubectl get secret kueue-webhook-server-cert -n kueue-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看证书详细信息</span></span><br><span class="line">kubectl describe secret kueue-webhook-server-cert -n kueue-system</span><br></pre></td></tr></table></figure></div><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="方案1：等待服务就绪"><a href="#方案1：等待服务就绪" class="headerlink" title="方案1：等待服务就绪"></a>方案1：等待服务就绪</h4><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 等待 Pod 就绪</span></span><br><span class="line">kubectl <span class="built_in">wait</span> --<span class="keyword">for</span>=condition=ready pod -l control-plane=controller-manager -n kueue-system --<span class="built_in">timeout</span>=120s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待服务端点就绪</span></span><br><span class="line">kubectl <span class="built_in">wait</span> --<span class="keyword">for</span>=condition=ready endpoints kueue-webhook-service -n kueue-system --<span class="built_in">timeout</span>=60s</span><br></pre></td></tr></table></figure></div><h4 id="方案2：重新生成证书"><a href="#方案2：重新生成证书" class="headerlink" title="方案2：重新生成证书"></a>方案2：重新生成证书</h4><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除现有证书</span></span><br><span class="line">kubectl delete secret kueue-webhook-server-cert -n kueue-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启 Kueue 控制器</span></span><br><span class="line">kubectl rollout restart deployment kueue-controller-manager -n kueue-system</span><br></pre></td></tr></table></figure></div><h4 id="方案3：检查网络连接"><a href="#方案3：检查网络连接" class="headerlink" title="方案3：检查网络连接"></a>方案3：检查网络连接</h4><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试服务可访问性</span></span><br><span class="line">kubectl run test-webhook --image=busybox --<span class="built_in">rm</span> -it --restart=Never -- \</span><br><span class="line">  wget -qO- --no-check-certificate https://kueue-webhook-service.kueue-system.svc:443/healthz</span><br></pre></td></tr></table></figure></div><h2 id="问题2：Webhook-超时"><a href="#问题2：Webhook-超时" class="headerlink" title="问题2：Webhook 超时"></a>问题2：Webhook 超时</h2><h3 id="错误现象-1"><a href="#错误现象-1" class="headerlink" title="错误现象"></a>错误现象</h3><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">failed calling webhook: timeout=10s</span><br></pre></td></tr></table></figure></div><h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="方案1：增加超时时间"><a href="#方案1：增加超时时间" class="headerlink" title="方案1：增加超时时间"></a>方案1：增加超时时间</h4><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改 webhook 配置</span></span><br><span class="line"><span class="attr">webhooks:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mjob.kb.io</span></span><br><span class="line">  <span class="attr">timeoutSeconds:</span> <span class="number">30</span>  <span class="comment"># 增加超时时间</span></span><br></pre></td></tr></table></figure></div><h4 id="方案2：优化-Webhook-性能"><a href="#方案2：优化-Webhook-性能" class="headerlink" title="方案2：优化 Webhook 性能"></a>方案2：优化 Webhook 性能</h4><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加资源限制</span></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line">  <span class="attr">requests:</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">512Mi</span></span><br><span class="line">  <span class="attr">limits:</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">1000m</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure></div><h4 id="方案3：调整并发处理"><a href="#方案3：调整并发处理" class="headerlink" title="方案3：调整并发处理"></a>方案3：调整并发处理</h4><div class="code-container" data-rel="Yaml"><figure class="iseeu highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加并发处理能力</span></span><br><span class="line"><span class="attr">controller:</span></span><br><span class="line">  <span class="attr">groupKindConcurrency:</span></span><br><span class="line">    <span class="attr">Job.batch:</span> <span class="number">100</span></span><br><span class="line">    <span class="attr">Pod:</span> <span class="number">100</span></span><br><span class="line">    <span class="attr">Workload.kueue.x-k8s.io:</span> <span class="number">100</span></span><br></pre></td></tr></table></figure></div><h2 id="问题3：证书问题"><a href="#问题3：证书问题" class="headerlink" title="问题3：证书问题"></a>问题3：证书问题</h2><h3 id="错误现象-2"><a href="#错误现象-2" class="headerlink" title="错误现象"></a>错误现象</h3><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x509: certificate signed by unknown authority</span><br></pre></td></tr></table></figure></div><h3 id="解决方案-2"><a href="#解决方案-2" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="方案1：重新生成证书"><a href="#方案1：重新生成证书" class="headerlink" title="方案1：重新生成证书"></a>方案1：重新生成证书</h4><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除现有证书</span></span><br><span class="line">kubectl delete secret kueue-webhook-server-cert -n kueue-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启控制器</span></span><br><span class="line">kubectl rollout restart deployment kueue-controller-manager -n kueue-system</span><br></pre></td></tr></table></figure></div><h4 id="方案2：检查证书配置"><a href="#方案2：检查证书配置" class="headerlink" title="方案2：检查证书配置"></a>方案2：检查证书配置</h4><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 验证证书配置</span></span><br><span class="line">kubectl get secret kueue-webhook-server-cert -n kueue-system -o yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查证书内容</span></span><br><span class="line">kubectl get secret kueue-webhook-server-cert -n kueue-system -o jsonpath=<span class="string">'{.data.tls\.crt}'</span> | <span class="built_in">base64</span> -d | openssl x509 -text -noout</span><br></pre></td></tr></table></figure></div><h1 id="🔍调试方法"><a href="#🔍调试方法" class="headerlink" title="🔍调试方法"></a>🔍调试方法</h1><h2 id="1-日志分析"><a href="#1-日志分析" class="headerlink" title="1. 日志分析"></a>1. 日志分析</h2><h3 id="查看-Webhook-服务器日志"><a href="#查看-Webhook-服务器日志" class="headerlink" title="查看 Webhook 服务器日志"></a>查看 Webhook 服务器日志</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 webhook 服务器日志</span></span><br><span class="line">kubectl logs -n kueue-system -l control-plane=controller-manager -c manager</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实时跟踪日志</span></span><br><span class="line">kubectl logs -n kueue-system -l control-plane=controller-manager -c manager -f</span><br></pre></td></tr></table></figure></div><h3 id="查看-API-服务器日志"><a href="#查看-API-服务器日志" class="headerlink" title="查看 API 服务器日志"></a>查看 API 服务器日志</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 API 服务器日志</span></span><br><span class="line">kubectl logs -n kube-system kube-apiserver-kind-control-plane</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 API 服务器审计日志</span></span><br><span class="line">kubectl logs -n kube-system kube-apiserver-kind-control-plane | grep webhook</span><br></pre></td></tr></table></figure></div><h2 id="2-配置检查"><a href="#2-配置检查" class="headerlink" title="2. 配置检查"></a>2. 配置检查</h2><h3 id="检查-Webhook-配置"><a href="#检查-Webhook-配置" class="headerlink" title="检查 Webhook 配置"></a>检查 Webhook 配置</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 mutating webhook 配置</span></span><br><span class="line">kubectl get mutatingwebhookconfiguration kueue-mutating-webhook-configuration -o yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 validating webhook 配置</span></span><br><span class="line">kubectl get validatingwebhookconfiguration kueue-validating-webhook-configuration -o yaml</span><br></pre></td></tr></table></figure></div><h3 id="检查服务配置"><a href="#检查服务配置" class="headerlink" title="检查服务配置"></a>检查服务配置</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看服务配置</span></span><br><span class="line">kubectl get service kueue-webhook-service -n kueue-system -o yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看端点配置</span></span><br><span class="line">kubectl get endpoints kueue-webhook-service -n kueue-system -o yaml</span><br></pre></td></tr></table></figure></div><h2 id="3-网络测试"><a href="#3-网络测试" class="headerlink" title="3. 网络测试"></a>3. 网络测试</h2><h3 id="测试服务连通性"><a href="#测试服务连通性" class="headerlink" title="测试服务连通性"></a>测试服务连通性</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试服务可访问性</span></span><br><span class="line">kubectl run test-webhook --image=busybox --<span class="built_in">rm</span> -it --restart=Never -- \</span><br><span class="line">  wget -qO- --no-check-certificate https://kueue-webhook-service.kueue-system.svc:443/healthz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试端口连通性</span></span><br><span class="line">kubectl run test-port --image=busybox --<span class="built_in">rm</span> -it --restart=Never -- \</span><br><span class="line">  nc -zv kueue-webhook-service.kueue-system.svc 443</span><br></pre></td></tr></table></figure></div><h1 id="🏥实战总结"><a href="#🏥实战总结" class="headerlink" title="🏥实战总结"></a>🏥实战总结</h1><h2 id="关键要点"><a href="#关键要点" class="headerlink" title="关键要点"></a>关键要点</h2><ol><li><strong>Webhook 是 Kueue 的核心组件</strong>：负责准入控制、资源验证和自动标签管理</li><li><strong>证书管理至关重要</strong>：TLS 证书是 Webhook 安全通信的基础</li><li><strong>服务就绪检查</strong>：确保 Webhook 服务完全就绪是避免连接问题的关键</li></ol><h2 id="实践经验"><a href="#实践经验" class="headerlink" title="实践经验"></a>实践经验</h2><p>通常可考虑的故障排除流程如下：</p><ol><li><strong>检查 Pod 状态</strong>：确认 Webhook 服务正在运行</li><li><strong>验证服务配置</strong>：检查服务和端点配置</li><li><strong>检查证书</strong>：验证 TLS 证书是否正确</li><li><strong>测试连通性</strong>：确认网络连接正常</li><li><strong>查看日志</strong>：分析错误信息和性能指标</li></ol><p>希望本文档能够帮助您更好地理解和解决 Kubernetes 集群中的 Webhook 相关问题！</p><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://zh.wikipedia.org/wiki/%E7%BD%91%E7%BB%9C%E9%92%A9%E5%AD%90">[1] Webhook - Wikipedia<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/admission-controllers/">[2] Kubernetes 中的准入控制 - 官方文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://github.com/volcano-sh/volcano/tree/master/pkg/webhooks">[3] Volcano Webhook Implementation - GitHub<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/admission-webhooks-good-practices/">[4] Admission Webhook 良好实践 - Kubernetes官方文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/webhook/">[5] Webhook Mode - Kubernetes官方文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-apiserver/">[6] Kubernetes API Server 参数 - 官方文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://juejin.cn/post/7437727364082040871">[7] 什么是Webhook？工作原理？如何实现？缺点？ - 掘金<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://zhuanlan.zhihu.com/p/606844215">[8] 详细介绍一下webhook技术 - 知乎<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://blog.csdn.net/m0_71808387/article/details/140469408">[9] Webhook 是什么？详解其工作原理 - CSDN<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.redhat.com/zh-cn/topics/automation-and-management/shenmeshi-webhook">[10] 什么是 Webhook？ - RedHat<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.cnblogs.com/keep-live/articles/16544143.html">[11] kubernetes的webhook开发 - 博客园<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">本文基于调度器性能测试项目中的实际故障案例，深入解析 Kueue 调度器的 Webhook 机制。从理论到实践，从故障现象到解决方案，提供完整的 Webhook 准入控制实战指南，助您快速定位和解决 Kubernetes 集群中的 Webhook 相关问题。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    <category term="实战" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/%E5%AE%9E%E6%88%98/"/>
    
    
    <category term="Kubernetes" scheme="https://freshwlnd.github.io/tags/Kubernetes/"/>
    
    <category term="Webhook" scheme="https://freshwlnd.github.io/tags/Webhook/"/>
    
    <category term="准入控制" scheme="https://freshwlnd.github.io/tags/%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/"/>
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="性能优化" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    
    <category term="Kueue" scheme="https://freshwlnd.github.io/tags/Kueue/"/>
    
    <category term="故障排除" scheme="https://freshwlnd.github.io/tags/%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4/"/>
    
  </entry>
  
  <entry>
    <title>【集群】Kubernetes Webhook入门：准入控制机制与性能瓶颈分析</title>
    <link href="https://freshwlnd.github.io/2025/07/08/k8s/k8s-webhook/"/>
    <id>https://freshwlnd.github.io/2025/07/08/k8s/k8s-webhook/</id>
    <published>2025-07-08T01:24:17.000Z</published>
    <updated>2025-07-11T03:57:39.105Z</updated>
    
    <content type="html"><![CDATA[<!-- > 本系列《Kubernetes深度解析》计划分为以下几篇，点击查看其它内容。 --><!-- > 1. <a href="/2025/07/08/k8s/k8s-webhook/" title="Kubernetes Webhook入门：准入控制机制与性能瓶颈分析">Kubernetes Webhook入门：准入控制机制与性能瓶颈分析</a> --><!-- > 2. （待续）Kubernetes调度器性能优化实践 --><!-- > 3. （待续）大规模集群资源管理策略 --><!-- > 4. （待续）云原生架构设计最佳实践 --><h1 id="💡简介"><a href="#💡简介" class="headerlink" title="💡简介"></a>💡简介</h1><p>在<a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="调度器性能对比分析">调度器性能对比分析</a>中，我们发现Webhook可能在大规模情况下成为Volcano创建Job的限制因素。为了深入理解这一现象，本文将从Webhook的基本概念出发，系统梳理其在Kubernetes生态系统中的作用机制和性能影响。</p><h1 id="🖼️背景"><a href="#🖼️背景" class="headerlink" title="🖼️背景"></a>🖼️背景</h1><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>在调度器性能测试中，我们发现了一个有趣的现象：Volcano调度器在大量Job创建时会出现阶段性阻塞，其中一个可能的原因是Webhook QPS限制。这引发了我们对Webhook机制的深入思考：</p><h3 id="核心问题"><a href="#核心问题" class="headerlink" title="核心问题"></a>核心问题</h3><ol><li><strong>大背景</strong>：Webhook的起源是什么？为什么Kubernetes需要这种机制？</li><li><strong>小背景</strong>：Webhook在K8s/Volcano中是如何应用的？</li><li><strong>出发点</strong>：在K8s中应用Webhook时是否有什么参数存在限制，如跟<code>--kube-api-qps</code>有什么关系？为什么要设计这种限制？</li><li><strong>挑战</strong>：什么情况下会成为瓶颈限制性能？</li></ol><h1 id="🧠问题回答"><a href="#🧠问题回答" class="headerlink" title="🧠问题回答"></a>🧠问题回答</h1><h2 id="问题一：Webhook的起源是什么？"><a href="#问题一：Webhook的起源是什么？" class="headerlink" title="问题一：Webhook的起源是什么？"></a>问题一：Webhook的起源是什么？</h2><h3 id="Webhook概念的起源"><a href="#Webhook概念的起源" class="headerlink" title="Webhook概念的起源"></a>Webhook概念的起源</h3><p><strong>Webhook</strong>一词最早出现在2007年，由Jeff Lindsay提出<a href="#refer-anchor-1"><sup>[1]</sup></a>。它描述了一种”反向API调用”的机制，即服务器在特定事件发生时主动向客户端发送HTTP请求，而不是客户端主动轮询服务器。</p><h3 id="什么是Webhook？一个简单的比喻"><a href="#什么是Webhook？一个简单的比喻" class="headerlink" title="什么是Webhook？一个简单的比喻"></a>什么是Webhook？一个简单的比喻</h3><p>想象一下<strong>闹钟</strong>的工作原理<a href="#refer-anchor-7"><sup>[7]</sup></a>：</p><blockquote><p>你在手机上定了一个明天早上6点的闹钟（注册webhook），当时间来到第二天早上6点时，手机闹钟响起（触发webhook），你就会被叫醒（你的服务器收到通知并执行相应操作）。</p></blockquote><p><strong>Webhook</strong>就是这样一种”反向通知”机制：</p><ul><li><strong>传统方式</strong>：你每隔几分钟就问一次”有新消息吗？”（周期性拉取）</li><li><strong>Webhook方式</strong>：有新消息时，系统主动告诉你”有新消息了！”（触发式推送）</li></ul><h3 id="Webhook在不同场景下的应用"><a href="#Webhook在不同场景下的应用" class="headerlink" title="Webhook在不同场景下的应用"></a>Webhook在不同场景下的应用</h3><h4 id="1-传统应用场景（事件驱动）"><a href="#1-传统应用场景（事件驱动）" class="headerlink" title="1. 传统应用场景（事件驱动）"></a>1. 传统应用场景（事件驱动）</h4><ul><li><strong>钉钉机器人</strong>：当有重要事件发生时，钉钉主动向你的webhook地址发送消息</li><li><strong>支付系统</strong>：支付成功后，支付平台主动通知商家系统</li><li><strong>GitHub代码推送</strong>：代码推送后，GitHub主动通知CI/CD系统</li></ul><h4 id="2-Kubernetes应用场景（请求拦截）"><a href="#2-Kubernetes应用场景（请求拦截）" class="headerlink" title="2. Kubernetes应用场景（请求拦截）"></a>2. Kubernetes应用场景（请求拦截）</h4><ul><li><strong>准入控制</strong>：当用户创建Pod时，API服务器主动调用Webhook进行检查</li><li><strong>资源验证</strong>：验证Pod是否符合集群的安全策略</li><li><strong>资源修改</strong>：给Pod添加默认标签或注解</li></ul><h3 id="在Kubernetes中的应用"><a href="#在Kubernetes中的应用" class="headerlink" title="在Kubernetes中的应用"></a>在Kubernetes中的应用</h3><p>在Kubernetes中，Webhook被用作<strong>准入控制器（Admission Controller）</strong>的一种实现方式<a href="#refer-anchor-2"><sup>[2]</sup></a>。准入控制器是Kubernetes API服务器的一个插件机制，用于在资源创建、修改或删除之前进行拦截和处理。</p><h3 id="为什么需要Webhook？"><a href="#为什么需要Webhook？" class="headerlink" title="为什么需要Webhook？"></a>为什么需要Webhook？</h3><p>想象一下<strong>海关检查</strong>的工作<a href="#refer-anchor-8"><sup>[8]</sup></a>：</p><blockquote><p>当有货物要进入国家时，海关会检查：</p><ul><li>这个货物符合进口规定吗？（验证）</li><li>需要添加标签或修改包装吗？（修改）</li><li>有安全隐患吗？（安全验证）</li></ul></blockquote><p>Kubernetes的Webhook就像这个海关：</p><ol><li><strong>扩展性需求</strong>：Kubernetes需要支持各种自定义的验证和修改逻辑</li><li><strong>动态配置</strong>：Webhook可以在不重启API服务器的情况下动态添加新的验证规则</li><li><strong>外部集成</strong>：允许外部系统参与Kubernetes的资源管理决策</li><li><strong>安全增强</strong>：提供额外的安全验证层</li></ol><h3 id="为什么K8s下的Webhook和其他场景的Webhook看起来似乎不一样？"><a href="#为什么K8s下的Webhook和其他场景的Webhook看起来似乎不一样？" class="headerlink" title="为什么K8s下的Webhook和其他场景的Webhook看起来似乎不一样？"></a>为什么K8s下的Webhook和其他场景的Webhook看起来似乎不一样？</h3><p><strong>关键理解</strong>：Webhook的本质都是<strong>HTTP回调机制</strong>，区别在于<strong>触发时机</strong>和<strong>处理方式</strong>：</p><table><thead><tr><th>场景</th><th>触发时机</th><th>处理方式</th><th>目的</th></tr></thead><tbody><tr><td><strong>传统应用</strong></td><td>事件发生时</td><td>异步通知</td><td>推送信息</td></tr><tr><td><strong>Kubernetes</strong></td><td>请求到达时</td><td>同步拦截</td><td>验证/修改</td></tr></tbody></table><p><strong>相同点</strong>：</p><ul><li>都是基于HTTP的回调机制（触发式推送而非周期性拉取）</li><li>都是服务器主动调用外部服务</li><li>都支持自定义处理逻辑</li></ul><p><strong>不同点</strong>：</p><ul><li><strong>触发条件</strong>：事件驱动 vs 请求驱动</li><li><strong>处理方式</strong>：异步推送 vs 同步拦截</li><li><strong>响应要求</strong>：无响应要求 vs 必须返回结果</li></ul><h2 id="问题二：Webhook在K8s-Volcano中是如何应用的？"><a href="#问题二：Webhook在K8s-Volcano中是如何应用的？" class="headerlink" title="问题二：Webhook在K8s/Volcano中是如何应用的？"></a>问题二：Webhook在K8s/Volcano中是如何应用的？</h2><h3 id="Kubernetes中的Webhook类型"><a href="#Kubernetes中的Webhook类型" class="headerlink" title="Kubernetes中的Webhook类型"></a>Kubernetes中的Webhook类型</h3><h4 id="1-验证性Webhook（Validating-Webhook）"><a href="#1-验证性Webhook（Validating-Webhook）" class="headerlink" title="1. 验证性Webhook（Validating Webhook）"></a>1. 验证性Webhook（Validating Webhook）</h4><ul><li><strong>作用</strong>：验证资源是否符合特定规则</li><li><strong>时机</strong>：在资源被持久化到etcd之前</li><li><strong>结果</strong>：允许或拒绝请求</li></ul><h4 id="2-修改性Webhook（Mutating-Webhook）"><a href="#2-修改性Webhook（Mutating-Webhook）" class="headerlink" title="2. 修改性Webhook（Mutating Webhook）"></a>2. 修改性Webhook（Mutating Webhook）</h4><ul><li><strong>作用</strong>：修改资源内容</li><li><strong>时机</strong>：在验证性Webhook之前</li><li><strong>结果</strong>：返回修改后的资源</li></ul><h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户创建Pod → API服务器接收请求 → 修改性Webhook → 验证性Webhook → 持久化到etcd</span><br></pre></td></tr></table></figure></div><p><strong>具体例子</strong>：</p><ol><li>用户提交一个Pod创建请求</li><li>API服务器收到请求后，先调用修改性Webhook</li><li>修改性Webhook可能会给Pod添加默认标签</li><li>然后调用验证性Webhook检查Pod是否符合规则</li><li>如果验证通过，Pod被保存到etcd；如果失败，请求被拒绝</li></ol><h2 id="问题三：Webhook的QPS限制机制"><a href="#问题三：Webhook的QPS限制机制" class="headerlink" title="问题三：Webhook的QPS限制机制"></a>问题三：Webhook的QPS限制机制</h2><h3 id="QPS限制参数"><a href="#QPS限制参数" class="headerlink" title="QPS限制参数"></a>QPS限制参数</h3><h4 id="1-kube-api-qps"><a href="#1-kube-api-qps" class="headerlink" title="1. --kube-api-qps"></a>1. <code>--kube-api-qps</code></h4><ul><li><strong>含义</strong>：API服务器向Webhook服务发送请求的速率限制</li><li><strong>默认值</strong>：通常为50 QPS</li><li><strong>作用范围</strong>：所有Webhook请求的总和</li></ul><h4 id="2-kube-api-burst"><a href="#2-kube-api-burst" class="headerlink" title="2. --kube-api-burst"></a>2. <code>--kube-api-burst</code></h4><ul><li><strong>含义</strong>：突发请求的最大数量</li><li><strong>默认值</strong>：通常为100</li><li><strong>作用</strong>：允许短时间的突发流量</li></ul><h3 id="与-kube-api-qps的关系"><a href="#与-kube-api-qps的关系" class="headerlink" title="与--kube-api-qps的关系"></a>与<code>--kube-api-qps</code>的关系</h3><p><strong>关系说明</strong>：</p><ul><li><code>--kube-api-qps</code>控制API服务器向所有外部服务（包括Webhook）发送请求的速率</li><li>Webhook请求也受到这个限制的约束</li><li>当Webhook服务响应慢时，会占用更多的QPS配额</li></ul><h3 id="为什么设计这种限制？"><a href="#为什么设计这种限制？" class="headerlink" title="为什么设计这种限制？"></a>为什么设计这种限制？</h3><h4 id="1-保护API服务器"><a href="#1-保护API服务器" class="headerlink" title="1. 保护API服务器"></a>1. 保护API服务器</h4><ul><li>防止Webhook服务过载影响API服务器性能</li><li>避免资源耗尽导致系统崩溃</li></ul><h4 id="2-公平性保证"><a href="#2-公平性保证" class="headerlink" title="2. 公平性保证"></a>2. 公平性保证</h4><ul><li>确保不同Webhook服务获得公平的处理机会</li><li>防止单个Webhook占用过多资源</li></ul><h2 id="问题四：什么情况下会成为瓶颈？"><a href="#问题四：什么情况下会成为瓶颈？" class="headerlink" title="问题四：什么情况下会成为瓶颈？"></a>问题四：什么情况下会成为瓶颈？</h2><h3 id="瓶颈场景分析"><a href="#瓶颈场景分析" class="headerlink" title="瓶颈场景分析"></a>瓶颈场景分析</h3><h4 id="1-大规模Job创建"><a href="#1-大规模Job创建" class="headerlink" title="1. 大规模Job创建"></a>1. 大规模Job创建</h4><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">场景：同时创建1000个Job，每个Job包含100个Pod</span><br><span class="line">影响：需要调用100,000次Webhook</span><br><span class="line">瓶颈：Webhook QPS限制导致请求排队</span><br></pre></td></tr></table></figure></div><h4 id="2-Webhook服务响应慢"><a href="#2-Webhook服务响应慢" class="headerlink" title="2. Webhook服务响应慢"></a>2. Webhook服务响应慢</h4><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">场景：Webhook服务处理单个请求需要100ms</span><br><span class="line">影响：即使QPS限制为50，实际吞吐量可能只有10 QPS</span><br><span class="line">瓶颈：Webhook服务成为性能瓶颈</span><br></pre></td></tr></table></figure></div><h4 id="3-复杂的验证逻辑"><a href="#3-复杂的验证逻辑" class="headerlink" title="3. 复杂的验证逻辑"></a>3. 复杂的验证逻辑</h4><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">场景：Webhook需要查询外部数据库进行验证</span><br><span class="line">影响：每次验证都需要网络调用，增加延迟</span><br><span class="line">瓶颈：网络延迟和外部服务响应时间</span><br></pre></td></tr></table></figure></div><h3 id="生活中的类比"><a href="#生活中的类比" class="headerlink" title="生活中的类比"></a>生活中的类比</h3><p>想象一下<strong>银行柜台</strong>的场景<a href="#refer-anchor-9"><sup>[9]</sup></a>：</p><blockquote><p>银行有10个柜台，每个柜台每分钟最多处理2个客户（QPS限制）。</p><ul><li>正常情况下：客户排队，柜台按顺序处理</li><li>高峰期：大量客户同时到达，柜台处理不过来，排队时间变长</li><li>柜台效率低：即使有10个柜台，如果每个客户处理时间很长，整体处理能力也会下降</li></ul></blockquote><p>Webhook的瓶颈就像这个银行柜台：</p><ul><li><strong>QPS限制</strong>：就像柜台数量有限</li><li><strong>处理时间</strong>：就像每个客户的处理时间</li><li><strong>排队等待</strong>：就像客户在银行排队</li></ul><h1 id="🏥反思"><a href="#🏥反思" class="headerlink" title="🏥反思"></a>🏥反思</h1><p>目前仅有粗浅地了解，接下来会进一步开展后续研究，例如：</p><ol><li><p><strong>Webhook最佳实践</strong></p><ul><li>性能优化策略</li><li>错误处理机制</li><li>监控和告警</li></ul></li><li><p><strong>调度器集成</strong></p><ul><li>Volcano Webhook实现细节</li><li>其他调度器的Webhook使用</li><li>性能对比分析</li></ul></li><li><p><strong>大规模集群优化</strong></p><ul><li>Webhook在高并发场景下的表现</li><li>瓶颈识别和解决方案</li><li>最佳配置参数</li></ul></li></ol><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://zh.wikipedia.org/wiki/%E7%BD%91%E7%BB%9C%E9%92%A9%E5%AD%90">[1] Webhook - Wikipedia<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/admission-controllers/">[2] Kubernetes 中的准入控制 - 官方文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://github.com/volcano-sh/volcano/tree/master/pkg/webhooks">[3] Volcano Webhook Implementation - GitHub<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/admission-webhooks-good-practices/">[4] Admission Webhook 良好实践 - Kubernetes官方文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/webhook/">[5] Webhook Mode - Kubernetes官方文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-apiserver/">[6] Kubernetes API Server 参数 - 官方文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://juejin.cn/post/7437727364082040871">[7] 什么是Webhook？工作原理？如何实现？缺点？ - 掘金<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://zhuanlan.zhihu.com/p/606844215">[8] 详细介绍一下webhook技术 - 知乎<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://blog.csdn.net/m0_71808387/article/details/140469408">[9] Webhook 是什么？详解其工作原理 - CSDN<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.redhat.com/zh-cn/topics/automation-and-management/shenmeshi-webhook">[10] 什么是 Webhook？ - RedHat<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.cnblogs.com/keep-live/articles/16544143.html">[11] kubernetes的webhook开发 - 博客园<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">本文初步解析Kubernetes Webhook的基本概念、工作原理和在调度器中的应用。从Webhook的起源出发，分析其在K8s中的应用场景，探讨QPS限制机制，为理解大规模集群中的性能瓶颈提供理论基础。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="Kubernetes" scheme="https://freshwlnd.github.io/tags/Kubernetes/"/>
    
    <category term="Webhook" scheme="https://freshwlnd.github.io/tags/Webhook/"/>
    
    <category term="准入控制" scheme="https://freshwlnd.github.io/tags/%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/"/>
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="性能优化" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>【操作系统】计算机硬件架构基础：CPU执行原理与架构演进</title>
    <link href="https://freshwlnd.github.io/2025/07/07/os/os-architecture/"/>
    <id>https://freshwlnd.github.io/2025/07/07/os/os-architecture/</id>
    <published>2025-07-07T00:48:11.000Z</published>
    <updated>2025-07-08T01:22:33.804Z</updated>
    
    <content type="html"><![CDATA[<!-- > 本系列《操作系统基础知识》计划分为以下几篇，点击查看其它内容。 --><!-- > 1. <a href="/2025/07/07/os/os-architecture/" title="计算机硬件架构基础：CPU执行原理与架构演进">计算机硬件架构基础：CPU执行原理与架构演进</a> --><!-- > 2. （待续）操作系统内存管理原理 --><!-- > 3. （待续）操作系统进程调度机制 --><!-- > 4. （待续）操作系统文件系统设计 --><h1 id="💡简介"><a href="#💡简介" class="headerlink" title="💡简介"></a>💡简介</h1><p>计算机硬件是操作系统运行的基础，理解硬件架构对于深入学习操作系统至关重要。本文基于小林coding的优质内容<a href="#refer-anchor-1"><sup>[1]</sup></a>，系统梳理计算机硬件的工作原理，重点解析CPU执行程序的机制、32/64位架构的区别，以及x86/x64/ARM64等主流架构的演进历程。</p><h1 id="🖼️背景"><a href="#🖼️背景" class="headerlink" title="🖼️背景"></a>🖼️背景</h1><h2 id="为什么需要理解硬件架构？"><a href="#为什么需要理解硬件架构？" class="headerlink" title="为什么需要理解硬件架构？"></a>为什么需要理解硬件架构？</h2><p>在深入学习操作系统之前，理解计算机硬件架构是必不可少的基础。操作系统作为硬件和软件之间的桥梁，其设计理念和实现机制都深深植根于底层硬件特性。</p><h3 id="核心问题"><a href="#核心问题" class="headerlink" title="核心问题"></a>核心问题</h3><ol><li><strong>CPU如何执行程序？</strong> - 理解指令执行的基本流程</li><li><strong>32位与64位架构的区别？</strong> - 掌握位宽对性能的影响</li><li><strong>不同架构的演进历程？</strong> - 了解技术发展的历史脉络</li></ol><h1 id="🧠问题回答"><a href="#🧠问题回答" class="headerlink" title="🧠问题回答"></a>🧠问题回答</h1><h2 id="问题一：CPU是如何执行程序的？"><a href="#问题一：CPU是如何执行程序的？" class="headerlink" title="问题一：CPU是如何执行程序的？"></a>问题一：CPU是如何执行程序的？</h2><h3 id="基本执行流程"><a href="#基本执行流程" class="headerlink" title="基本执行流程"></a>基本执行流程</h3><p>CPU执行程序的基本流程可以概括为：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">程序编译 → CPU读取指令 → 执行指令 → 跳转下一条指令</span><br></pre></td></tr></table></figure></div><ol><li><strong>程序编译</strong>：高级语言程序被编译成汇编代码，最终转换为机器指令</li><li><strong>指令读取</strong>：CPU从内存中读取指令到指令寄存器</li><li><strong>指令执行</strong>：CPU解析指令并执行相应的操作</li><li><strong>地址跳转</strong>：程序计数器更新，指向下一条指令</li></ol><h3 id="冯诺依曼架构"><a href="#冯诺依曼架构" class="headerlink" title="冯诺依曼架构"></a>冯诺依曼架构</h3><p>现代计算机都基于冯诺依曼架构，包含五个核心设备：</p><h4 id="CPU内部设备"><a href="#CPU内部设备" class="headerlink" title="CPU内部设备"></a>CPU内部设备</h4><ul><li><strong>运算器</strong>：执行算术和逻辑运算</li><li><strong>控制器</strong>：协调各组件工作，控制指令执行流程</li><li><strong>寄存器组</strong>：（为优化而生，不包含在五个核心组件之内）<ul><li><strong>通用寄存器</strong>：存储计算过程中的临时数据</li><li><strong>程序计数器（PC）</strong>：存储当前执行指令的内存地址</li><li><strong>指令寄存器（IR）</strong>：存储当前正在执行的指令</li></ul></li></ul><h4 id="外部设备"><a href="#外部设备" class="headerlink" title="外部设备"></a>外部设备</h4><ul><li><strong>存储器</strong>：存储程序和数据</li><li><strong>输入设备</strong>：接收外部数据</li><li><strong>输出设备</strong>：输出计算结果</li></ul><h4 id="总线连接（连接CPU内外部设备，不包含在五个核心设备之内）"><a href="#总线连接（连接CPU内外部设备，不包含在五个核心设备之内）" class="headerlink" title="总线连接（连接CPU内外部设备，不包含在五个核心设备之内）"></a>总线连接（连接CPU内外部设备，不包含在五个核心设备之内）</h4><ul><li><strong>地址总线</strong>：指定要访问的内存地址</li><li><strong>控制总线</strong>：传输控制信号</li><li><strong>数据总线</strong>：传输实际数据</li></ul><h3 id="关键技术细节"><a href="#关键技术细节" class="headerlink" title="关键技术细节"></a>关键技术细节</h3><h4 id="电压表示"><a href="#电压表示" class="headerlink" title="电压表示"></a>电压表示</h4><ul><li><strong>0和1的表示</strong>：通过低电压和高电压来表示二进制数据</li><li><strong>信号传输</strong>：总线上的电信号传输数字信息</li></ul><h4 id="总线带宽"><a href="#总线带宽" class="headerlink" title="总线带宽"></a>总线带宽</h4><ul><li><strong>地址总线带宽</strong>：决定可访问的内存地址范围</li><li><strong>数据总线带宽</strong>：决定一次传输的数据量</li><li><strong>带宽匹配</strong>：CPU位宽应与总线带宽匹配以获得最佳性能</li></ul><h4 id="位宽影响"><a href="#位宽影响" class="headerlink" title="位宽影响"></a>位宽影响</h4><ul><li><strong>32位CPU</strong>：理论上可访问4GB内存空间</li><li><strong>64位CPU</strong>：可访问巨大的内存空间（理论上限为2^64字节）</li></ul><h2 id="问题二：32位与64位架构的区别和优劣？"><a href="#问题二：32位与64位架构的区别和优劣？" class="headerlink" title="问题二：32位与64位架构的区别和优劣？"></a>问题二：32位与64位架构的区别和优劣？</h2><h3 id="CPU层面"><a href="#CPU层面" class="headerlink" title="CPU层面"></a>CPU层面</h3><h4 id="计算能力"><a href="#计算能力" class="headerlink" title="计算能力"></a>计算能力</h4><ul><li><strong>32位CPU</strong>：一次最多处理32位数据</li><li><strong>64位CPU</strong>：一次最多处理64位数据</li><li><strong>性能影响</strong>：对于32位以内的计算，两者性能相近；64位计算时，64位CPU有明显优势</li></ul><h4 id="内存寻址"><a href="#内存寻址" class="headerlink" title="内存寻址"></a>内存寻址</h4><ul><li><strong>32位限制</strong>：理论上最多访问4GB内存</li><li><strong>64位优势</strong>：可访问巨大的内存空间</li></ul><h3 id="软件层面"><a href="#软件层面" class="headerlink" title="软件层面"></a>软件层面</h3><h4 id="指令集差异"><a href="#指令集差异" class="headerlink" title="指令集差异"></a>指令集差异</h4><ul><li><strong>32位软件</strong>：使用32位指令集</li><li><strong>64位软件</strong>：使用64位指令集</li><li><strong>兼容性</strong>：64位CPU通常向下兼容32位软件</li></ul><h4 id="性能影响"><a href="#性能影响" class="headerlink" title="性能影响"></a>性能影响</h4><ul><li><strong>寄存器数量</strong>：64位架构通常有更多寄存器</li><li><strong>指令效率</strong>：64位指令可以处理更大数据块</li><li><strong>内存带宽</strong>：64位架构可以更高效地利用内存带宽</li></ul><h3 id="操作系统层面"><a href="#操作系统层面" class="headerlink" title="操作系统层面"></a>操作系统层面</h3><p>操作系统也是一种特殊的软件，其位宽决定了：</p><ul><li><strong>内存管理能力</strong>：64位系统可以管理更大内存</li><li><strong>进程地址空间</strong>：64位系统为每个进程提供更大地址空间</li><li><strong>系统调用接口</strong>：64位系统提供64位系统调用</li></ul><h2 id="问题三：主流处理器架构的演进历程"><a href="#问题三：主流处理器架构的演进历程" class="headerlink" title="问题三：主流处理器架构的演进历程"></a>问题三：主流处理器架构的演进历程</h2><h3 id="x86架构（Intel-AMD）"><a href="#x86架构（Intel-AMD）" class="headerlink" title="x86架构（Intel/AMD）"></a>x86架构（Intel/AMD）</h3><h4 id="历史发展"><a href="#历史发展" class="headerlink" title="历史发展"></a>历史发展</h4><ul><li><strong>起源</strong>：1978年Intel推出8086处理器，开创x86架构</li><li><strong>演进</strong>：从16位（8086）→32位（80386）→64位（x86-64）</li><li><strong>特点</strong>：复杂指令集（CISC），指令丰富但复杂</li></ul><h4 id="代表产品"><a href="#代表产品" class="headerlink" title="代表产品"></a>代表产品</h4><ul><li><strong>早期产品</strong>：Intel 8086、80286、80386、80486</li><li><strong>经典产品</strong>：Intel Pentium系列（奔腾）</li><li><strong>现代产品</strong>：<ul><li><strong>Intel</strong>：Core系列（i3/i5/i7/i9）、Xeon系列（服务器）</li><li><strong>AMD</strong>：Athlon系列、Phenom系列、Ryzen系列、EPYC系列</li></ul></li></ul><h3 id="x64架构（x86-64-AMD64）"><a href="#x64架构（x86-64-AMD64）" class="headerlink" title="x64架构（x86-64/AMD64）"></a>x64架构（x86-64/AMD64）</h3><h4 id="技术特点"><a href="#技术特点" class="headerlink" title="技术特点"></a>技术特点</h4><ul><li><strong>64位扩展</strong>：在x86基础上扩展64位能力</li><li><strong>向下兼容</strong>：完全兼容32位x86软件</li><li><strong>性能提升</strong>：更大的内存空间和更高的计算能力</li></ul><h4 id="代表产品-1"><a href="#代表产品-1" class="headerlink" title="代表产品"></a>代表产品</h4><ul><li><strong>Intel</strong>：Core系列（i3/i5/i7/i9）、Xeon系列</li><li><strong>AMD</strong>：Ryzen系列、EPYC系列</li></ul><h3 id="ARM64架构"><a href="#ARM64架构" class="headerlink" title="ARM64架构"></a>ARM64架构</h3><h4 id="技术特点-1"><a href="#技术特点-1" class="headerlink" title="技术特点"></a>技术特点</h4><ul><li><strong>精简指令集</strong>：RISC架构，指令简单高效</li><li><strong>低功耗设计</strong>：同等性能下功耗更低</li><li><strong>模块化设计</strong>：可根据需求定制处理器核心</li></ul><h4 id="代表产品-2"><a href="#代表产品-2" class="headerlink" title="代表产品"></a>代表产品</h4><ul><li><strong>移动设备</strong>：<ul><li><strong>Apple</strong>：A系列芯片（A14、A15、M1、M2等）</li><li><strong>Qualcomm</strong>：Snapdragon系列</li><li><strong>Huawei</strong>：Kirin系列</li><li><strong>Samsung</strong>：Exynos系列</li></ul></li><li><strong>服务器</strong>：Amazon Graviton、Ampere Altra等</li></ul><h3 id="架构对比总结"><a href="#架构对比总结" class="headerlink" title="架构对比总结"></a>架构对比总结</h3><table><thead><tr><th>特性</th><th>x86</th><th>x64</th><th>ARM64</th></tr></thead><tbody><tr><td>指令集</td><td>CISC</td><td>CISC</td><td>RISC</td></tr><tr><td>位宽</td><td>32位</td><td>64位</td><td>64位</td></tr><tr><td>功耗</td><td>较高</td><td>较高</td><td>较低</td></tr><tr><td>性能</td><td>中等</td><td>高</td><td>中等-高</td></tr><tr><td>应用场景</td><td>传统PC</td><td>主流计算</td><td>移动设备+新兴领域</td></tr></tbody></table><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://xiaolincoding.com/os/1_hardware/how_cpu_run.html">[1] 小林coding - 图解系统<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://blog.csdn.net/qq_41063141/article/details/131444672">[2] x86_64和ARM64的区别以及发展 - CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://blog.csdn.net/qq_24433609/article/details/125991550">[3] x86-64、amd64、arm、aarch64 都是些什么？ - CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.cnblogs.com/zhaoqingqing/p/13145115.html">[4] x86 x64 arm64的区别  - 博客园<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.intel.cn/content/www/cn/zh/processors/processor-numbers.html">[5] Intel处理器产品线 - Intel官网<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.amd.com/zh-cn/products/specifications.html">[6] AMD处理器产品线 - AMD官网<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://www.arm.com/zh-TW/products/silicon-ip-cpu">[7] ARM架构发展历程 - ARM官网<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">本文深入解析计算机硬件架构的基础知识，包括CPU执行程序的原理、32/64位架构的区别、x86/x64/ARM64架构的演进历程。基于小林coding的优质内容，系统梳理计算机硬件的工作原理和架构发展脉络。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="计算机基础" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="操作系统" scheme="https://freshwlnd.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="计算机架构" scheme="https://freshwlnd.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%9E%B6%E6%9E%84/"/>
    
    <category term="CPU" scheme="https://freshwlnd.github.io/tags/CPU/"/>
    
    <category term="硬件原理" scheme="https://freshwlnd.github.io/tags/%E7%A1%AC%E4%BB%B6%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</title>
    <link href="https://freshwlnd.github.io/2025/07/01/k8s/k8s-scheduler-performance-test-debug/"/>
    <id>https://freshwlnd.github.io/2025/07/01/k8s/k8s-scheduler-performance-test-debug/</id>
    <published>2025-07-01T03:17:44.000Z</published>
    <updated>2025-08-11T09:10:28.437Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li></ol></blockquote><h1 id="💡简介"><a href="#💡简介" class="headerlink" title="💡简介"></a>💡简介</h1><blockquote><p><strong>📖 文档定位</strong>：本文为 kube-scheduling-perf 项目的<strong>实际部署篇</strong>，与 <a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="理论介绍文档">理论介绍文档</a> 形成互补。理论介绍文档重点解析项目的架构设计和自动化原理，而本文则专注于解决实际部署过程中的各种技术难题。</p></blockquote><p><strong>适用场景</strong>：如果您已经阅读了理论介绍文档，并计划在实际环境中部署和使用 kube-scheduling-perf 工具，那么本文档将为您提供必要的技术支持和故障排除指南。</p><h1 id="🖼️背景"><a href="#🖼️背景" class="headerlink" title="🖼️背景"></a>🖼️背景</h1><h2 id="为什么需要这份注意事项文档？"><a href="#为什么需要这份注意事项文档？" class="headerlink" title="为什么需要这份注意事项文档？"></a>为什么需要这份注意事项文档？</h2><p>kube-scheduling-perf 项目虽然提供了完善的自动化测试框架，但在实际部署过程中，由于以下因素，用户往往会遇到各种技术障碍：</p><h3 id="1-环境差异"><a href="#1-环境差异" class="headerlink" title="1. 环境差异"></a>1. 环境差异</h3><ul><li><strong>网络环境</strong>：国内用户访问海外镜像仓库时经常遇到网络超时问题</li><li><strong>系统版本</strong>：不同Linux发行版和内核版本对Docker、Kubernetes的支持程度不同</li><li><strong>硬件配置</strong>：老旧服务器可能无法运行最新版本的容器和工具</li></ul><h3 id="2-权限和配置问题"><a href="#2-权限和配置问题" class="headerlink" title="2. 权限和配置问题"></a>2. 权限和配置问题</h3><ul><li><strong>用户权限</strong>：Docker容器运行时的用户权限配置不当</li><li><strong>目录权限</strong>：自动生成的目录和文件所有权问题</li><li><strong>系统配置</strong>：内核参数、cgroup配置等系统级设置</li></ul><h3 id="3-版本兼容性"><a href="#3-版本兼容性" class="headerlink" title="3. 版本兼容性"></a>3. 版本兼容性</h3><ul><li><strong>Go版本</strong>：不同Go版本对测试代码的兼容性差异</li><li><strong>Docker版本</strong>：容器运行时版本与Kubernetes版本的匹配问题</li><li><strong>Kubernetes版本</strong>：API版本变化导致的兼容性问题</li></ul><h2 id="文档价值"><a href="#文档价值" class="headerlink" title="文档价值"></a>文档价值</h2><p>本文档基于实际部署经验总结，提供了：</p><ul><li><strong>系统性的问题分类</strong>：将常见问题按类型进行归类</li><li><strong>详细的解决方案</strong>：每个问题都提供具体的解决步骤</li><li><strong>预防性建议</strong>：帮助用户提前避免可能遇到的问题</li><li><strong>故障排除指南</strong>：快速定位和解决部署过程中的技术难题</li></ul><p>希望通过本文档，帮助大家避免重复踩坑，提高部署效率，顺利运行测试工具～</p><h1 id="🔨注意事项"><a href="#🔨注意事项" class="headerlink" title="🔨注意事项"></a>🔨注意事项</h1><h2 id="注意1：加速镜像拉取"><a href="#注意1：加速镜像拉取" class="headerlink" title="注意1：加速镜像拉取"></a>注意1：加速镜像拉取</h2><p>在国内环境下需要使用CDN加速镜像拉取<a href="#refer-anchor-1"><sup>[2]</sup></a>。</p><h3 id="Go-相关包"><a href="#Go-相关包" class="headerlink" title="Go 相关包"></a>Go 相关包</h3><p>在<code>Makefile</code>文件中替换<code>GOPROXY ?= https://proxy.golang.org,direct</code>为<code>GOPROXY ?= https://mirrors.aliyun.com/goproxy/,direct</code>。</p><h3 id="Docker-相关包"><a href="#Docker-相关包" class="headerlink" title="Docker 相关包"></a>Docker 相关包</h3><p>在执行 <code>make</code> 命令时，由于需要从海外服务器拉取镜像，频繁出现超时错误：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Error response from daemon: Head "https://asia-east1-docker.pkg.dev/v2/k8s-artifacts-prod/images/kwok/kwok/manifests/v0.6.1": dial tcp 142.250.157.82:443: i/o timeout</span><br><span class="line">Error response from daemon: Get "https://registry.k8s.io/v2/": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br></pre></td></tr></table></figure></div><p><a class="link" href="https://github.com/DaoCloud/public-image-mirror">DaoCloud 镜像仓库<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>提供了非常方便的解决方案，感恩！使用 DaoCloud 镜像加速，只需要在镜像前加上前缀 <code>m.daocloud.io/</code>。</p><h4 id="修改内容"><a href="#修改内容" class="headerlink" title="修改内容"></a>修改内容</h4><p><strong>1. Makefile 配置</strong></p><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line">IMAGE_PREFIX ?= </span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改后  </span></span><br><span class="line">IMAGE_PREFIX ?= m.daocloud.io/</span><br></pre></td></tr></table></figure></div><p><strong>2. 脚本配置</strong><br>在 <code>hack/local-registry-with-load-images.sh</code> 中：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line">IMAGE_PREFIX=<span class="string">"<span class="variable">${IMAGE_PREFIX:-}</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改后</span></span><br><span class="line">IMAGE_PREFIX=<span class="string">"<span class="variable">${IMAGE_PREFIX:-m.daocloud.io/}</span>"</span></span><br></pre></td></tr></table></figure></div><h4 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h4><ul><li><strong>镜像处理流程</strong>：所有镜像通过 <code>hack/local-registry-with-load-images.sh</code> 脚本处理</li><li><strong>DaoCloud 加速</strong>：脚本会自动从 <code>m.daocloud.io/</code> 拉取镜像，然后推送到本地仓库 <code>localhost:5001/</code></li><li><strong>容器内 Docker</strong>：即使使用容器内的 Docker，也会通过 <code>IMAGE_PREFIX</code> 环境变量传递镜像前缀</li></ul><h2 id="注意2：内核版本适配"><a href="#注意2：内核版本适配" class="headerlink" title="注意2：内核版本适配"></a>注意2：内核版本适配</h2><h3 id="问题说明"><a href="#问题说明" class="headerlink" title="问题说明"></a>问题说明</h3><p>如之前<a href="/2025/05/19/k8s/k8s-kind-install/" title="KIND安装博客">KIND安装博客</a>所述，本人所使用服务器内核版本过低（3.10.0-1160.71.1.el7.x86_64），无法运行较高版本的Kubernetes和Kind，权衡之计是进行版本降级以解决兼容性问题。</p><ul><li>降级之后，仍然会收到报错：<code>✗ Preparing nodes 📦 ; Command Output: WARNING: Your kernel does not support cgroup namespaces.  Cgroup namespace setting discarded.</code><ul><li>具体分析后发现 Kind 自动添加 cgroupns 参数：从日志中可以看到，kind 在创建集群时自动添加了 –cgroupns=private 参数，这是较新版本 kind 的默认行为。</li><li>解决方案：修改 kind 配置，<code>./hack/kind-with-local-registry.sh</code>中，在<code>kind create ...</code>之前增加以下代码：<div class="code-container" data-rel="Sh"><figure class="iseeu highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment"># 新增以下代码：Disable cgroup namespaces for older kernels</span></span><br><span class="line"><span class="built_in">export</span> KIND_EXPERIMENTAL_DISABLE_CGROUP_NAMESPACES=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create kind cluster with containerd registry configuration</span></span><br><span class="line">kind create cluster --config <span class="string">"<span class="variable">${KIND_CONFIG:-}</span>"</span> --name <span class="string">"<span class="variable">${KIND_CLUSTER_NAME:-kind}</span>"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure></div></li></ul></li></ul><h3 id="版本降级目标"><a href="#版本降级目标" class="headerlink" title="版本降级目标"></a>版本降级目标</h3><ul><li><strong>Go版本</strong>: 1.24 → 1.23.10</li><li><strong>Kind版本</strong>: v0.27.0 → v0.19.0</li><li><strong>Kubernetes版本</strong>: v1.25.3 → v1.27.1</li></ul><h3 id="修改列表"><a href="#修改列表" class="headerlink" title="修改列表"></a>修改列表</h3><h4 id="Go"><a href="#Go" class="headerlink" title="Go"></a>Go</h4><ul><li><code>go.mod</code>中：<code>go 1.24</code> → <code>go 1.23.10</code>。</li><li><code>Makefile</code>中：<code>GO_IMAGE ?= $(IMAGE_PREFIX)docker.io/library/golang:1.24</code> → <code>GO_IMAGE ?= $(IMAGE_PREFIX)docker.io/library/golang:1.23.10</code></li></ul><h4 id="Kind"><a href="#Kind" class="headerlink" title="Kind"></a>Kind</h4><ul><li><code>go.mod</code>中：<code>sigs.k8s.io/kind v0.27.0</code> → <code>sigs.k8s.io/kind v0.19.0</code>。</li></ul><h4 id="节点Kubernetes："><a href="#节点Kubernetes：" class="headerlink" title="节点Kubernetes："></a>节点Kubernetes：</h4><ul><li><code>go.mod</code>中：修改k8s配置 <code>v0.32.1</code> → <code>v0.27.1</code><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">k8s.io/api v0<span class="number">.27</span><span class="number">.1</span></span><br><span class="line">k8s.io/apimachinery v0<span class="number">.27</span><span class="number">.1</span></span><br><span class="line">k8s.io/apiextensions-apiserver v0<span class="number">.27</span><span class="number">.1</span> <span class="comment">// indirect</span></span><br><span class="line">k8s.io/client-<span class="keyword">go</span> v0<span class="number">.27</span><span class="number">.1</span> <span class="comment">// indirect</span></span><br><span class="line">k8s.io/component-base v0<span class="number">.27</span><span class="number">.1</span> <span class="comment">// indirect</span></span><br></pre></td></tr></table></figure></div></li><li>在<code>./cluster</code>目录下的<code>kueue</code>、<code>volcano</code>、<code>yunikorn</code>、<code>overview</code>四个目录中修改<code>kind.yaml</code>文件：<code>docker.io/kindest/node:v1.32.2</code> → <code>docker.io/kindest/node:v1.27.1</code>；</li></ul><h4 id="go-sum-版本管理文件-3"><a href="#go-sum-版本管理文件-3" class="headerlink" title="go.sum 版本管理文件[3]"></a>go.sum 版本管理文件<a href="#refer-anchor-1"><sup>[3]</sup></a></h4><ul><li>修改<code>go.mod</code>后，需要删除<code>go.sum</code>并执行<code>go mod tidy</code>以重新生成<code>go.sum</code>以匹配新的依赖版本。</li><li>必要时开启CDN镜像加速<code>export GOPROXY=https://mirrors.aliyun.com/goproxy/,direct</code>。</li><li>如果一直出现奇怪的错误，例如<code>go: github.com/wzshiming/kube-scheduling-perf/gopath/pkg/mod/github.com/pkg/errors@v0.9.1: import path "github.com/wzshiming/kube-scheduling-perf/gopath/pkg/mod/github.com/pkg/errors@v0.9.1" should not have @version</code>，可能是因为已安装的旧版本未删除，应该删除旧的 gopath 并重新构建，以确保参数生效：  <div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">rm</span> -rf gopath</span><br><span class="line"> make</span><br></pre></td></tr></table></figure></div></li></ul><h4 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h4><ul><li>之后<code>./hack/local-registry-with-load-images.sh</code>会自动提前拉取镜像。</li><li>注意：如果你是通过 Makefile 自动构建 bin/kind，请务必删除旧的 bin/kind 并重新构建，以确保新参数生效：  <div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">rm</span> -f bin/kind</span><br><span class="line">   make bin/kind</span><br></pre></td></tr></table></figure></div></li><li>注意：同理，如果修改版本前已经下载了相关go包，也应该删除旧的 gopath 并重新构建，以确保参数生效：  <div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">rm</span> -rf gopath</span><br><span class="line">make</span><br></pre></td></tr></table></figure></div></li></ul><h2 id="注意3：Go-版本兼容性问题"><a href="#注意3：Go-版本兼容性问题" class="headerlink" title="注意3：Go 版本兼容性问题"></a>注意3：Go 版本兼容性问题</h2><h3 id="问题说明-1"><a href="#问题说明-1" class="headerlink" title="问题说明"></a>问题说明</h3><p>降级 Go 版本后，测试代码中出现编译错误：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test/yunikorn/batch_job_test.go:10:29: t.Context undefined (type *"testing".T has no field or method Context, but does have unexported field context)</span><br></pre></td></tr></table></figure></div><h3 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h3><p><code>t.Context()</code> 方法在 Go 1.23.10 中可能不被完全支持或存在兼容性问题，导致测试代码无法编译。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>将所有测试文件中的 <code>t.Context()</code> 替换为 <code>context.Background()</code>。</p><h4 id="修改文件"><a href="#修改文件" class="headerlink" title="修改文件"></a>修改文件</h4><p><strong>1. test/yunikorn/batch_job_test.go</strong></p><div class="code-container" data-rel="Go"><figure class="iseeu highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改前</span></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"testing"</span></span><br><span class="line"><span class="string">"github.com/wzshiming/kube-scheduling-perf/test/utils"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestInit</span><span class="params">(t *testing.T)</span></span> {</span><br><span class="line">err := provider.AddNodes(t.Context())</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 修改后</span></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"context"</span></span><br><span class="line"><span class="string">"testing"</span></span><br><span class="line"><span class="string">"github.com/wzshiming/kube-scheduling-perf/test/utils"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestInit</span><span class="params">(t *testing.T)</span></span> {</span><br><span class="line">err := provider.AddNodes(context.Background())</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">}</span><br></pre></td></tr></table></figure></div><p><strong>2. test/volcano/batch_job_test.go</strong></p><ul><li>同样添加 <code>"context"</code> 导入</li><li>将所有 <code>t.Context()</code> 替换为 <code>context.Background()</code></li></ul><p><strong>3. test/kueue/batch_job_test.go</strong></p><ul><li>同样添加 <code>"context"</code> 导入</li><li>将所有 <code>t.Context()</code> 替换为 <code>context.Background()</code></li></ul><h2 id="注意4：Docker-容器权限问题"><a href="#注意4：Docker-容器权限问题" class="headerlink" title="注意4：Docker 容器权限问题"></a>注意4：Docker 容器权限问题</h2><h3 id="问题说明-2"><a href="#问题说明-2" class="headerlink" title="问题说明"></a>问题说明</h3><p>执行 <code>make</code> 命令时出现权限错误：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv: 无法将"./logs" 移动至"./tmp/logs": 权限不够</span><br></pre></td></tr></table></figure></div><p>检查发现多个目录（<code>./logs</code>、<code>./bin</code>、<code>./gopath</code>、<code>./registry-data</code>）的归属者是 <code>root</code>，而不是当前用户（当使用非 root 用户时）。</p><h3 id="原因分析-1"><a href="#原因分析-1" class="headerlink" title="原因分析"></a>原因分析</h3><p>Makefile 中的 <code>GO_IN_DOCKER</code> 命令使用 Docker 容器执行，容器内进程默认以 root 用户运行，导致创建的文件/目录归属 root。</p><h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="1-修改-Makefile，让容器以当前用户身份运行"><a href="#1-修改-Makefile，让容器以当前用户身份运行" class="headerlink" title="1. 修改 Makefile，让容器以当前用户身份运行"></a>1. 修改 Makefile，让容器以当前用户身份运行</h4><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line">GO_IN_DOCKER = docker run --rm --network host \</span><br><span class="line">-v <span class="variable">$(<span class="built_in">shell</span> pwd)</span>:/workspace/ -w /workspace/ \</span><br><span class="line">-e GOOS=<span class="variable">$(GOOS)</span> -e CGO_ENABLED=0 -e GOPATH=/workspace/gopath/ -e GOPROXY=<span class="variable">$(GOPROXY)</span> <span class="variable">$(GO_IMAGE)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改后</span></span><br><span class="line">GO_IN_DOCKER = docker run --rm --network host \</span><br><span class="line">-u <span class="variable">$(<span class="built_in">shell</span> id -u)</span>:<span class="variable">$(<span class="built_in">shell</span> id -g)</span> \</span><br><span class="line">-v <span class="variable">$(<span class="built_in">shell</span> pwd)</span>:/workspace/ -w /workspace/ \</span><br><span class="line">-e GOOS=<span class="variable">$(GOOS)</span> -e CGO_ENABLED=0 -e GOPATH=/workspace/gopath/ -e GOPROXY=<span class="variable">$(GOPROXY)</span> <span class="variable">$(GO_IMAGE)</span></span><br></pre></td></tr></table></figure></div><h4 id="2-创建目录权限修复脚本"><a href="#2-创建目录权限修复脚本" class="headerlink" title="2. 创建目录权限修复脚本"></a>2. 创建目录权限修复脚本</h4><p><strong>hack/ensure-directories.sh</strong></p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="built_in">set</span> -o errexit</span><br><span class="line"><span class="built_in">set</span> -o nounset</span><br><span class="line"><span class="built_in">set</span> -o pipefail</span><br><span class="line"></span><br><span class="line">DIR=<span class="string">"<span class="subst">$(dirname <span class="string">"<span class="variable">${BASH_SOURCE[0]}</span>"</span>)</span>"</span></span><br><span class="line">ROOT_DIR=<span class="string">"<span class="subst">$(realpath <span class="string">"<span class="variable">${DIR}</span>/.."</span>)</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Function to ensure directory has correct ownership</span></span><br><span class="line"><span class="function"><span class="title">ensure_directory</span></span>() {</span><br><span class="line">    <span class="built_in">local</span> <span class="built_in">dir</span>=<span class="string">"<span class="variable">$1</span>"</span></span><br><span class="line">    <span class="built_in">local</span> owner=$(<span class="built_in">stat</span> -c <span class="string">'%U'</span> <span class="string">"<span class="variable">$dir</span>"</span> 2&gt;/dev/null || <span class="built_in">echo</span> <span class="string">"none"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$owner</span>"</span> == <span class="string">"root"</span> ]]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Removing root-owned directory: <span class="variable">$dir</span>"</span></span><br><span class="line">        <span class="built_in">sudo</span> <span class="built_in">rm</span> -rf <span class="string">"<span class="variable">$dir</span>"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">mkdir</span> -p <span class="string">"<span class="variable">$dir</span>"</span></span><br><span class="line">    <span class="built_in">chmod</span> 755 <span class="string">"<span class="variable">$dir</span>"</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"Created/updated directory: <span class="variable">$dir</span>"</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create all necessary directories</span></span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> logs bin gopath registry-data output results tmp; <span class="keyword">do</span></span><br><span class="line">    ensure_directory <span class="string">"<span class="variable">${ROOT_DIR}</span>/<span class="variable">${d}</span>"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></div><h4 id="3-在-Makefile-中集成权限修复"><a href="#3-在-Makefile-中集成权限修复" class="headerlink" title="3. 在 Makefile 中集成权限修复"></a>3. 在 Makefile 中集成权限修复</h4><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: ensure-directories</span></span><br><span class="line"><span class="section">ensure-directories:</span></span><br><span class="line">./hack/ensure-directories.sh</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: default</span></span><br><span class="line"><span class="section">default: ensure-directories</span></span><br><span class="line"><span class="comment"># ... existing content ...</span></span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: serial-test</span></span><br><span class="line"><span class="section">serial-test: ensure-directories bin/kind</span></span><br><span class="line"><span class="comment"># ... existing content ...</span></span><br></pre></td></tr></table></figure></div><h4 id="4-修改镜像处理脚本"><a href="#4-修改镜像处理脚本" class="headerlink" title="4. 修改镜像处理脚本"></a>4. 修改镜像处理脚本</h4><p>在 <code>hack/local-registry-with-load-images.sh</code> 中确保 registry-data 目录正确创建：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ensure registry-data directory exists with correct permissions</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="string">"<span class="variable">${ROOT_DIR}</span>/registry-data"</span></span><br></pre></td></tr></table></figure></div><h3 id="验证方法"><a href="#验证方法" class="headerlink" title="验证方法"></a>验证方法</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行权限修复脚本</span></span><br><span class="line">./hack/ensure-directories.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查目录权限</span></span><br><span class="line"><span class="built_in">ls</span> -la | grep -E <span class="string">"(logs|bin|gopath|registry-data)"</span></span><br></pre></td></tr></table></figure></div><h2 id="注意5：Go-构建缓存权限问题"><a href="#注意5：Go-构建缓存权限问题" class="headerlink" title="注意5：Go 构建缓存权限问题"></a>注意5：Go 构建缓存权限问题</h2><h3 id="问题说明-3"><a href="#问题说明-3" class="headerlink" title="问题说明"></a>问题说明</h3><p>在执行 <code>make</code> 命令时出现 Go 构建缓存权限错误：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">failed to initialize build cache at /.cache/go-build: mkdir /.cache: permission denied</span><br></pre></td></tr></table></figure></div><h3 id="原因分析-2"><a href="#原因分析-2" class="headerlink" title="原因分析"></a>原因分析</h3><ol><li><strong>Docker 容器用户权限</strong>：当使用 <code>-u $(shell id -u):$(shell id -g)</code> 让容器以当前用户身份运行时，容器内的 <code>$HOME</code> 变量变为 <code>/</code>（因为没有为普通用户设置 home 目录）</li><li><strong>Go 默认行为</strong>：Go 在没有明确设置 <code>GOCACHE</code> 环境变量时，会尝试在 <code>$HOME/.cache/go-build</code> 或 <code>/.cache/go-build</code> 下创建构建缓存</li><li><strong>权限冲突</strong>：普通用户没有权限在容器根目录 <code>/</code> 下创建 <code>.cache</code> 目录，导致权限被拒绝</li></ol><h3 id="解决方案-2"><a href="#解决方案-2" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="1-设置-Go-构建缓存目录"><a href="#1-设置-Go-构建缓存目录" class="headerlink" title="1. 设置 Go 构建缓存目录"></a>1. 设置 Go 构建缓存目录</h4><p>在 Makefile 的 <code>GO_IN_DOCKER</code> 命令中添加 <code>GOCACHE</code> 环境变量：</p><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line">GO_IN_DOCKER = docker run --rm --network host \</span><br><span class="line">-u <span class="variable">$(<span class="built_in">shell</span> id -u)</span>:<span class="variable">$(<span class="built_in">shell</span> id -g)</span> \</span><br><span class="line">-v <span class="variable">$(<span class="built_in">shell</span> pwd)</span>:/workspace/ -w /workspace/ \</span><br><span class="line">-e GOOS=<span class="variable">$(GOOS)</span> -e CGO_ENABLED=0 -e GOPATH=/workspace/gopath/ -e GOPROXY=<span class="variable">$(GOPROXY)</span> <span class="variable">$(GO_IMAGE)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改后</span></span><br><span class="line">GO_IN_DOCKER = docker run --rm --network host \</span><br><span class="line">-u <span class="variable">$(<span class="built_in">shell</span> id -u)</span>:<span class="variable">$(<span class="built_in">shell</span> id -g)</span> \</span><br><span class="line">-v <span class="variable">$(<span class="built_in">shell</span> pwd)</span>:/workspace/ -w /workspace/ \</span><br><span class="line">-e GOOS=<span class="variable">$(GOOS)</span> -e CGO_ENABLED=0 -e GOPATH=/workspace/gopath/ -e GOPROXY=<span class="variable">$(GOPROXY)</span> -e GOCACHE=/workspace/.cache <span class="variable">$(GO_IMAGE)</span></span><br></pre></td></tr></table></figure></div><h4 id="2-设计原理"><a href="#2-设计原理" class="headerlink" title="2. 设计原理"></a>2. 设计原理</h4><ul><li><strong>权限一致性</strong>：通过设置 <code>GOCACHE=/workspace/.cache</code>，确保 Go 构建缓存在挂载的工作目录下创建，当前用户有完全权限</li><li><strong>兼容性</strong>：这个修改不会影响其他构建逻辑，只是改变了缓存存储位置</li><li><strong>最佳实践</strong>：这是使用非 root 用户运行 Docker 容器时的标准做法</li></ul><h3 id="验证方法-1"><a href="#验证方法-1" class="headerlink" title="验证方法"></a>验证方法</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试 Go 构建是否正常</span></span><br><span class="line">make bin/test-kueue</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查缓存目录是否创建</span></span><br><span class="line"><span class="built_in">ls</span> -la .cache/</span><br></pre></td></tr></table></figure></div><h3 id="预期效果"><a href="#预期效果" class="headerlink" title="预期效果"></a>预期效果</h3><p>修改后：</p><ol><li>Go 构建缓存会在项目根目录的 <code>.cache</code> 文件夹下创建</li><li>缓存目录属于当前用户，权限正确</li><li>不再出现 <code>permission denied</code> 错误</li><li>构建过程正常进行</li></ol><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ul><li>这个问题<strong>只有在使用 <code>-u</code> 参数让容器以非 root 用户运行时才会出现</strong></li><li>如果使用默认的 root 用户运行容器，不会有此问题，但会导致生成的文件归 root 所有</li><li>设置 <code>GOCACHE</code> 是使用非 root 用户运行 Go 容器的标准做法</li></ul><h2 id="注意6：Kueue-Webhook-连接问题"><a href="#注意6：Kueue-Webhook-连接问题" class="headerlink" title="注意6：Kueue Webhook 连接问题"></a>注意6：Kueue Webhook 连接问题</h2><h3 id="问题说明-4"><a href="#问题说明-4" class="headerlink" title="问题说明"></a>问题说明</h3><p>在执行 Kueue 测试时，经常出现 webhook 连接被拒绝的错误：</p><div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Internal error occurred: failed calling webhook "mresourceflavor.kb.io": failed to call webhook: </span><br><span class="line">Post "https://kueue-webhook-service.kueue-system.svc:443/mutate-kueue-x-k8s-io-v1beta1-resourceflavor?timeout=10s": </span><br><span class="line">dial tcp 10.96.33.70:443: connect: connection refused</span><br></pre></td></tr></table></figure></div><h3 id="原因分析-3"><a href="#原因分析-3" class="headerlink" title="原因分析"></a>原因分析</h3><ol><li><strong>Webhook 服务未就绪</strong>：Kueue 的 webhook 服务在部署后需要时间启动和初始化</li><li><strong>证书生成延迟</strong>：webhook 服务器需要生成 TLS 证书，这个过程可能需要几秒钟</li><li><strong>服务端点未配置</strong>：webhook 服务的端点（endpoints）可能还未正确配置</li><li><strong>缺少等待机制</strong>：原有的部署流程没有等待 webhook 服务完全就绪</li></ol><h3 id="解决方案-3"><a href="#解决方案-3" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="1-创建-Webhook-等待脚本"><a href="#1-创建-Webhook-等待脚本" class="headerlink" title="1. 创建 Webhook 等待脚本"></a>1. 创建 Webhook 等待脚本</h4><p><strong>hack/wait-for-webhook.sh</strong></p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="built_in">set</span> -o errexit</span><br><span class="line"><span class="built_in">set</span> -o nounset</span><br><span class="line"><span class="built_in">set</span> -o pipefail</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取脚本参数</span></span><br><span class="line">SCHEDULER_NAME=<span class="string">"<span class="variable">${1:-}</span>"</span></span><br><span class="line"><span class="keyword">if</span> [[ -z <span class="string">"<span class="variable">$SCHEDULER_NAME</span>"</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"Usage: <span class="variable">$0</span> &lt;scheduler-name&gt;"</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"Example: <span class="variable">$0</span> kueue"</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置变量</span></span><br><span class="line">KUBECONFIG=<span class="string">"<span class="variable">${KUBECONFIG:-./kubeconfig.yaml}</span>"</span></span><br><span class="line">NAMESPACE=<span class="string">"<span class="variable">${SCHEDULER_NAME}</span>-system"</span></span><br><span class="line">WEBHOOK_SERVICE=<span class="string">"<span class="variable">${SCHEDULER_NAME}</span>-webhook-service"</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"等待 <span class="variable">${SCHEDULER_NAME}</span> webhook 服务就绪..."</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待 webhook 证书生成</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"检查 webhook 证书..."</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> $(<span class="built_in">seq</span> 1 30); <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> kubectl --kubeconfig=<span class="string">"<span class="variable">$KUBECONFIG</span>"</span> get secret -n <span class="string">"<span class="variable">$NAMESPACE</span>"</span> <span class="string">"<span class="variable">${SCHEDULER_NAME}</span>-webhook-server-cert"</span> &gt;/dev/null 2&gt;&amp;1; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"✓ Webhook 证书已生成"</span></span><br><span class="line">        <span class="built_in">break</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [[ <span class="variable">$i</span> -eq 30 ]]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"✗ 等待 webhook 证书超时"</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">sleep</span> 2</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待 webhook Pod 就绪</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"等待 webhook Pod 就绪..."</span></span><br><span class="line">kubectl --kubeconfig=<span class="string">"<span class="variable">$KUBECONFIG</span>"</span> <span class="built_in">wait</span> --<span class="keyword">for</span>=condition=ready pod -l control-plane=controller-manager -n <span class="string">"<span class="variable">$NAMESPACE</span>"</span> --<span class="built_in">timeout</span>=120s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等待 webhook 服务端点就绪</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"等待 webhook 服务端点..."</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> $(<span class="built_in">seq</span> 1 30); <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> kubectl --kubeconfig=<span class="string">"<span class="variable">$KUBECONFIG</span>"</span> get endpoints -n <span class="string">"<span class="variable">$NAMESPACE</span>"</span> <span class="string">"<span class="variable">$WEBHOOK_SERVICE</span>"</span> -o jsonpath=<span class="string">'{.subsets[0].addresses}'</span> | grep -q .; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"✓ Webhook 服务端点已就绪"</span></span><br><span class="line">        <span class="built_in">break</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">if</span> [[ <span class="variable">$i</span> -eq 30 ]]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"✗ 等待 webhook 服务端点超时"</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">sleep</span> 2</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"✓ <span class="variable">${SCHEDULER_NAME}</span> webhook 服务已完全就绪"</span></span><br></pre></td></tr></table></figure></div><h4 id="2-修改-Kueue-部署流程"><a href="#2-修改-Kueue-部署流程" class="headerlink" title="2. 修改 Kueue 部署流程"></a>2. 修改 Kueue 部署流程</h4><p>在 <code>clusters/kueue/Makefile</code> 中的 <code>create-kueue</code> 目标中添加 webhook 等待：</p><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: create-kueue</span></span><br><span class="line"><span class="section">create-kueue:</span></span><br><span class="line">KUBECONFIG=<span class="variable">$(KUBECONFIG)</span> kubectl kustomize ../../schedulers/kueue | ../../hack/local-registry-with-load-images.sh</span><br><span class="line">KUBECONFIG=<span class="variable">$(KUBECONFIG)</span> kubectl create -k ../../schedulers/kueue</span><br><span class="line">KUBECONFIG=<span class="variable">$(KUBECONFIG)</span> kubectl patch deploy -n kueue-system kueue-controller-manager --type json \</span><br><span class="line">-p '[{<span class="string">"op"</span>: <span class="string">"replace"</span>, <span class="string">"path"</span>: <span class="string">"/spec/template/spec/containers/0/resources"</span>, <span class="string">"value"</span>: {<span class="string">"requests"</span>:{<span class="string">"cpu"</span>: <span class="string">"500m"</span>}, <span class="string">"limits"</span>:{<span class="string">"cpu"</span>: <span class="variable">$(LIMIT_CPU)</span>}}}, {<span class="string">"op"</span>: <span class="string">"replace"</span>, <span class="string">"path"</span>: <span class="string">"/spec/template/spec/containers/1/resources"</span>, <span class="string">"value"</span>: {<span class="string">"requests"</span>:{<span class="string">"cpu"</span>: <span class="string">"500m"</span>}, <span class="string">"limits"</span>:{<span class="string">"cpu"</span>: <span class="variable">$(LIMIT_CPU)</span>}}}]'</span><br><span class="line">sleep 1</span><br><span class="line">KUBECONFIG=<span class="variable">$(KUBECONFIG)</span> ../../hack/wait-for-webhook.sh kueue</span><br></pre></td></tr></table></figure></div><h4 id="3-设计原理"><a href="#3-设计原理" class="headerlink" title="3. 设计原理"></a>3. 设计原理</h4><p>参考 Makefile 中现有的 <code>wait</code> 目标设计：</p><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: wait</span></span><br><span class="line"><span class="section">wait:</span></span><br><span class="line">-for i in $<span class="variable">$(seq 1 60)</span>; do \</span><br><span class="line">KUBECONFIG=<span class="variable">$(KUBECONFIG)</span> kubectl wait -A \</span><br><span class="line">--for=condition=Ready=True pod \</span><br><span class="line">--all \</span><br><span class="line">--timeout=100s &gt;/dev/null 2&gt;&amp;1 &amp;&amp; break; \</span><br><span class="line">done</span><br><span class="line">sleep 1</span><br></pre></td></tr></table></figure></div><p>webhook 等待脚本采用相同的设计模式：</p><ul><li><strong>循环检查</strong>：使用 for 循环定期检查状态</li><li><strong>超时机制</strong>：设置合理的超时时间避免无限等待</li><li><strong>详细日志</strong>：提供清晰的进度信息</li><li><strong>错误处理</strong>：在超时或失败时提供明确的错误信息</li></ul><h3 id="验证方法-2"><a href="#验证方法-2" class="headerlink" title="验证方法"></a>验证方法</h3><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 手动测试 webhook 等待脚本</span></span><br><span class="line"><span class="built_in">cd</span> clusters/kueue</span><br><span class="line">KUBECONFIG=./kubeconfig.yaml ../../hack/wait-for-webhook.sh kueue</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 webhook 服务状态</span></span><br><span class="line">kubectl get pods -n kueue-system -l control-plane=controller-manager</span><br><span class="line">kubectl get service kueue-webhook-service -n kueue-system</span><br><span class="line">kubectl get endpoints kueue-webhook-service -n kueue-system</span><br></pre></td></tr></table></figure></div><h3 id="预期效果-1"><a href="#预期效果-1" class="headerlink" title="预期效果"></a>预期效果</h3><p>修改后，每次执行 <code>make prepare-kueue</code> 时：</p><ol><li>Kueue 集群正常启动</li><li>Webhook 服务自动部署</li><li>脚本等待 webhook 完全就绪</li><li>测试可以正常进行，不再出现连接拒绝错误</li></ol><h2 id="注意7：目录结构说明"><a href="#注意7：目录结构说明" class="headerlink" title="注意7：目录结构说明"></a>注意7：目录结构说明</h2><h3 id="核心目录"><a href="#核心目录" class="headerlink" title="核心目录"></a>核心目录</h3><ul><li><p><strong>hack/</strong>: 存放辅助脚本，如权限修复、镜像处理、结果保存等</p><ul><li><code>ensure-directories.sh</code>: 确保目录权限正确</li><li><code>local-registry-with-load-images.sh</code>: 处理镜像拉取和本地仓库</li><li><code>save-result-images.sh</code>: 保存测试结果和监控图表</li><li><code>kind-with-local-registry.sh</code>: 创建带本地仓库的 kind 集群</li></ul></li><li><p><strong>clusters/</strong>: 各调度器的集群配置和生命周期管理</p><ul><li><code>kueue/</code>: Kueue 调度器集群配置</li><li><code>volcano/</code>: Volcano 调度器集群配置  </li><li><code>yunikorn/</code>: YuniKorn 调度器集群配置</li><li><code>overview/</code>: 监控集群配置（Prometheus + Grafana）</li></ul></li><li><p><strong>test/</strong>: 测试代码和测试用例</p><ul><li><code>utils/</code>: 通用测试工具和辅助函数</li><li><code>kueue/</code>: Kueue 调度器测试代码</li><li><code>volcano/</code>: Volcano 调度器测试代码</li><li><code>yunikorn/</code>: YuniKorn 调度器测试代码</li></ul></li></ul><h3 id="生成目录"><a href="#生成目录" class="headerlink" title="生成目录"></a>生成目录</h3><ul><li><strong>bin/</strong>: 自动生成的二进制文件<ul><li><code>kind</code>: 用于创建 Kubernetes 集群的工具</li><li><code>test-kueue</code>: Kueue 测试可执行文件</li><li><code>test-volcano</code>: Volcano 测试可执行文件</li><li><code>test-yunikorn</code>: YuniKorn 测试可执行文件</li></ul></li><li><strong>gopath/</strong>: Go 模块缓存和依赖<ul><li><code>pkg/mod/</code>: Go 模块缓存</li><li><code>src/</code>: 源代码（如果使用 GOPATH 模式）</li></ul></li></ul><h3 id="数据目录"><a href="#数据目录" class="headerlink" title="数据目录"></a>数据目录</h3><ul><li><strong>logs/</strong>: 审计日志和测试日志<ul><li><code>kube-apiserver-audit.*.log</code>: Kubernetes API 服务器审计日志</li><li>其他测试过程中生成的日志文件</li></ul></li><li><strong>registry-data/</strong>: 本地 Docker 仓库数据<ul><li>存储从远程仓库拉取的镜像</li><li>供 kind 集群使用的本地镜像仓库</li></ul></li><li><strong>output/</strong>: 测试输出和监控数据<ul><li><code>panel-*.png</code>: Grafana 监控图表</li><li>其他测试输出文件</li></ul></li><li><strong>results/</strong>: 测试结果归档<ul><li>按时间戳组织的测试结果目录</li><li>包含环境变量、日志、输出等完整信息</li></ul></li><li><strong>tmp/</strong>: 临时文件目录<ul><li>测试过程中的临时文件</li><li>结果归档前的临时存储</li></ul></li></ul><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[1] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://learnku.com/go/wikis/38122">[2] Go 国内加速镜像<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://cloud.tencent.com/developer/article/2020911">[3] 深入理解 Go Modules 的 go.mod 与 go.sum<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">本文详细记录了 kube-scheduling-perf 项目在实际部署过程中可能遇到的各种技术难题及其解决方案，包括网络访问、系统兼容性、权限配置等问题。本文为实际部署篇，与理论介绍文档形成互补，助您顺利部署和使用调度器性能测试工具。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
  </entry>
  
  <entry>
    <title>【集群】K8s调度器性能对比分析：Kueue vs Volcano vs YuniKorn</title>
    <link href="https://freshwlnd.github.io/2025/06/26/k8s/k8s-scheduler-performance-vedio/"/>
    <id>https://freshwlnd.github.io/2025/06/26/k8s/k8s-scheduler-performance-vedio/</id>
    <published>2025-06-26T02:27:49.000Z</published>
    <updated>2025-08-11T09:12:52.316Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li><li><a href="/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/" title="监控与测试环境解析：测试流程拆解篇">监控与测试环境解析：测试流程拆解篇</a></li><li><a href="/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/" title="监控与测试环境解析：指标采集与可视化篇">监控与测试环境解析：指标采集与可视化篇</a></li><li><a href="/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/" title="监控与测试环境解析：自定义镜像性能回归测试">监控与测试环境解析：自定义镜像性能回归测试</a></li><li><a href="/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/" title="监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题">监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题</a></li></ol></blockquote><h1 id="💡简介"><a href="#💡简介" class="headerlink" title="💡简介"></a>💡简介</h1><p>在Kubernetes生态系统中，调度器是集群资源管理的核心组件。随着云原生应用的快速发展，传统的默认调度器已经无法满足大规模、高并发的调度需求。<br>本文整理自Apple工程师Wei Huang与DaoCloud工程师Shiming Zhang在KubeCon上的技术分享<a href="#refer-anchor-1"><sup>[1]</sup></a>，聚焦Kubernetes生态中三大主流批处理调度器（Kueue, Volcano, YuniKorn）的性能对比实验。通过分析测试方法论、关键指标和实际数据，揭示各调度器在不同负载场景下的表现差异。<br>在<a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="上一篇博客">上一篇博客</a>中所介绍的性能测试与监控工具，也就是这次技术分享中的shiming Zhang 及相关成员所设计的，测试监控工具结果也在这次技术分享中有所体现。</p><h1 id="🖼️背景"><a href="#🖼️背景" class="headerlink" title="🖼️背景"></a>🖼️背景</h1><h2 id="背景参数"><a href="#背景参数" class="headerlink" title="背景参数"></a>背景参数</h2><ol><li>API QPS 限制是一个重要的参数，而本次测试所涉及的调度器间、该参数存在差异，有可能导致部分测试不公平，因此在此提前说明。</li><li>K8s 下，调度器默认使用的 API QPS（<code>--kube-api-qps</code>）限制为 50，而 YuniKorn 为 1000。而前者暂时没找到修改的方法。</li><li>除该参数外，在测试中已尽量保持其它参数的一致，尽可能保证了测试的公平性。</li></ol><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/Freshwlnd/image/refs/heads/blog/2025-KubeCon-A-Comparative-Analysis-1.png" alt="图1：QPS限制参数情况"><figcaption>图1：QPS限制参数情况</figcaption></figure></p><h2 id="数据收集方法优化"><a href="#数据收集方法优化" class="headerlink" title="数据收集方法优化"></a>数据收集方法优化</h2><ol><li>传统方法使用 Prometheus，会造成很大的误差。<ul><li>以前看到过的<a class="link" href="https://cloud.tencent.com/developer/article/2210383">博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>也有类似的观点：<blockquote><p>十分不建议在大规模压测时通过 grafana 看板进行调度时间的统计。因为调度器暴露的调度时间指标是通过 histogram 直方图的方式，而 histogram 是假定位于每个 bucket 的样本在该 bucket 内满足均匀分布。当压测进行时，短时间大量创建 Pod，必定有部分 Pod 的调度时长达到分钟级别，此时其所属的 bucket 范围更广，均匀分布的条件就越不可能成立，从 metrics 这统计的调度时间会产生很大的误差(<a class="link" href="https://hulining.gitbook.io/prometheus/practices/histograms#errors-of-quantile-estimation)%E3%80%82">https://hulining.gitbook.io/prometheus/practices/histograms#errors-of-quantile-estimation)。<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p></blockquote></li></ul></li><li>本方法基于 audit-exporter，从 kube-apiserver 中收集信息并记录在 audit.log 来准确地记录具体性能。</li></ol><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/Freshwlnd/image/refs/heads/blog/2025-KubeCon-A-Comparative-Analysis-2.png" alt="图2：传统数据收集方法示意图"><figcaption>图2：传统数据收集方法示意图</figcaption></figure></p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/Freshwlnd/image/refs/heads/blog/2025-KubeCon-A-Comparative-Analysis-3.png" alt="图3：数据收集方法优化示意图"><figcaption>图3：数据收集方法优化示意图</figcaption></figure></p><h1 id="🧠环境说明"><a href="#🧠环境说明" class="headerlink" title="🧠环境说明"></a>🧠环境说明</h1><h2 id="测试软件环境"><a href="#测试软件环境" class="headerlink" title="测试软件环境"></a>测试软件环境</h2><p>本次性能测试软件版本如下图：</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/Freshwlnd/image/refs/heads/blog/2025-KubeCon-A-Comparative-Analysis-4.png" alt="图4：性能测试中的软件版本"><figcaption>图4：性能测试中的软件版本</figcaption></figure></p><h2 id="测试参数配置"><a href="#测试参数配置" class="headerlink" title="测试参数配置"></a>测试参数配置</h2><p>本次测试benchmark如下图，在10k总Pod量下，分别在启用/不启用Gang Scheduling的情况下（个人理解，即分别在更偏向在线服务/离线作业特征的场景下），调整Job数量和每Job的Pod数量。</p><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/Freshwlnd/image/refs/heads/blog/2025-KubeCon-A-Comparative-Analysis-5.png" alt="图5：性能测试中的参数配置"><figcaption>图5：性能测试中的参数配置</figcaption></figure></p><p>注意：在测试中没有使用很多Queue，因为发现Queue的数量对测试结果影响不大。</p><h1 id="🔨结果"><a href="#🔨结果" class="headerlink" title="🔨结果"></a>🔨结果</h1><p>对于不要求Gang调度的benchmark，Volcano表现较差；对于要求Gang调度的benchmark，Volcano的性能都是最佳的。</p><p>我们对于不要求Gang调度的四种benchmark结果进行具体介绍。其中第二种测试中的现象很有意思。<br><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/Freshwlnd/image/refs/heads/blog/2025-KubeCon-A-Comparative-Analysis-6.png" alt="图6：无GangScheduling要求下，第一种benchmark测试结果"><figcaption>图6：无GangScheduling要求下，第一种benchmark测试结果</figcaption></figure></p><ol><li>第一种benchmark下，每个Job只有1个Pod，共10K个Job，共10kPod。有以下现象：<ul><li>YuniKorn吞吐量比另外两种调度器更高，主要是因为 Kueue 和 Volcano 的 Job 受 K8s Webhook QPS限制，还没有找到可以调整这个限制的方法。</li><li>CREATED和SCHEDULED事件之间的差距很小，说明没有调度阶段不为瓶颈、没有排队，此时性能瓶颈为创建阶段。</li></ul></li></ol><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/Freshwlnd/image/refs/heads/blog/2025-KubeCon-A-Comparative-Analysis-7.png" alt="图7：无GangScheduling要求下，第二种benchmark测试结果"><figcaption>图7：无GangScheduling要求下，第二种benchmark测试结果</figcaption></figure><br>2. 第二种benchmark下，每个Job有20个Pod，共500个Job，共10kPod。有以下现象：</p><ul><li>Volcano的调度速度慢于另外两种调度器；</li><li>SCHEDULED明显滞后于CREATED，说明调度速度较慢，此时性能瓶颈为调度（且根据斜率，前期调度速度快、后期逐渐变慢）；</li><li>CREATED阶段性突变现象（正常情况下CREATED应该匀速增加，这里的现象说明controller会间歇性卡住一会儿）。<ul><li>可能的原因是Volcano会分批处理Job，在视频中的用词是“create pod in Batch”，当一批Job处理完后才会继续处理下一批Job。</li><li>此外，还有一个可能的影响因素是 Volcano 需要更多的webhook来创建Pod，而受限于webhook QPS（如前文所述），所以出现排队阻塞和卡顿。前期阻塞情况比后期更严重，可能是因为后期部分webhook可以复用。</li><li>后续还需验证该猜想。</li></ul></li></ul><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/Freshwlnd/image/refs/heads/blog/2025-KubeCon-A-Comparative-Analysis-8.png" alt="图8：无GangScheduling要求下，第三种benchmark测试结果"><figcaption>图8：无GangScheduling要求下，第三种benchmark测试结果</figcaption></figure><br>3. 第三种benchmark下，每个Job有500Pod，共20个Job，共10kPod。有以下现象：</p><ul><li>Volcano的调度速度仍然慢于另外两种调度器；</li><li>SCHEDULED仍然明显滞后于CREATED，说明调度速度较慢，此时性能瓶颈为调度（且根据斜率，前期调度速度比第二种benchmark下更慢、后期逐渐加速）；</li><li>不存在CREATED阶段性突变现象。<ul><li>和第二种benchmark的区别是Job只有20个，说明Volcano分批处理Job的粒度应该在[20,500]区间内（根据第二、三种benchmark的Job数量推断）。</li></ul></li></ul><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/Freshwlnd/image/refs/heads/blog/2025-KubeCon-A-Comparative-Analysis-9.png" alt="图9：无GangScheduling要求下，第四种benchmark测试结果"><figcaption>图9：无GangScheduling要求下，第四种benchmark测试结果</figcaption></figure><br>4. 第四种benchmark下，每个Job有10kPod，共1个Job，共10kPod。</p><ul><li><p>现象与第三种benchmark类似；根据斜率，调度速度整体比较平稳。</p></li><li><p>有意思的是四种情况下Volcano的总调度时间似乎都是差不多的。</p></li></ul><h1 id="🏥疑问及解答"><a href="#🏥疑问及解答" class="headerlink" title="🏥疑问及解答"></a>🏥疑问及解答</h1><ol><li><p>K8s参数中的<code>--kube-api-qps</code>是什么意思？</p><ul><li><strong>总结</strong>：<code>--kube-api-qps</code>是Kubernetes调度器向API服务器发送请求的速率限制参数，控制调度器每秒可以向kube-apiserver发送的最大请求数量。</li><li><strong>参数含义</strong>：QPS（Queries Per Second）表示每秒查询次数，默认值为50，意味着调度器每秒最多可以向API服务器发送50个请求。这个限制包括所有类型的API请求，如获取节点信息、创建Pod、更新Pod状态等。</li><li><strong>原因说明</strong>：设置QPS限制是为了防止调度器过度占用API服务器资源，避免API服务器过载。在大规模集群中，如果调度器不受限制地向API服务器发送请求，可能会导致API服务器响应变慢或崩溃。YuniKorn设置为1000是因为它针对高吞吐量场景进行了优化，而Kueue和Volcano使用默认值50，在大量Job创建时容易达到限制。</li></ul></li><li><p>视频中图表的含义是什么？</p><ul><li>纵轴是事件数量，横轴是测试过程时间点。</li><li>斜率表示吞吐量（每秒事件数量），斜率越大说明速度越快、效率越高。</li></ul></li><li><p>图中Created、Scheduled 代表着什么？</p><ul><li><strong>总结</strong>：Created事件表示Pod被成功创建并提交到API服务器的时间点，Scheduled事件表示Pod被调度器成功调度到某个节点的时间点。两者之间的时间差反映了调度延迟。</li><li><strong>事件含义</strong>：<ul><li><strong>CREATED事件</strong>：当Job Controller成功创建Pod并提交到kube-apiserver时触发，表示Pod已进入待调度队列。</li><li><strong>SCHEDULED事件</strong>：当调度器完成Pod的节点选择并成功绑定到节点时触发，表示Pod已获得运行位置。</li></ul></li><li><strong>原因说明</strong>：在正常情况下，CREATED和SCHEDULED事件应该紧密跟随，时间差很小。如果SCHEDULED明显滞后于CREATED，说明调度器处理能力不足，存在调度瓶颈。如果两者差距很小但整体斜率较低，说明瓶颈在Pod创建阶段而非调度阶段。</li></ul></li><li><p>视频中频繁提到的 webhook 是什么？webhook QPS是什么？为什么关注这个点？</p><ul><li><strong>总结</strong>：Webhook是Kubernetes的准入控制器机制，用于在资源创建/更新时进行验证和修改。Webhook QPS限制影响调度器创建Pod的速度，可能成为性能瓶颈。</li><li><strong>含义</strong>：<ul><li><strong>Webhook</strong>：Kubernetes准入控制器的一种实现方式，当API服务器接收到资源请求时，会调用配置的webhook服务进行验证、修改或拒绝操作。调度器（如Volcano、Kueue）通常使用webhook来实现自定义的调度逻辑，如PodGroup验证、资源配额检查等。</li><li><strong>Webhook QPS</strong>：API服务器向webhook服务发送请求的速率限制，默认通常为50 QPS，与<code>--kube-api-qps</code>类似。</li></ul></li><li><strong>原因说明</strong>：调度器需要通过webhook来验证和创建Pod，当大量Job同时提交时，webhook QPS限制会导致请求排队等待，从而影响整体调度性能。Volcano需要更多的webhook调用（如PodGroup验证、资源分配等），因此更容易受到webhook QPS限制的影响，出现阶段性阻塞现象。</li></ul></li></ol><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="📝参考文献"><a href="#📝参考文献" class="headerlink" title="📝参考文献"></a>📝参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://www.youtube.com/watch?v=njT5r3JjIaA&list=PLj6h78yzYM2MP0QhYFK8HOb8UqgbIkLMc&index=226">[1] A Comparative Analysis of Kueue, Volcano, and YuniKorn - Wei Huang, Apple &amp; Shiming Zhang, DaoCloud<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kueue.sigs.k8s.io/">[2] Kueue Documentation<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://volcano.sh/">[3] Volcano Documentation<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://yunikorn.apache.org/">[4] YuniKorn Documentation<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">[5] kubelet 参数文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://cloud.tencent.com/developer/article/2210383">[6] 大规模集群仿真模拟与调度器压测方法<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[7] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">本文基于Apple与DaoCloud工程师的分享，对三种K8s调度器（Kueue、Volcano、YuniKorn）性能对比分析进行总结。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
  </entry>
  
  <entry>
    <title>【集群】云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</title>
    <link href="https://freshwlnd.github.io/2025/06/24/k8s/k8s-scheduler-performance-test/"/>
    <id>https://freshwlnd.github.io/2025/06/24/k8s/k8s-scheduler-performance-test/</id>
    <published>2025-06-24T13:12:48.000Z</published>
    <updated>2025-08-11T09:34:59.522Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本系列《云原生批调度实战：Volcano 监控与性能测试》计划分为以下几篇，点击查看其它内容。</p><ol><li><a href="/2025/06/24/k8s/k8s-scheduler-performance-test/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf</a></li><li><a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a></li><li><a href="/2025/06/26/k8s/k8s-scheduler-performance-vedio/" title="云原生批调度实战：调度器测试监控结果">云原生批调度实战：调度器测试监控结果</a></li><li><a href="/2025/07/23/k8s/k8s-scheduler-performance-test-local/" title="云原生批调度实战：本地环境测试结果与视频对比分析">云原生批调度实战：本地环境测试结果与视频对比分析</a></li><li><a href="/2025/07/27/k8s/k8s-scheduler-performance-volcano-process/" title="监控与测试环境解析：测试流程拆解篇">监控与测试环境解析：测试流程拆解篇</a></li><li><a href="/2025/08/07/k8s/k8s-scheduler-performance-volcano-metrics/" title="监控与测试环境解析：指标采集与可视化篇">监控与测试环境解析：指标采集与可视化篇</a></li><li><a href="/2025/08/08/k8s/k8s-scheduler-performance-volcano-custom/" title="监控与测试环境解析：自定义镜像性能回归测试">监控与测试环境解析：自定义镜像性能回归测试</a></li><li><a href="/2025/08/10/k8s/k8s-scheduler-performance-volcano-analysis/" title="监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题">监控与测试环境解析：数据收集方法深度解析与Prometheus Histogram误差问题</a></li></ol></blockquote><h1 id="kube-scheduling-perf：Kubernetes多调度器性能测试框架全解析"><a href="#kube-scheduling-perf：Kubernetes多调度器性能测试框架全解析" class="headerlink" title="kube-scheduling-perf：Kubernetes多调度器性能测试框架全解析"></a>kube-scheduling-perf：Kubernetes多调度器性能测试框架全解析</h1><h2 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h2><p><code>kube-scheduling-perf</code> 项目为 Kubernetes 社区主流调度器（如 Kueue、Volcano、YuniKorn）提供了统一、自动化的性能测试与对比分析框架。通过自动化脚本和标准化测试流程，用户可在本地快速搭建测试集群，批量运行多种调度器的性能基准测试，并自动收集、汇总测试结果，极大提升了调度器性能评测的效率和可复现性。</p><blockquote><p><strong>📖 阅读说明</strong>：本文为 kube-scheduling-perf 项目的<strong>理论介绍篇</strong>，重点解析项目的架构设计、自动化流程和核心原理。如果您计划实际部署和使用该工具，强烈建议同时阅读 <a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="实操注意事项说明文档">实操注意事项说明文档</a>，该文档详细记录了在实际环境中可能遇到的各种问题（如网络访问、系统兼容性、权限配置等）及其解决方案。</p></blockquote><hr><h1 id="目录结构与核心组件说明"><a href="#目录结构与核心组件说明" class="headerlink" title="目录结构与核心组件说明"></a>目录结构与核心组件说明</h1><h2 id="clusters-目录"><a href="#clusters-目录" class="headerlink" title="clusters 目录"></a>clusters 目录</h2><ul><li><code>clusters/</code> 目录下包含了每个调度器（kueue、volcano、yunikorn）及 overview 的子目录。</li><li>每个调度器子目录下都包含一个 <code>Makefile</code> 和 <code>kind.yaml</code>，用于定义该调度器测试集群的启动、销毁、等待等操作。</li><li><code>overview/</code> 子目录用于搭建统一的监控与可视化环境（如 Prometheus + Grafana），并负责性能数据的采集与展示。</li></ul><h2 id="bin-目录"><a href="#bin-目录" class="headerlink" title="bin 目录"></a>bin 目录</h2><ul><li><code>bin/</code> 目录用于存放自动编译生成的二进制文件，如 <code>kind</code>（用于创建K8s集群）、<code>test-xxx</code>（各调度器的测试用例可执行文件）。</li><li>这些二进制文件由 Makefile 自动生成和调用，用户无需手动干预。</li></ul><hr><h1 id="Makefile-语法与目标说明"><a href="#Makefile-语法与目标说明" class="headerlink" title="Makefile 语法与目标说明"></a>Makefile 语法与目标说明</h1><h2 id="PHONY-说明"><a href="#PHONY-说明" class="headerlink" title=".PHONY 说明"></a>.PHONY 说明</h2><ul><li><code>.PHONY</code> 是 Makefile 的一个特殊声明，用于标记”伪目标”。</li><li>被 <code>.PHONY</code> 声明的目标不会与同名文件或目录冲突，每次执行 <code>make</code> 时都会被强制执行。</li><li>例如：<div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: up</span></span><br><span class="line"><span class="section">up:</span></span><br><span class="line">    <span class="comment"># ...命令...</span></span><br></pre></td></tr></table></figure></div></li></ul><h2 id="目标与依赖格式"><a href="#目标与依赖格式" class="headerlink" title="目标与依赖格式"></a>目标与依赖格式</h2><ul><li>Makefile 的每个目标格式为：<div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">目标名: 依赖1 依赖2 ...</span></span><br><span class="line">    命令1</span><br><span class="line">    命令2</span><br></pre></td></tr></table></figure></div></li><li>冒号后面可以跟依赖目标，表示在执行当前目标前会先执行依赖目标。</li><li>命令必须以Tab缩进。</li></ul><hr><h1 id="自动化测试流程详解"><a href="#自动化测试流程详解" class="headerlink" title="自动化测试流程详解"></a>自动化测试流程详解</h1><h2 id="1-make命令的起点：default-目标"><a href="#1-make命令的起点：default-目标" class="headerlink" title="1. make命令的起点：default 目标"></a>1. make命令的起点：default 目标</h2><p>当你在项目根目录下执行 <code>make</code> 时，实际上会触发 <code>Makefile</code> 中的 <code>default</code> 目标。其内容如下：</p><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: default</span></span><br><span class="line"><span class="section">default:</span></span><br><span class="line">make serial-test \</span><br><span class="line">RESULT_RECENT_DURATION_SECONDS=250 TEST_TIMEOUT_SECONDS=350 \</span><br><span class="line">NODES_SIZE=1000 \</span><br><span class="line">QUEUES_SIZE=1  JOBS_SIZE_PER_QUEUE=10000  PODS_SIZE_PER_JOB=1</span><br><span class="line"><span class="comment"># ...（省略多组不同参数的serial-test调用）</span></span><br><span class="line">make serial-test \</span><br><span class="line">RESULT_RECENT_DURATION_SECONDS=300 TEST_TIMEOUT_SECONDS=400 \</span><br><span class="line">NODES_SIZE=1000 GANG=true \</span><br><span class="line">QUEUES_SIZE=1  JOBS_SIZE_PER_QUEUE=1      PODS_SIZE_PER_JOB=10000</span><br></pre></td></tr></table></figure></div><ul><li>这里依次调用了多次 <code>serial-test</code>，每次传入不同的集群规模、作业数量、Pod数量、是否GANG调度等参数。</li><li>这样做的目的是<strong>批量测试不同场景下各调度器的性能</strong>，保证测试的全面性和对比性。</li></ul><h2 id="2-serial-test-目标的作用"><a href="#2-serial-test-目标的作用" class="headerlink" title="2. serial-test 目标的作用"></a>2. serial-test 目标的作用</h2><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: serial-test</span></span><br><span class="line"><span class="section">serial-test: bin/kind</span></span><br><span class="line"><span class="variable">$(<span class="built_in">foreach</span> sched,<span class="variable">$(SCHEDULERS)</span>, \</span></span><br><span class="line"><span class="variable">make prepare-<span class="variable">$(sched)</span>; \</span></span><br><span class="line"><span class="variable">make start-<span class="variable">$(sched)</span>; \</span></span><br><span class="line"><span class="variable">make end-<span class="variable">$(sched)</span>; \</span></span><br><span class="line"><span class="variable">)</span></span><br><span class="line">make \</span><br><span class="line">prepare-overview \</span><br><span class="line">start-overview \</span><br><span class="line">save-result \</span><br><span class="line">end-overview</span><br></pre></td></tr></table></figure></div><ul><li><code>serial-test</code> 首先依赖 <code>bin/kind</code>，确保本地有 kind 工具（用于创建K8s集群）。</li><li>然后对 <code>SCHEDULERS</code>（即 Kueue、Volcano、YuniKorn）中的每个调度器，依次执行 <code>prepare-xxx</code>、<code>start-xxx</code>、<code>end-xxx</code> 三个目标。</li><li>最后执行 overview 相关目标和结果保存。</li></ul><h2 id="3-serial-test-的每一步剖析"><a href="#3-serial-test-的每一步剖析" class="headerlink" title="3. serial-test 的每一步剖析"></a>3. serial-test 的每一步剖析</h2><h3 id="3-1-bin-kind"><a href="#3-1-bin-kind" class="headerlink" title="3.1 bin/kind"></a>3.1 bin/kind</h3><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">bin/kind:</span></span><br><span class="line"><span class="variable">$(GO_IN_DOCKER)</span> go build -o ./bin/kind sigs.k8s.io/kind</span><br></pre></td></tr></table></figure></div><ul><li>用 Docker 构建 <code>kind</code> 工具的二进制文件，确保后续可以用 kind 创建本地K8s集群。</li></ul><h3 id="3-2-prepare-xxx、start-xxx、end-xxx"><a href="#3-2-prepare-xxx、start-xxx、end-xxx" class="headerlink" title="3.2 prepare-xxx、start-xxx、end-xxx"></a>3.2 prepare-xxx、start-xxx、end-xxx</h3><p>这些目标通过 <code>define test-scheduler</code> 宏自动生成。以 <code>kueue</code> 为例：</p><ul><li><code>prepare-kueue</code>：启动集群、等待就绪、初始化测试。</li><li><code>start-kueue</code>：重置审计日志，运行批量作业调度测试。</li><li><code>end-kueue</code>：销毁测试集群。</li></ul><p>其它调度器（如 volcano、yunikorn）流程类似。</p><h3 id="3-3-overview-相关目标"><a href="#3-3-overview-相关目标" class="headerlink" title="3.3 overview 相关目标"></a>3.3 overview 相关目标</h3><h4 id="prepare-overview"><a href="#prepare-overview" class="headerlink" title="prepare-overview"></a>prepare-overview</h4><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: prepare-overview</span></span><br><span class="line"><span class="section">prepare-overview:</span></span><br><span class="line">make up-overview</span><br><span class="line">make wait-overview</span><br></pre></td></tr></table></figure></div><ul><li>启动 overview 监控集群，并等待其所有服务就绪。</li></ul><h4 id="start-overview"><a href="#start-overview" class="headerlink" title="start-overview"></a>start-overview</h4><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: start-overview</span></span><br><span class="line"><span class="section">start-overview:</span></span><br><span class="line">make -C ./clusters/overview start-export</span><br></pre></td></tr></table></figure></div><ul><li>在 overview 集群中启动数据导出与采集服务（如 patch、kustomize、kubectl create 等）。</li></ul><h4 id="save-result"><a href="#save-result" class="headerlink" title="save-result"></a>save-result</h4><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: save-result</span></span><br><span class="line"><span class="section">save-result:</span></span><br><span class="line">sleep <span class="variable">$(RESULT_RECENT_DURATION_SECONDS)</span></span><br><span class="line">RECENT_DURATION=<span class="string">"<span class="variable">$(RESULT_RECENT_DURATION_SECONDS)</span>second"</span> ./hack/save-result-images.sh</span><br><span class="line">make down</span><br><span class="line"><span class="comment"># 归档测试环境变量、日志、输出到 results 目录</span></span><br></pre></td></tr></table></figure></div><ul><li>等待一段时间，采集最新的监控数据，调用脚本保存结果，并归档到 <code>results/</code> 目录。</li></ul><h4 id="end-overview"><a href="#end-overview" class="headerlink" title="end-overview"></a>end-overview</h4><div class="code-container" data-rel="Makefile"><figure class="iseeu highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="keyword">.PHONY</span>: end-overview</span></span><br><span class="line"><span class="section">end-overview:</span></span><br><span class="line">make down-overview</span><br></pre></td></tr></table></figure></div><ul><li>销毁 overview 监控集群，释放资源。</li></ul><hr><h1 id="目标之间的调用关系图"><a href="#目标之间的调用关系图" class="headerlink" title="目标之间的调用关系图"></a>目标之间的调用关系图</h1><pre class="mermaid">graph TD    A[make] --&gt; B[default]    B --&gt; C1[serial-test(参数1)]    B --&gt; C2[serial-test(参数2)]    B --&gt; Cn[serial-test(参数n)]    C1 --&gt; D1[prepare-scheduler]    C1 --&gt; D2[start-scheduler]    C1 --&gt; D3[end-scheduler]    C1 --&gt; E[prepare-overview/start-overview/save-result/end-overview]    D1 --&gt; F1[up-scheduler]    D1 --&gt; F2[wait-scheduler]    D1 --&gt; F3[test-init-scheduler]    D2 --&gt; G1[reset-auditlog-scheduler]    D2 --&gt; G2[test-batch-job-scheduler]    D3 --&gt; H[down-scheduler]</pre><hr><h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><blockquote><p><strong>⚠️ 重要提醒</strong>：本文档主要介绍 kube-scheduling-perf 项目的理论架构和自动化流程。在实际运行过程中，由于外网访问限制、系统版本兼容性、用户权限配置等问题，您可能会遇到各种部署和使用障碍。</p></blockquote><p><strong>实际部署使用指南</strong>：请务必参考 <a href="/2025/07/01/k8s/k8s-scheduler-performance-test-debug/" title="云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明">云原生批调度实战：调度器测试与监控工具 kube-scheduling-perf 实操注意事项说明</a> 文档，该文档详细记录了：</p><ul><li><strong>网络访问问题</strong>：国内环境下的镜像加速配置、Go模块代理设置</li><li><strong>系统兼容性问题</strong>：内核版本适配、Docker版本要求、Go版本兼容性</li><li><strong>权限配置问题</strong>：Docker容器权限、目录所有权、用户权限设置</li><li><strong>版本降级方案</strong>：针对老旧系统的版本适配策略</li><li><strong>故障排除指南</strong>：常见错误及解决方案</li></ul><p>该注意事项文档基于实际部署经验总结，能够帮助您快速解决部署过程中的各种技术难题，确保测试工具能够正常运行。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li><strong><code>make</code></strong> 触发 <code>default</code>，批量执行多组参数化的 <code>serial-test</code>。</li><li><strong><code>serial-test</code></strong> 依次对每个调度器完成：集群部署→初始化→批量作业测试→集群销毁→结果采集。</li><li><strong>overview 相关目标</strong> 负责性能数据的统一采集与可视化。</li><li><strong>clusters 目录</strong> 负责各调度器及监控集群的生命周期管理。</li><li><strong>bin 目录</strong> 存放自动生成的工具和测试用例二进制文件。</li><li><strong>.PHONY</strong> 声明伪目标，保证每次都能正确执行。</li></ul><p>本项目通过 Makefile 的自动化编排、参数化测试、统一的日志与指标采集、可视化监控等手段，实现了 Kubernetes 多调度器性能对比的”一键化”与标准化。极大降低了测试门槛，提高了效率和可复现性，非常适合调度器开发者、性能分析师和社区贡献者使用与扩展。</p><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://github.com/wzshiming/kube-scheduling-perf">[1] Github - kube-scheduling-perf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://learnku.com/go/wikis/38122">[2] Go 国内加速镜像<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://cloud.tencent.com/developer/article/2020911">[3] 深入理解 Go Modules 的 go.mod 与 go.sum<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">本文深入解析 kube-scheduling-perf 项目的自动化测试流程，详细剖析 Makefile 设计、各目录结构、核心目标调用关系及其背后的自动化原理，助你一键对比 Kueue、Volcano、YuniKorn 等主流调度器的性能。本文为理论介绍篇，实际部署使用请参考注意事项说明文档。</summary>
    
    
    
    <category term="技术" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E6%8A%80%E6%9C%AF/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="调度器" scheme="https://freshwlnd.github.io/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/"/>
    
    <category term="K8s" scheme="https://freshwlnd.github.io/tags/K8s/"/>
    
    <category term="性能测试" scheme="https://freshwlnd.github.io/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"/>
    
    <category term="Volcano" scheme="https://freshwlnd.github.io/tags/Volcano/"/>
    
  </entry>
  
  <entry>
    <title>【论文】精读笔记7-前沿-Meta跨地域ML训练MAST-B-相关工作发展脉络梳理</title>
    <link href="https://freshwlnd.github.io/2025/06/24/literature/literatureNotesIntensive7/"/>
    <id>https://freshwlnd.github.io/2025/06/24/literature/literatureNotesIntensive7/</id>
    <published>2025-06-24T00:05:59.000Z</published>
    <updated>2025-06-26T02:33:36.469Z</updated>
    
    <content type="html"><![CDATA[<h1 id="x1f4d6-《MAST-Global-Scheduling-of-ML-Training-across-Geo-Distributed-Datacenters-at-Hyperscale》"><a href="#x1f4d6-《MAST-Global-Scheduling-of-ML-Training-across-Geo-Distributed-Datacenters-at-Hyperscale》" class="headerlink" title="📖《MAST: Global Scheduling of ML Training across Geo-Distributed Datacenters at Hyperscale》"></a><span class="emoji" alias="book" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png?v8">📖</span>《MAST: Global Scheduling of ML Training across Geo-Distributed Datacenters at Hyperscale》</h1><p>2024 年 Meta、The Ohio State University团队 发表于 CCF-A 类会议 OSDI。</p><blockquote><p>系列博客：</p><ol><li><a href="/2025/01/10/literature/literatureNotes78/" title="MAST-初步略读笔记">MAST-初步略读笔记</a></li><li><a href="/2025/06/24/literature/literatureNotesIntensive7/" title="MAST-相关工作发展脉络梳理">MAST-相关工作发展脉络梳理</a></li></ol></blockquote><h2 id="Meta-公司-机器学习训练背景-Background-of-ML-Training-at-Meta"><a href="#Meta-公司-机器学习训练背景-Background-of-ML-Training-at-Meta" class="headerlink" title="Meta 公司 机器学习训练背景 Background of ML Training at Meta"></a>Meta 公司 机器学习训练背景 Background of ML Training at Meta</h2><h3 id="数据中心和硬件-Datacenter-and-hardware"><a href="#数据中心和硬件-Datacenter-and-hardware" class="headerlink" title="数据中心和硬件 Datacenter and hardware"></a>数据中心和硬件 Datacenter and hardware</h3><p>我们的私有云由数十个区域和数百万台机器组成。</p><ul><li>一个<strong>区域</strong>包括多个相互靠近的<strong>数据中心</strong>。</li><li>跨区域网络<strong>带宽</strong>比区域内数据中心之间的分段带宽低约 10 倍。</li><li>数据中心的部分区域被 <strong>ML 训练集群</strong>占据，这些集群的机器配置了<strong>多个 GPU</strong>，并通过 <strong>8x200Gbps RoCE 网络</strong>和 <strong>4x100Gbps 以太网</strong>连接。</li></ul><p>ML 训练是数据密集型的，因此更倾向于将训练工作负载的<strong>计算和数据放在同一地点</strong>。</p><ul><li>对于属于同一 ML 训练工作负载的任务，我们倾向于将它们依次放置在同一个机架、集群、数据中心和区域中。</li><li>将计算和数据分开放置在不同区域或将任务放置在不同区域<strong>会导致无法接受的性能</strong>。</li></ul><p>一直以来，<strong>数据中心硬件</strong>都是根据不同时期的具体需求<strong>逐步采购</strong>的，这导致了<strong>硬件类型</strong>在不同地区的<strong>分布不均</strong>。Flux 对此进行了讨论，图 2 也显示了这一点。</p><ul><li>这种不均衡使得数据和计算<strong>难以同地放置</strong>，需要进行<strong>全局优化</strong>。（根据资源情况，选择数据和计算分布；必要时复制后分别拆开放，而非一股脑堆在一起）</li><li>例如，<ul><li>由于 Region6 缺少 GPU，因此最好将<strong>基于 CPU</strong> 的分析作业使用的数据放在 Region6。</li><li>如果一些基于 GPU 的 ML 训练工作负载与这些分析作业共享相同的数据，我们也应该将它们安排在 Region6 中。</li><li>但是，如果此类 ML 工作负载过多，我们就必须将它们的<strong>数据复制到其他区域</strong>并在那里执行。</li></ul></li></ul><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://figures.semanticscholar.org/5ee81aaa497294cc10f6d43b47089516034a52d2/5-Figure2-1.png" alt="图2：硬件在各地区分布不均。存储容量归一化，GPU和CPU按服务器数量归一化。"><figcaption>图2：硬件在各地区分布不均。存储容量归一化，GPU和CPU按服务器数量归一化。</figcaption></figure></p><h3 id="动态集群-Dynamic-clusters"><a href="#动态集群-Dynamic-clusters" class="headerlink" title="动态集群 Dynamic clusters"></a>动态集群 Dynamic clusters</h3><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/Freshwlnd/image/refs/heads/blog/2024-OSDI-Choudhury-MAST.png" alt="图1：MAST 的概念架构。全局 ML 调度器（GMS）、区域 ML 调度器（RMS）和群集管理器（CM）分别处理不同范围内的不同调度职责：全局、区域和群集。"><figcaption>图1：MAST 的概念架构。全局 ML 调度器（GMS）、区域 ML 调度器（RMS）和群集管理器（CM）分别处理不同范围内的不同调度职责：全局、区域和群集。</figcaption></figure></p><p>如图 1 所示，一个名为 <strong>RAS</strong> 的<strong>慢速路径组件</strong>将机器预先分配到动态集群，这在 RAS 论文中被称为 “保留 Reservations”。</p><ul><li>MAST方案：这样，区域 ML 调度器（RMS）就可以<strong>只搜索</strong> ML 动态集群内的机器，从而实现扩展。<ul><li>MAST 消耗 RAS 的输出（即 RAS 创建的动态群集），MAST 的调度决策不会影响或反馈到 RAS。</li></ul></li><li>RAS方案：通常，一个 ML 动态集群包括 GPU 和 CPU 机器。为了更新动态群集，RAS 将一个区域内的<strong>所有机器</strong>作为输入，并对每个动态群集的<strong>预定规模</strong>和对某些<strong>硬件类型的偏好</strong>进行新的或更新的规范。<ul><li>RAS 提出了一个 MIP 问题，用于为动态集群分配机器。</li></ul></li></ul><p>我们将简要介绍 RAS，详情请读者参阅 RAS 论文<a href="#refer-anchor-1"><sup>[2]</sup></a>。</p><ul><li>背景：RAS 可确保分配给动态群集的机器<strong>总容</strong>量满足管理员指定的要求，并包含足够的<strong>缓冲区</strong>来处理随机和<strong>相关机器故障</strong>。<strong>相关故障</strong>（如数据中心内大型故障域的断电）可能导致数以万计的机器无法使用。</li><li>方案：<ul><li>RAS 将动态集群的机器分布在<strong>不同的故障域</strong>中，以确保在大型故障域发生故障时，仍有足够的健康机器可用。</li><li>此外，RAS 还能确保每个数据中心的<strong>计算机器与存储机器比例适当</strong>，从而减少不必要的跨数据中心通信。</li><li>最后，RAS 会<strong>定期</strong>（如每 30 分钟）<strong>重新运行优</strong>化，以适应变化。<ul><li>例如，当新的数据中心上线时，RAS 可以将动态群集的机器进一步分散到这些新的数据中心，从而减少处理相关故障所需的缓冲区大小。</li></ul></li></ul></li></ul><h3 id="机器学习训练负载-ML-training-workload"><a href="#机器学习训练负载-ML-training-workload" class="headerlink" title="机器学习训练负载 ML training workload"></a>机器学习训练负载 ML training workload</h3><p>一个训练<strong>工作负载 workload</strong>包含多个异构<strong>作业 jobs</strong>，每个作业 job 包含多个同构的<strong>任务 tasks</strong>，而任务 task 被映射到一个Linux容器 container中。因此，层次结构是<strong>工作负载→作业→任务</strong>。</p><ul><li>例如，一个训练工作负载可能包括（1）执行反向传播训练的训练作业；（2）数据预处理作业；（3）参数服务器作业；以及（4）评估作业，用于评估生成的模型。</li></ul><p><strong>工作负载</strong>的所有<strong>任务</strong>需要<strong>集体调度Gang Scheduling</strong>，即它们必须一起分配。</p><ul><li>理论上，如果一个训练作业使用的GPU少于一个完整的GPU，可以使用多实例GPU（MIG）或其他软件方法将<strong>GPU共享</strong>给多个作业。</li><li>然而，在实践中，由于训练数据量庞大，我们所有的训练作业至少使用一个完整的GPU。</li></ul><h3 id="数据仓库-Data-warehouse"><a href="#数据仓库-Data-warehouse" class="headerlink" title="数据仓库 Data warehouse"></a>数据仓库 Data warehouse</h3><p>我们的数据仓库在<strong>三级层次</strong>结构中存储了<strong>数十亿字节</strong>的数据：数百个命名空间→数百万个表→数十亿个数据分区。</p><ul><li>分区<strong>一旦创建就不可更改</strong>，但可以在现有表中添加新的分区。<ul><li>例如，”user_activity “表每天都可以添加一个新分区，以记录过去 24 小时内的用户活动。</li></ul></li></ul><p>有些数据分区同时用于 ML 训练和数据分析，如 Spark 和 Presto。我们开发了一个名为 “俄罗斯方块”（Tetris）的系统，它能在考虑到 Spark、Presto 和 ML 训练作业的数据访问模式的情况下，<strong>优化跨区域的数据放置</strong>。</p><h3 id="负载共享数据分区-Sharing-of-data-partitions-by-workload"><a href="#负载共享数据分区-Sharing-of-data-partitions-by-workload" class="headerlink" title="负载共享数据分区 Sharing of data partitions by workload"></a>负载共享数据分区 Sharing of data partitions by workload</h3><p><figure class="image-caption"><img lazyload="" src="/images/loading.svg" data-src="https://figures.semanticscholar.org/5ee81aaa497294cc10f6d43b47089516034a52d2/5-Figure3-1.png" alt="图 3：数据分区的热度，以访问每个分区的不同 ML 工作负载的数量来衡量。"><figcaption>图 3：数据分区的热度，以访问每个分区的不同 ML 工作负载的数量来衡量。</figcaption></figure></p><p>图 3 显示，数据分区经常被多个 ML 工作负载<strong>共享</strong>。</p><ul><li>在 P50、P90 和 P99 百分位数下，一个数据分区分别被 3、17 和 45 个不同的工作负载共享。</li><li><strong>数据共享</strong>使<strong>数据放置问题</strong>变得复杂，因为<strong>跨区域迁移</strong>一个<strong>数据分区</strong>可能需要迁移依赖于该分区的多个<strong>工作负载</strong>。</li><li>此外，为了防止负载失衡，有必要在<strong>多个区域复制</strong>最热门的分区，否则大量依赖于这些分区的工作负载将被迫在少数区域运行。</li></ul><h3 id="机器学习训练作业的长执行时间-Long-execution-time-of-ML-training-jobs"><a href="#机器学习训练作业的长执行时间-Long-execution-time-of-ML-training-jobs" class="headerlink" title="机器学习训练作业的长执行时间 Long execution time of ML training jobs"></a>机器学习训练作业的长执行时间 Long execution time of ML training jobs</h3><p>ML 训练是<strong>资源密集型</strong>工作，可能需要<strong>很长时间</strong>才能完成。</p><ul><li>在 Meta，ML 训练工作负载的完成时间往往是 Spark 分析作业的 <strong>10 倍</strong>。因此，<strong>次优放置决策</strong>会对 ML 训练产生更大的负面影响。这就是本文所述的<strong>穷举搜索原则</strong>的动机。</li><li>此外，当<strong>工作负载在更多机器上、运行更长时间</strong>时，工作负载调度吞吐量也会下降。<ul><li>因此，如图 1 所示，<strong>分别在全局和区域范围内管理</strong>作业队列和资源分配是可行的，而不是在导致更多<strong>碎片</strong>的小集群级别进行管理。</li></ul></li></ul><h3 id="配额逾期作业抢占-Quota-and-job-preemption"><a href="#配额逾期作业抢占-Quota-and-job-preemption" class="headerlink" title="配额逾期作业抢占 Quota and job preemption"></a>配额逾期作业抢占 Quota and job preemption</h3><p>不同优先级的训练工作负载按<strong>优先级</strong>分配容量配额。</p><ul><li>如果一个团队的容量使用<strong>在配额内</strong>，MAST 保证在一定的延迟内启动其培训工作负载。</li><li>一旦团队<strong>超出配额</strong>，他们仍可提交工作负载，以最低优先级伺机运行，但当更高优先级的工作负载到来时，他们将被抢占。</li><li>因此，出于实验目的的低优先级工作负载总是能充分利用培训集群。</li><li>调度新的工作负载往往涉及抢占低优先级作业的复杂决策。这种复杂性使简单的联邦管理器Federation Manager变得不那么有效。</li></ul><h3 id="用于恢复的检查点-Checkpoint-for-recovery"><a href="#用于恢复的检查点-Checkpoint-for-recovery" class="headerlink" title="用于恢复的检查点 Checkpoint for recovery"></a>用于恢复的检查点 Checkpoint for recovery</h3><p>训练工作负载会定期检查其状态。</p><ul><li>当一台机器出现<strong>故障</strong>时，集群管理器会在替代机器上<strong>重启</strong>工作负载，使其能够从检查点恢复状态并继续执行。</li><li>在为高优先级工作负载<strong>抢占</strong>低优先级工作负载之前，集群管理器还会<strong>保存一个检查点</strong>，以便日后恢复。</li><li>随着我们不断<strong>缩短保存检查点所需的时间</strong>，我们正逐步<strong>增加检查点的频率</strong>，以尽量减少恢复过程中两个检查点之间丢失的工作量。</li><li>随着大型语言模型的训练工作量不断增加，恢复成本也越来越高，这一点变得越来越重要。</li></ul><h3 id="对于机器学习-非机器学习负载，使用分离的应用级调度器-Separate-application-level-schedulers-for-ML-and-non-ML-workloads"><a href="#对于机器学习-非机器学习负载，使用分离的应用级调度器-Separate-application-level-schedulers-for-ML-and-non-ML-workloads" class="headerlink" title="对于机器学习/非机器学习负载，使用分离的应用级调度器 Separate application-level schedulers for ML and non-ML workloads"></a>对于机器学习/非机器学习负载，使用分离的应用级调度器 Separate application-level schedulers for ML and non-ML workloads</h3><p>如图 1 所示，ML 和非ML 工作负载由不同的调度器管理。</p><ul><li>Twine 的可扩展架构允许所有工作负载共享一个用于机器和容器管理的通用集群管理器，同时针对特定工作负载采用不同的应用级调度器。例如，<ul><li>MAST 用于 ML 训练工作负载，</li><li>Shard Manager 用于有状态数据库，</li><li>Turbine 用于流处理，</li><li>Chronos 用于分析作业。</li></ul></li><li>每个应用级调度程序都针对特定目的进行了优化。例如，<ul><li>Shard Manager 针对数据库的高可用性进行了优化，</li><li>Chronos 针对短期分析作业的高调度吞吐量进行了优化，</li><li>而 MAST 则针对高质量决策和数据-GPU 主机托管进行了优化。</li></ul></li></ul><h2 id="x1f9e0-疑问"><a href="#x1f9e0-疑问" class="headerlink" title="🧠疑问"></a><span class="emoji" alias="brain" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f9e0.png?v8">🧠</span>疑问</h2><ol><li>本文可作为“跨地域供需不均衡”的论据，且强调了“不同区域资源供应有差异”是历史原因、由于设备都是逐批采购的。</li><li>RAS和我们之前考虑过的“动态分区”很像，在Godel中提过这种方案原本在字节服务器中也有使用、但是调整频率还不够高所以有滞后性。</li><li>META对任务层级的命名和业界主流不同，不知道是出于什么考虑。不过本质都是一样的三层架构。<ol><li>META：<strong>工作负载→作业→任务</strong></li><li>业界主流：<strong>作业→任务→实例</strong></li></ol></li><li>Gang Scheduling要求的是“作业中的实例”（或用本文说法就是“工作负载中的任务”），在Volcano的yaml文件中也可以看出。</li><li>在META，每个训练作业至少使用一个GPU（因为训练数据量大）。这是一个比较强的假设，可能是为了方便后续形式化定义和算法说明。但想来是有优化空间的。</li><li>数据分区（数据集）放置和复制的权衡会是一个需要解决的问题。</li><li>在META，ML 训练工作负载的完成时间往往是 Spark 分析作业的 <strong>10 倍</strong>，是一个可以使用的论据。</li><li>如果仅在一个小集群内调度，可能会产生更多的碎片，从而导致调度吞吐量下降。</li><li>当存在抢占决策时，多调度器运行会变得更困难。</li><li>在META中，Twine架构下可同时使用Shard Manager、Turbine、Chronos等管理器，但多种架构同时运行会对MAST调度带来什么影响没有明确？</li></ol><hr><hr><ul><li>希望这篇博客对你有帮助！如果你有任何问题或需要进一步的帮助，请随时提问。</li><li>如果你喜欢这篇文章，欢迎<a class="link" href="https://github.com/freshwlnd/">动动小手<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>给我一个follow或star。</li></ul><h1 id="x1f5fa-参考文献"><a href="#x1f5fa-参考文献" class="headerlink" title="🗺参考文献"></a><span class="emoji" alias="world_map" style="" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f5fa.png?v8">🗺</span>参考文献</h1><div id="refer-anchor-1"></div><p><a class="link" href="https://www.usenix.org/conference/osdi24/presentation/choudhury">[1] Choudhury, Arnab, et al. “MAST: Global scheduling of ML training across Geo-Distributed datacenters at hyperscale.” 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). 2024.<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p><p><a class="link" href="https://dl.acm.org/doi/10.1145/3477132.3483578">[2] Andrew Newell, Dimitrios Skarlatos, Jingyuan Fan, Pavan Kumar, Maxim Khutornenko, Mayank Pundir, Yirui Zhang, Mingjun Zhang, Yuanlai Liu, Linh Le, Brendon Daugherty, Apurva Samudra, Prashasti Baid, James Kneeland, Igor Kabiljo, Dmitry Shchukin, Andre Rodrigues, Scott Michelson, Ben Christensen, Kaushik Veeraraghavan, and Chunqiang Tang. RAS: Continuously Optimized Region-Wide Datacenter Resource Allocation. In Proceedings of the 28th ACM Symposium on Operating Systems Principles, 2021.<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>]]></content>
    
    
    <summary type="html">《MAST: Global Scheduling of ML Training across Geo-Distributed Datacenters at Hyperscale》，MAST：跨地理分布式数据中心的超大规模 ML 训练的全局调度</summary>
    
    
    
    <category term="论文" scheme="https://freshwlnd.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    <category term="精读" scheme="https://freshwlnd.github.io/categories/%E8%AE%BA%E6%96%87/%E7%B2%BE%E8%AF%BB/"/>
    
    <category term="云计算" scheme="https://freshwlnd.github.io/categories/%E8%AE%BA%E6%96%87/%E7%B2%BE%E8%AF%BB/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
    <category term="大规模" scheme="https://freshwlnd.github.io/tags/%E5%A4%A7%E8%A7%84%E6%A8%A1/"/>
    
    <category term="跨地域" scheme="https://freshwlnd.github.io/tags/%E8%B7%A8%E5%9C%B0%E5%9F%9F/"/>
    
    <category term="机器学习训练" scheme="https://freshwlnd.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
</feed>
